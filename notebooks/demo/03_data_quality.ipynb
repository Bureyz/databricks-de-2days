{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27d8de6a-6ffc-4a65-a1b0-1890f3387beb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Quality and Cleaning\n",
    "\n",
    "**Training Objective:** Understand techniques for identifying and resolving data quality issues, understand strategies for handling null values, type validation, deduplication, and data standardization.\n",
    "\n",
    "**Topics Covered:**\n",
    "- Handling null values\n",
    "- Type validation\n",
    "- Deduplication\n",
    "- Standardization\n",
    "- Common quality issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "610a58ee-9635-4aa4-a50b-479fda10b741",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Theoretical Introduction\n",
    "\n",
    "**Section Objective:** Understand the foundations of data quality and data cleansing techniques.\n",
    "\n",
    "**Basic Concepts:**\n",
    "- **Data Quality**: A measure of data suitability for its intended purpose\n",
    "- **Data Cleansing**: The process of identifying and correcting errors in data\n",
    "- **Data Validation**: Verification of data compliance with business rules\n",
    "- **Data Standardization**: Unification of data formats and representation\n",
    "- **Data Profiling**: Analysis of structure, content, and relationships in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ae724e7-9abf-4a35-80a5-08e9c8e9b55c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## User Isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13d33558-b424-4e25-ab88-4da72280f040",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "497f6109-4d20-48b3-b7be-dd8f207a5109",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b15728b9-230e-458c-b7fe-84a7c8b27798",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Display user context (variables from 00_setup)\n",
    "print(\"=== User Context ===\")\n",
    "print(f\"Catalog: {CATALOG}\")\n",
    "print(f\"Bronze Schema: {BRONZE_SCHEMA}\")\n",
    "print(f\"Silver Schema: {SILVER_SCHEMA}\")\n",
    "print(f\"Gold Schema: {GOLD_SCHEMA}\")\n",
    "print(f\"User: {raw_user}\")\n",
    "\n",
    "# Set default catalog and schema\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "print(\"\\n=== Configuration completed successfully ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c0f73f-744a-4914-8988-c57a06823df8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### User Context\n",
    "\n",
    "After running setup, we check environment variables:\n",
    "\n",
    "**We use:**\n",
    "- **CATALOG**: Isolated catalog per user\n",
    "- **BRONZE_SCHEMA**: Raw data layer\n",
    "- **SILVER_SCHEMA**: Cleaned data layer\n",
    "- **GOLD_SCHEMA**: Aggregated data layer\n",
    "- **raw_user**: User identifier\n",
    "- **DATASET_BASE_PATH**: Path to the data folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "576533ef-4937-4ef4-aa89-0f6cbfa378cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Library Imports:**\n",
    "- `functions as F` - PySpark functions for data transformation\n",
    "- `types` - Data type definitions (StructType, StringType)\n",
    "- `Window` - Window functions for deduplication and ranking\n",
    "- `re` - Regular expressions for validation\n",
    "- `datetime` - Date and time operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92c8aaa9-b9c7-43fe-a718-21cd1f96d858",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Loading Data from Dataset\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "In this notebook, we use files from the local `dataset/` folder (Training Day 1-2). CSV, JSON, and Parquet files are loaded directly from the file system using the `DATASET_BASE_PATH` path from `00_setup.ipynb`.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **DATASET_BASE_PATH**: Path to the dataset/ folder defined in 00_setup.ipynb\n",
    "- **CSV Reader**: spark.read.format(\"csv\") with options (header, inferSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2ee3ace-8a8b-4fa9-bad8-04b0f4e2418d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example: Loading Customers Data\n",
    "\n",
    "**Goal:** Load customer data from a CSV file stored in the dataset/ folder.\n",
    "\n",
    "**Approach:**\n",
    "1. Define the path using DATASET_BASE_PATH from 00_setup.ipynb\n",
    "2. Load CSV with options (header, inferSchema)\n",
    "3. Basic exploration of loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84389d0f-05ab-4c08-a3a7-099069cb1c18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: CSV file: {DATASET_BASE_PATH}/customers/customers.csv\n",
    "# VARIABLE: df_customers - DataFrame with customer data\n",
    "\n",
    "# Path to file in dataset\n",
    "customers_path = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "\n",
    "# Load data\n",
    "df_customers = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(customers_path)\n",
    "\n",
    "# Basic exploration\n",
    "print(\"=== Customer data loaded ===\")\n",
    "print(f\"Record count: {df_customers.count()}\")\n",
    "print(f\"Column count: {len(df_customers.columns)}\")\n",
    "print(f\"\\nColumns: {df_customers.columns}\")\n",
    "\n",
    "# Data schema\n",
    "print(\"\\n=== Data Schema ===\")\n",
    "df_customers.printSchema()\n",
    "\n",
    "# Preview first records\n",
    "print(\"\\n=== First 10 records ===\")\n",
    "display(df_customers.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d450a25c-9d45-46e7-92a7-5857ffa4738e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We load customer data from a CSV file using the `DATASET_BASE_PATH` defined in `00_setup.ipynb`. The `inferSchema=true` option automatically detects data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72bc36fd-d623-49e5-8e31-0133294cc9f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Basic statistics of loaded data\n",
    "total_rows = df_customers.count()\n",
    "total_columns = len(df_customers.columns)\n",
    "\n",
    "display(f\"Record count: {total_rows}\")\n",
    "display(f\"Column count: {total_columns}\")\n",
    "display(f\"Columns: {df_customers.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "251af436-db05-4869-98ee-0fc198de2e8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We display basic information about the loaded DataFrame - number of records, columns, and column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd144bd1-72e3-43f6-b2bf-53473b6703e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data schema - column types\n",
    "df_customers.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4e55f7d-37e6-4df9-963a-e22acb549592",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We check the data schema - column types automatically detected by Spark. In production, explicit schema definition is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4937e134-78ff-4213-b15f-36ca89b17458",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preview first records\n",
    "display(df_customers.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f8f2ce9-cd15-4539-a94c-300f68f6d0fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We display the first 10 records to see the actual data and identify potential quality issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fcc0ef1-e096-4e35-95fe-92df5681abc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Profiling - Data Quality Analysis\n",
    "\n",
    "**Goal:** Identify quality issues in loaded data before cleaning begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c72c6a73-48b0-4712-b96c-fce6312339e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: profiling_report - dict with quality statistics\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA QUALITY PROFILING REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Completeness - Null value analysis\n",
    "print(\"\\n1. COMPLETENESS - Null values per column:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "null_analysis = []\n",
    "for col_name in df_customers.columns:\n",
    "    null_count = df_customers.filter(F.col(col_name).isNull()).count()\n",
    "    total_count = df_customers.count()\n",
    "    null_pct = (null_count / total_count) * 100\n",
    "    null_analysis.append((col_name, null_count, null_pct))\n",
    "\n",
    "# Display null value statistics\n",
    "for col_name, null_count, null_pct in null_analysis:\n",
    "    display(f\"{col_name:20s}: {null_count:4d} nulls ({null_pct:5.1f}%)\")\n",
    "\n",
    "# 2. Uniqueness - Duplicate analysis\n",
    "print(\"\\n2. UNIQUENESS - Duplicate analysis:\")\n",
    "print(\"-\" * 80)\n",
    "total_rows = df_customers.count()\n",
    "unique_rows = df_customers.distinct().count()\n",
    "duplicate_rows = total_rows - unique_rows\n",
    "print(f\"  Total rows: {total_rows}\")\n",
    "print(f\"  Unique rows: {unique_rows}\")\n",
    "print(f\"  Duplicate rows: {duplicate_rows}\")\n",
    "print(f\"  Duplication rate: {(duplicate_rows/total_rows)*100:.1f}%\")\n",
    "\n",
    "# 3. Consistency - Unique values in key columns\n",
    "print(\"\\n3. CONSISTENCY - Unique values:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for col_name in df_customers.columns:\n",
    "    distinct_count = df_customers.select(col_name).distinct().count()\n",
    "    print(f\"  {col_name:20s}: {distinct_count:4d} unique values\")\n",
    "\n",
    "# 4. Accuracy - Sample values\n",
    "print(\"\\n4. ACCURACY - Sample values (first 5):\")\n",
    "print(\"-\" * 80)\n",
    "display(df_customers.limit(5))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PROFILING COMPLETED\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43ec9d3d-e247-4d4b-9944-c0742fbbfa63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Completeness Analysis:** We check the percentage of missing values in each column. A high percentage of null values may indicate issues in the source system or the need for alternative filling strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e20d3ff1-6862-4872-9f55-9f98d51259e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Uniqueness - Duplicate analysis\n",
    "total_rows = df_customers.count()\n",
    "unique_rows = df_customers.distinct().count()\n",
    "duplicate_rows = total_rows - unique_rows\n",
    "duplication_rate = (duplicate_rows/total_rows)*100\n",
    "\n",
    "display(f\"Total rows: {total_rows}\")\n",
    "display(f\"Unique rows: {unique_rows}\")\n",
    "display(f\"Duplicate rows: {duplicate_rows}\")\n",
    "display(f\"Duplication rate: {duplication_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "703c519e-f8a9-468f-bcda-b141421e911a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Uniqueness Analysis:** We identify duplicates - records that are completely identical in all columns. Duplicates can result from ETL errors or reloading the same data multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "725b86f2-2e91-435f-880d-67b9a203b0f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Consistency - Unique value analysis\n",
    "consistency_analysis = []\n",
    "for col_name in df_customers.columns:\n",
    "    distinct_count = df_customers.select(col_name).distinct().count()\n",
    "    consistency_analysis.append((col_name, distinct_count))\n",
    "\n",
    "# Display unique value statistics\n",
    "for col_name, distinct_count in consistency_analysis:\n",
    "    display(f\"{col_name:20s}: {distinct_count:4d} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e1f3b2a-6028-4d57-88a1-4cc792442072",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Consistency Analysis:** We check the number of unique values in each column. This helps identify categorical columns, potential keys, and data formatting issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbd453f2-0035-4f4c-9c3f-57268c901b64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Accuracy - Preview sample values\n",
    "display(df_customers.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "000c6320-981d-434e-8dec-2ecbbc0e03fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Accuracy Analysis:** Previewing actual data values allows for manual verification of correctness - whether the data looks realistic and meets business expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c334e71-0521-44f6-a728-8306e58fddff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Handling Null Values\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Missing values are one of the most common data quality issues. The strategy for handling null values depends on the business context and the nature of the data. Incorrect handling can lead to errors in analysis and ML models.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **fillna()**: Fill null values with a specific value or strategy\n",
    "- **dropna()**: Remove records containing null values\n",
    "- **coalesce()**: Select the first non-null value from multiple columns\n",
    "- **Imputation**: Statistical filling methods (mean, median, mode)\n",
    "\n",
    "**Practical Application:**\n",
    "- Filling missing values with sensible defaults\n",
    "- Removing records with critical missing data\n",
    "- Fallback to alternative data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b239f4ac-bd75-41a3-85f6-e6c8bdcbeda0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Filling Null Values (fillna)\n",
    "\n",
    "**Goal:** Fill missing values in columns using different strategies.\n",
    "\n",
    "**Approach:**\n",
    "1. Identify columns with null values\n",
    "2. Choose appropriate strategy per column\n",
    "3. Apply fillna() with a dictionary of values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3395356-1ec9-46ba-ba6b-2bfd7e68c5dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We define a null filling strategy using a dictionary with default values for each column. We choose sensible defaults consistent with the business context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1351b22-e50e-405d-8022-ba644843ec8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We verify the effectiveness of the fillna operation by comparing the number of null values before and after the transformation. All values in the selected columns should be filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f0e7095-14bf-4600-8e87-7d5280d444b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_filled - DataFrame with filled null values\n",
    "\n",
    "# Null filling strategy - only for columns that can actually have nulls\n",
    "fill_values = {\n",
    "    \"phone\": \"no phone\",\n",
    "    \"city\": \"Unknown\", \n",
    "    \"state\": \"Unknown\",\n",
    "    \"country\": \"Unknown\"\n",
    "}\n",
    "\n",
    "# Fill null values\n",
    "df_filled = df_customers.fillna(fill_values)\n",
    "\n",
    "# Verify changes\n",
    "print(\"=== Comparison before and after fillna ===\")\n",
    "for col_name in fill_values.keys():\n",
    "    before_nulls = df_customers.filter(F.col(col_name).isNull()).count()\n",
    "    after_nulls = df_filled.filter(F.col(col_name).isNull()).count()\n",
    "    print(f\"{col_name:15s}: {before_nulls:3d} nulls → {after_nulls:3d} nulls\")\n",
    "\n",
    "# Sample records after filling\n",
    "print(\"\\n=== Sample filled records ===\")\n",
    "display(df_filled.filter(\n",
    "    df_customers[\"phone\"].isNull() | \n",
    "    df_customers[\"city\"].isNull() |\n",
    "    df_customers[\"state\"].isNull()\n",
    ").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d1a13ab-d37e-4a12-a436-32f8c62c0017",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We check records that originally had null values - they should now be filled with default values according to our strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc32e771-0a4d-48e3-9bd2-349161677f77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample records after filling - records that had null values\n",
    "display(df_filled.filter(\n",
    "    df_customers[\"phone\"].isNull() | \n",
    "    df_customers[\"city\"].isNull() |\n",
    "    df_customers[\"state\"].isNull()\n",
    ").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40e1ca53-8820-4535-85cf-4fc3239f11f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Dropping Null Values (dropna)\n",
    "\n",
    "**Goal:** Remove records with missing values in key columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b54ac61-9b65-43ca-9594-c351604ac707",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_valid - DataFrame with records removed where key columns are null\n",
    "\n",
    "# Remove records without key information (customer_id is required)\n",
    "df_valid = df_customers.dropna(subset=[\"customer_id\"])\n",
    "\n",
    "# Verify changes\n",
    "print(\"=== Comparison before and after dropna ===\")\n",
    "print(f\"Record count BEFORE: {df_customers.count()}\")\n",
    "print(f\"Record count AFTER: {df_valid.count()}\")\n",
    "print(f\"Records removed: {df_customers.count() - df_valid.count()}\")\n",
    "\n",
    "# Check if there are still nulls in customer_id\n",
    "null_ids = df_valid.filter(F.col(\"customer_id\").isNull()).count()\n",
    "print(f\"\\nNulls in customer_id after dropna: {null_ids}\")\n",
    "\n",
    "# Alternative usage: dropna with how='all' (removes only if all columns are null)\n",
    "df_any_data = df_customers.dropna(how='all')\n",
    "print(f\"\\nRecords with any data: {df_any_data.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bbca206-5e96-4ad3-8692-982921b99831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We remove records that do not have values in key columns. `customer_id` is mandatory - records without it are useless in business analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ecfb226-12bb-4219-8e94-c36993a315de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify changes after dropna\n",
    "records_before = df_customers.count()\n",
    "records_after = df_valid.count()\n",
    "records_removed = records_before - records_after\n",
    "\n",
    "display(f\"Record count BEFORE: {records_before}\")\n",
    "display(f\"Record count AFTER: {records_after}\")\n",
    "display(f\"Records removed: {records_removed}\")\n",
    "\n",
    "# Check if there are still nulls in customer_id\n",
    "null_ids = df_valid.filter(F.col(\"customer_id\").isNull()).count()\n",
    "display(f\"Nulls in customer_id after dropna: {null_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a52ada58-7436-4839-bedb-21242d2a5750",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We verify the effectiveness of the dropna operation - we check how many records were removed and if there are no more null values in the `customer_id` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60590856-30be-49ec-b0eb-82c1933a5fbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Alternative usage: dropna with how='all' (removes only if all columns are null)\n",
    "df_any_data = df_customers.dropna(how='all')\n",
    "display(f\"Records with any data: {df_any_data.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b127574-6ed5-49a6-8eb6-f14c217ef935",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Alternative strategy: `how='all'` removes only records where all columns are null. It is less restrictive and preserves records with at least one non-null value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "771829fd-0deb-480a-8612-22d6e0681da7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Coalesce Strategy\n",
    "\n",
    "**Goal:** Use coalesce() to fallback between alternative data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5561f47d-aae3-46f0-88b9-d0228a706693",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_with_contact - DataFrame with new column primary_contact\n",
    "\n",
    "from pyspark.sql.functions import coalesce, lit\n",
    "\n",
    "# Example: Create primary_contact selecting the first non-null value\n",
    "df_with_contact = df_customers.withColumn(\n",
    "    \"primary_contact\",\n",
    "    coalesce(F.col(\"email\"), F.col(\"phone\"), lit(\"no contact\"))\n",
    ")\n",
    "\n",
    "# Create full address from available fields\n",
    "df_with_contact = df_with_contact.withColumn(\n",
    "    \"full_address\",\n",
    "    coalesce(\n",
    "        F.concat_ws(\", \", F.col(\"city\"), F.col(\"state\"), F.col(\"country\")),\n",
    "        F.concat_ws(\", \", F.col(\"city\"), F.col(\"country\")),\n",
    "        F.col(\"country\"),\n",
    "        lit(\"Address Unknown\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Verify\n",
    "print(\"=== primary_contact Analysis ===\")\n",
    "contact_stats = df_with_contact.groupBy(\"primary_contact\").count().orderBy(F.desc(\"count\"))\n",
    "display(contact_stats.limit(10))\n",
    "\n",
    "# Examples\n",
    "print(\"\\n=== Sample records with primary_contact and full_address ===\")\n",
    "display(df_with_contact.select(\"customer_id\", \"email\", \"phone\", \"primary_contact\", \"city\", \"state\", \"country\", \"full_address\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "678969ba-a5d1-4524-bb42-3606bdb51a00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We use `coalesce()` to create the `primary_contact` column - selecting the first non-null value from `email`, `phone`, or a default value. This implements fallback logic in case of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bae87360-ce13-4f8c-a703-0b6e8c9477ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create full address from available fields\n",
    "df_with_contact = df_with_contact.withColumn(\n",
    "    \"full_address\",\n",
    "    coalesce(\n",
    "        F.concat_ws(\", \", F.col(\"city\"), F.col(\"state\"), F.col(\"country\")),\n",
    "        F.concat_ws(\", \", F.col(\"city\"), F.col(\"country\")),\n",
    "        F.col(\"country\"),\n",
    "        lit(\"Address Unknown\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba292a9b-402e-4015-9a19-9b226de1390d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We create `full_address` using advanced coalesce with `concat_ws()` - trying different combinations of address fields, selecting the first non-null one. Fallback strategies: full → city+country → country only → default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58aab105-710a-4244-8235-9b3779553378",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# primary_contact analysis - statistics of using different contact types\n",
    "contact_stats = df_with_contact.groupBy(\"primary_contact\").count().orderBy(F.desc(\"count\"))\n",
    "display(contact_stats.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c3b7f6c-30b5-40bb-b074-2d594fd02938",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We analyze the effectiveness of the coalesce strategy - checking how many records use email, phone, or default value as primary_contact. This helps assess the quality of contact data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aa6c92c-0712-4420-bd99-c68b8aeff4a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample records with primary_contact and full_address\n",
    "display(df_with_contact.select(\n",
    "    \"customer_id\", \"email\", \"phone\", \"primary_contact\", \n",
    "    \"city\", \"state\", \"country\", \"full_address\"\n",
    ").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2edf35d5-1318-4548-8a6e-72c73fdbc618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The record preview shows how coalesce filled the new columns `primary_contact` and `full_address` using available source data or default values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d50d87ed-2e88-41e8-a281-7aadc227e06a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Type Validation and Conversion\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Incorrect data types are a common problem when loading data from text files (CSV, JSON). Type conversions must be performed safely with error handling to avoid data loss or incorrect analysis results.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **cast()**: Data type conversion (string → int, date, timestamp)\n",
    "- **to_date()**: Parsing strings to DateType with format specification\n",
    "- **to_timestamp()**: Parsing strings to TimestampType\n",
    "- **try_cast()**: Safe conversion returning null on error (Spark 3.4+)\n",
    "\n",
    "**Practical Application:**\n",
    "- Type validation after loading CSV with inferSchema\n",
    "- Parsing dates in non-standard formats\n",
    "- Type conversion before joins and aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e057d71-27de-46e4-8351-bc0043852b65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Numeric Conversion (cast)\n",
    "\n",
    "**Goal:** Safe conversion of strings to numeric types with validation.\n",
    "\n",
    "**Approach:**\n",
    "1. Clean values before conversion (remove special characters)\n",
    "2. Convert type using cast()\n",
    "3. Validate value range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "133bf1e7-20ba-4c5d-bf39-96d20bc625a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_typed - DataFrame with correctly converted types\n",
    "\n",
    "# Example: Validate registration dates and add quality flags\n",
    "df_typed = df_customers.withColumn(\n",
    "    \"registration_date_parsed\",\n",
    "    F.to_date(F.col(\"registration_date\"), \"yyyy-MM-dd\")\n",
    ")\n",
    "\n",
    "# Validate date range (2020-2026)\n",
    "df_typed = df_typed.withColumn(\n",
    "    \"registration_date_valid\",\n",
    "    (F.col(\"registration_date_parsed\").isNotNull()) & \n",
    "    (F.col(\"registration_date_parsed\") >= \"2020-01-01\") & \n",
    "    (F.col(\"registration_date_parsed\") <= \"2026-12-31\")\n",
    ")\n",
    "\n",
    "# Add account age in days\n",
    "df_typed = df_typed.withColumn(\n",
    "    \"account_age_days\",\n",
    "    F.datediff(F.current_date(), F.col(\"registration_date_parsed\"))\n",
    ")\n",
    "\n",
    "# Validate email format\n",
    "df_typed = df_typed.withColumn(\n",
    "    \"email_valid\",\n",
    "    (F.col(\"email\").isNotNull()) & \n",
    "    F.col(\"email\").rlike(\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\")\n",
    ")\n",
    "\n",
    "# Conversion statistics\n",
    "total = df_typed.count()\n",
    "valid_dates = df_typed.filter(F.col(\"registration_date_valid\") == True).count()\n",
    "invalid_dates = df_typed.filter(F.col(\"registration_date_valid\") == False).count()\n",
    "valid_emails = df_typed.filter(F.col(\"email_valid\") == True).count()\n",
    "\n",
    "print(\"=== Validation Statistics ===\")\n",
    "print(f\"Total: {total}\")\n",
    "print(f\"Valid registration dates: {valid_dates} ({(valid_dates/total)*100:.1f}%)\")\n",
    "print(f\"Invalid registration dates: {invalid_dates} ({(invalid_dates/total)*100:.1f}%)\")\n",
    "print(f\"Valid emails: {valid_emails} ({(valid_emails/total)*100:.1f}%)\")\n",
    "\n",
    "# Examples of invalid values\n",
    "if invalid_dates > 0:\n",
    "    print(\"\\n=== Examples of invalid registration dates ===\")\n",
    "    display(df_typed.filter(F.col(\"registration_date_valid\") == False).select(\"customer_id\", \"registration_date\", \"registration_date_parsed\", \"registration_date_valid\").limit(5))\n",
    "\n",
    "# Schema after conversion\n",
    "print(\"\\n=== Schema after conversion ===\")\n",
    "df_typed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f7df7cc-ee25-4111-96c1-36bd0f91279b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We calculate the account age in days using `datediff()` between the current date and the registration date. This is a useful metric for behavioral analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66d9c4f5-f806-465c-b025-baaa5dad3d85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validate email format using regular expression\n",
    "df_typed = df_typed.withColumn(\n",
    "    \"email_valid\",\n",
    "    (F.col(\"email\").isNotNull()) & \n",
    "    F.col(\"email\").rlike(\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79f7be03-1445-46a9-858d-759d629fc351",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We validate the email format using the `rlike()` regular expression. We check the basic structure: local_part@domain.extension. The `email_valid` flag can be used for filtering and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d18aa0d-9e01-49b0-8ca9-541407efb059",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate validation statistics\n",
    "total = df_typed.count()\n",
    "valid_dates = df_typed.filter(F.col(\"registration_date_valid\") == True).count()\n",
    "invalid_dates = df_typed.filter(F.col(\"registration_date_valid\") == False).count()\n",
    "valid_emails = df_typed.filter(F.col(\"email_valid\") == True).count()\n",
    "\n",
    "# Display statistics using display()\n",
    "display(f\"Total records: {total}\")\n",
    "display(f\"Valid registration dates: {valid_dates} ({(valid_dates/total)*100:.1f}%)\")\n",
    "display(f\"Invalid registration dates: {invalid_dates} ({(invalid_dates/total)*100:.1f}%)\")\n",
    "display(f\"Valid emails: {valid_emails} ({(valid_emails/total)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16fc1445-e421-4248-85c1-4a9f67692d71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We calculate and display validation effectiveness statistics - the percentage of valid dates and emails. These metrics are crucial for assessing source data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea23d401-b928-4e00-a459-7355e521aabc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Examples of invalid registration dates (if any)\n",
    "invalid_dates_sample = df_typed.filter(F.col(\"registration_date_valid\") == False).select(\n",
    "    \"customer_id\", \"registration_date\", \"registration_date_parsed\", \"registration_date_valid\"\n",
    ").limit(5)\n",
    "\n",
    "if invalid_dates_sample.count() > 0:\n",
    "    display(invalid_dates_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58b77bad-1b79-40f4-9a8e-a4cd8045df67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Previewing records with invalid dates helps understand the nature of issues in the source data. This may indicate the need for additional validation rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97a1f1fc-0213-4e46-a317-575979c95f0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check data schema after adding new columns\n",
    "df_typed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd6ff183-43bc-45ab-9797-0c460b73ea17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We check the updated DataFrame schema after adding new columns with validation and calculations. New columns of DateType, BooleanType, and LongType extend the original schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1205430e-bf83-43e3-83da-1674639a6f2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We parse date strings to DateType using `to_date()` with an explicitly specified format. The `yyyy-MM-dd` format is the ISO 8601 standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22e1e5de-4a9b-4f97-84c4-7941a953a533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validate date range (2020-2026) and add quality flag\n",
    "df_typed = df_typed.withColumn(\n",
    "    \"registration_date_valid\",\n",
    "    (F.col(\"registration_date_parsed\").isNotNull()) & \n",
    "    (F.col(\"registration_date_parsed\") >= \"2020-01-01\") & \n",
    "    (F.col(\"registration_date_parsed\") <= \"2026-12-31\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77e880d4-9dc0-43c8-ae20-33aa8be2b8fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We add the `registration_date_valid` flag checking the logical date range (2020-2026). Quality flags are crucial for auditability and monitoring of data pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "993abebe-2a10-4750-94e4-9edd132fccc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Date Conversion (to_date)\n",
    "\n",
    "**Goal:** Parse strings to DateType with support for multiple formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2c49468-ea46-4f0d-9d28-690643fa3248",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_with_dates - DataFrame with correctly parsed dates\n",
    "\n",
    "# Convert registration_date with support for multiple formats\n",
    "df_with_dates = df_customers.withColumn(\n",
    "    \"registration_date_parsed\",\n",
    "    coalesce(\n",
    "        F.to_date(F.col(\"registration_date\"), \"yyyy-MM-dd\"),     # Format: 2024-01-15\n",
    "        F.to_date(F.col(\"registration_date\"), \"dd/MM/yyyy\"),     # Format: 15/01/2024\n",
    "        F.to_date(F.col(\"registration_date\"), \"MM-dd-yyyy\"),     # Format: 01-15-2024\n",
    "        F.to_date(F.col(\"registration_date\"), \"yyyy.MM.dd\"),     # Format: 2024.01.15\n",
    "        F.to_date(F.col(\"registration_date\"))                    # Automatic detection\n",
    "    )\n",
    ")\n",
    "\n",
    "# Validate conversion\n",
    "total = df_with_dates.count()\n",
    "parsed = df_with_dates.filter(F.col(\"registration_date_parsed\").isNotNull()).count()\n",
    "failed = df_with_dates.filter(\n",
    "    F.col(\"registration_date\").isNotNull() & \n",
    "    F.col(\"registration_date_parsed\").isNull()\n",
    ").count()\n",
    "\n",
    "print(\"=== Date Conversion Statistics ===\")\n",
    "print(f\"Total: {total}\")\n",
    "print(f\"Successfully parsed: {parsed} ({(parsed/total)*100:.1f}%)\")\n",
    "print(f\"Failed to parse: {failed} ({(failed/total)*100:.1f}%)\")\n",
    "\n",
    "# Examples of conversion\n",
    "print(\"\\n=== Sample date conversions ===\")\n",
    "display(df_with_dates.select(\"customer_id\", \"registration_date\", \"registration_date_parsed\").limit(10))\n",
    "\n",
    "# Records with parsing errors\n",
    "if failed > 0:\n",
    "    print(\"\\n=== Records with parsing errors ===\")\n",
    "    display(df_with_dates.filter(\n",
    "        F.col(\"registration_date\").isNotNull() & \n",
    "        F.col(\"registration_date_parsed\").isNull()\n",
    "    ).select(\"customer_id\", \"registration_date\", \"registration_date_parsed\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6722dd3f-1e41-4613-a539-ce116e0c2a53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We use an advanced date conversion strategy with `coalesce()` - trying various popular date formats, selecting the first one that succeeds. This ensures greater flexibility with inconsistent formats in source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa0dbc1b-6a37-4532-a253-2b097f869ead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate date conversion effectiveness statistics\n",
    "total = df_with_dates.count()\n",
    "parsed = df_with_dates.filter(F.col(\"registration_date_parsed\").isNotNull()).count()\n",
    "failed = df_with_dates.filter(\n",
    "    F.col(\"registration_date\").isNotNull() & \n",
    "    F.col(\"registration_date_parsed\").isNull()\n",
    ").count()\n",
    "\n",
    "# Display statistics\n",
    "display(f\"Total records: {total}\")\n",
    "display(f\"Successfully parsed: {parsed} ({(parsed/total)*100:.1f}%)\")\n",
    "display(f\"Failed to parse: {failed} ({(failed/total)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c22dcf5-0d02-4423-b5e3-b51261b45991",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We calculate the effectiveness of the coalesce strategy for date conversion - what percentage of records were successfully parsed. A high failure rate may indicate the need to add other formats to coalesce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "033d41b0-cce0-4f6a-9d71-68b5feca2a39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preview sample date conversions\n",
    "display(df_with_dates.select(\"customer_id\", \"registration_date\", \"registration_date_parsed\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cf10623-3980-4ef5-96bb-484b5e1dffa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The preview shows original date values and their parsed counterparts. This allows for verification of conversion correctness and identification of format issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0684e15-1bfb-443f-a3b5-34440dba84a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preview records with parsing errors (if any)\n",
    "parsing_errors = df_with_dates.filter(\n",
    "    F.col(\"registration_date\").isNotNull() & \n",
    "    F.col(\"registration_date_parsed\").isNull()\n",
    ").select(\"customer_id\", \"registration_date\", \"registration_date_parsed\").limit(5)\n",
    "\n",
    "if parsing_errors.count() > 0:\n",
    "    display(parsing_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d6c6a51-fd7b-4e99-88a7-e7ce00d54898",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Analysis of records with parsing errors helps identify unplanned date formats in source data. This information can be used to extend the coalesce strategy with additional formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09bd7ce6-fbf1-44d0-8f29-d1835d3bbfa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Timestamp Conversion and Time Calculations\n",
    "\n",
    "**Goal:** Convert to timestamp and perform time calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8f3b5d8-50c6-46fe-b0cf-b95d13fde87a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_with_dates\n",
    "# VARIABLE: df_with_timestamp - DataFrame with timestamp and calculations\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp, current_timestamp, current_date, datediff\n",
    "\n",
    "# Convert registration_date_parsed to timestamp (adds time 00:00:00)\n",
    "df_with_timestamp = df_with_dates.withColumn(\n",
    "    \"registration_timestamp\",\n",
    "    F.to_timestamp(F.col(\"registration_date_parsed\"))\n",
    ")\n",
    "\n",
    "# Time calculations\n",
    "df_with_timestamp = df_with_timestamp \\\n",
    "    .withColumn(\"current_date\", current_date()) \\\n",
    "    .withColumn(\"days_since_registration\", \n",
    "        datediff(F.col(\"current_date\"), F.col(\"registration_date_parsed\"))\n",
    "    )\n",
    "\n",
    "# Statistics\n",
    "print(\"=== Time Statistics ===\")\n",
    "df_with_timestamp.select(\n",
    "    F.min(\"days_since_registration\").alias(\"min_days\"),\n",
    "    F.max(\"days_since_registration\").alias(\"max_days\"),\n",
    "    F.avg(\"days_since_registration\").alias(\"avg_days\")\n",
    ").show()\n",
    "\n",
    "# Examples\n",
    "print(\"\\n=== Sample time calculations ===\")\n",
    "display(df_with_timestamp.select(\n",
    "    \"customer_id\",\n",
    "    \"registration_date\",\n",
    "    \"registration_date_parsed\",\n",
    "    \"registration_timestamp\",\n",
    "    \"days_since_registration\"\n",
    ").orderBy(F.desc(\"days_since_registration\")).limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51a50f68-1d2d-4bc6-b7d2-43cf5fb0a916",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We convert DateType to TimestampType using `to_timestamp()`. Timestamp contains date and time information (default 00:00:00 for dates only), which is useful for precise time calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c3dbb1d-0f84-4a5f-b712-92d20f104d63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Time calculations - add current date and days since registration\n",
    "df_with_timestamp = df_with_timestamp \\\n",
    "    .withColumn(\"current_date\", current_date()) \\\n",
    "    .withColumn(\"days_since_registration\", \n",
    "        datediff(F.col(\"current_date\"), F.col(\"registration_date_parsed\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86a2df2c-0ae8-4d31-9706-50a881c63bd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We perform time calculations using `current_date()` and `datediff()`. The `days_since_registration` column shows the \"age\" of each customer account, which is useful for segmentation and cohort analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c26269e9-69d6-4c4b-b39d-f7701f55d43b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Time statistics - min, max, average days since registration\n",
    "time_stats = df_with_timestamp.select(\n",
    "    F.min(\"days_since_registration\").alias(\"min_days\"),\n",
    "    F.max(\"days_since_registration\").alias(\"max_days\"),\n",
    "    F.avg(\"days_since_registration\").alias(\"avg_days\")\n",
    ")\n",
    "\n",
    "display(time_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "460efef6-fa03-40ad-9b8d-455c91268fc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We calculate descriptive statistics for account age - minimum, maximum, and average days since registration. These metrics help understand the demographic profile of the customer base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1ef11d4-ccf6-49b9-b469-a206484c375c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preview sample time calculations\n",
    "display(df_with_timestamp.select(\n",
    "    \"customer_id\",\n",
    "    \"registration_date\",\n",
    "    \"registration_date_parsed\",\n",
    "    \"registration_timestamp\",\n",
    "    \"days_since_registration\"\n",
    ").orderBy(F.desc(\"days_since_registration\")).limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66e32fd0-1b51-4ed6-8171-856ee1a3af6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Preview of records sorted by account age (oldest first) shows all stages of date transformation - from original string, through parsed date, timestamp, to calculated number of days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d796273-f020-436e-bb3c-9ce06ae27354",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Deduplication\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Duplicates are a common data quality problem resulting from errors in source systems, reloading the same data multiple times, or errors in ETL processes. The deduplication strategy depends on the business context.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **dropDuplicates()**: Remove duplicates based on all or selected columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "338e49bb-24f7-4612-a0e5-5320428ee8e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Deduplication - All Columns\n",
    "\n",
    "**Goal:** Remove completely identical records (exact duplicates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc07a589-27b8-4ccf-8bf3-e2ed1e0094b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_distinct - DataFrame without exact duplicates\n",
    "\n",
    "# Remove exact duplicates (all columns identical)\n",
    "df_distinct = df_customers.distinct()\n",
    "\n",
    "# Statistics\n",
    "total = df_customers.count()\n",
    "distinct = df_distinct.count()\n",
    "duplicates = total - distinct\n",
    "\n",
    "print(\"=== Deduplication - all columns ===\")\n",
    "print(f\"Total records: {total}\")\n",
    "print(f\"Unique records: {distinct}\")\n",
    "print(f\"Removed duplicates: {duplicates}\")\n",
    "print(f\"Duplication rate: {(duplicates/total)*100:.1f}%\")\n",
    "\n",
    "# Identify duplicates before removal\n",
    "from pyspark.sql.functions import count as spark_count\n",
    "\n",
    "duplicated_records = df_customers \\\n",
    "    .groupBy(df_customers.columns) \\\n",
    "    .agg(spark_count(\"*\").alias(\"count\")) \\\n",
    "    .filter(F.col(\"count\") > 1) \\\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "\n",
    "if duplicated_records.count() > 0:\n",
    "    print(\"\\n=== Examples of duplicated records ===\")\n",
    "    display(duplicated_records.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e8258f7-c833-49eb-8cb9-cb1c057d5677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We remove completely identical duplicates (exact duplicates) using `distinct()`. This operation compares all columns and keeps only unique records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76fbe0ed-707b-480b-be4c-f9f4881e5cb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate deduplication statistics\n",
    "total = df_customers.count()\n",
    "distinct = df_distinct.count()\n",
    "duplicates = total - distinct\n",
    "duplication_rate = (duplicates/total)*100\n",
    "\n",
    "display(f\"Total records: {total}\")\n",
    "display(f\"Unique records: {distinct}\")\n",
    "display(f\"Removed duplicates: {duplicates}\")\n",
    "display(f\"Duplication rate: {duplication_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbde5368-71c8-4939-8993-a4c22b2eb701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We calculate deduplication statistics - the number of removed duplicates and the duplication rate. These metrics help assess source data quality and the effectiveness of the cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8682a9ba-fcf4-4689-89ae-792623392cad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify duplicates before removal (if any)\n",
    "from pyspark.sql.functions import count as spark_count\n",
    "\n",
    "duplicated_records = df_customers \\\n",
    "    .groupBy(df_customers.columns) \\\n",
    "    .agg(spark_count(\"*\").alias(\"count\")) \\\n",
    "    .filter(F.col(\"count\") > 1) \\\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "\n",
    "if duplicated_records.count() > 0:\n",
    "    display(duplicated_records.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d21bc62-a3b8-4092-93c0-1aa9dff4daff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We identify specific records that are duplicates - grouping by all columns and looking for groups with more than one record. This helps understand the nature of duplicates in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1711638e-b2b3-4c65-987a-925ff3176546",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Deduplication - Key Columns\n",
    "\n",
    "**Goal:** Remove duplicates based on business key (customer_id), keeping the latest record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "697fb1a6-1703-483e-b4d9-355f8fc2e1a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_deduped - DataFrame with duplicates removed per customer_id\n",
    "\n",
    "# Strategy 1: dropDuplicates() per customer_id - keeps the first encountered record\n",
    "df_deduped_simple = df_customers.dropDuplicates([\"customer_id\"])\n",
    "\n",
    "print(\"=== Deduplication per customer_id (simple strategy) ===\")\n",
    "print(f\"Before: {df_customers.count()} records\")\n",
    "print(f\"After: {df_deduped_simple.count()} records\")\n",
    "print(f\"Removed: {df_customers.count() - df_deduped_simple.count()} duplicates\")\n",
    "\n",
    "# Strategy 2: Window function - keep the latest record (if we have a timestamp)\n",
    "# Assuming we have a created_at or other timestamp column\n",
    "\n",
    "if \"created_at\" in df_customers.columns or \"last_updated\" in df_customers.columns:\n",
    "    from pyspark.sql.window import Window\n",
    "    \n",
    "    timestamp_col = \"created_at\" if \"created_at\" in df_customers.columns else \"last_updated\"\n",
    "    \n",
    "    # Window partitioned by customer_id, sorted by timestamp desc\n",
    "    window_spec = Window.partitionBy(\"customer_id\").orderBy(F.desc(timestamp_col))\n",
    "    \n",
    "    df_deduped = df_customers \\\n",
    "        .withColumn(\"row_num\", F.row_number().over(window_spec)) \\\n",
    "        .filter(F.col(\"row_num\") == 1) \\\n",
    "        .drop(\"row_num\")\n",
    "    \n",
    "    print(f\"\\n=== Deduplication per customer_id (strategy: latest record) ===\")\n",
    "    print(f\"Before: {df_customers.count()} records\")\n",
    "    print(f\"After: {df_deduped.count()} records\")\n",
    "    print(f\"Removed: {df_customers.count() - df_deduped.count()} duplicates\")\n",
    "else:\n",
    "    # If no timestamp, use simple strategy\n",
    "    df_deduped = df_deduped_simple\n",
    "    print(\"\\n(No timestamp column - used simple strategy)\")\n",
    "\n",
    "# Identify duplicates before removal\n",
    "duplicate_ids = df_customers \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(spark_count(\"*\").alias(\"count\")) \\\n",
    "    .filter(F.col(\"count\") > 1) \\\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "\n",
    "if duplicate_ids.count() > 0:\n",
    "    print(f\"\\n=== {duplicate_ids.count()} customer_id with duplicates ===\")\n",
    "    display(duplicate_ids.limit(10))\n",
    "    \n",
    "    # Examples of duplicates\n",
    "    sample_duplicate_id = duplicate_ids.first()[\"customer_id\"]\n",
    "    print(f\"\\n=== Example of duplicates for customer_id={sample_duplicate_id} ===\")\n",
    "    display(df_customers.filter(F.col(\"customer_id\") == sample_duplicate_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a3b1a21-88d2-4cff-9a25-d8721de74922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We apply a simple deduplication strategy per business key (`customer_id`) using `dropDuplicates()`. It keeps the first encountered record for each customer_id, which is fast but does not allow selecting the \"best\" record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35025bf0-e70f-4ff6-a72a-b402ed6779af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Deduplication statistics per customer_id\n",
    "records_before = df_customers.count()\n",
    "records_after = df_deduped_simple.count()\n",
    "records_removed = records_before - records_after\n",
    "\n",
    "display(f\"Records before: {records_before}\")\n",
    "display(f\"Records after: {records_after}\")\n",
    "display(f\"Removed duplicates: {records_removed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4afa9b74-4c77-4d6f-941e-c69977e0341c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We calculate the effectiveness of deduplication per business key - how many records were removed due to customer_id duplicates. This is a key metric for assessing data quality and business key uniqueness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a564b0b-a641-440e-87b7-39e634d32224",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Standardization\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Standardization involves unifying data formats and representations according to established business rules. Non-standard data (different case, whitespace, formats) hinders analysis, joins, and aggregations.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Text standardization**: trim(), lower(), upper(), initcap()\n",
    "- **Pattern standardization**: regexp_replace() for codes, phones\n",
    "- **Format standardization**: Unifying date and address formats\n",
    "- **Categorical standardization**: Mapping variants to standard values\n",
    "\n",
    "**Practical Application:**\n",
    "- Unifying case in text fields\n",
    "- Removing whitespace from beginning and end\n",
    "- Standardizing country codes, phone numbers, zip codes\n",
    "- Consolidating category variants (Active/active/ACTIVE → Active)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f80f07b2-120c-4d91-a4af-cfcb6a264dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Text Standardization\n",
    "\n",
    "**Goal:** Clean and standardize text fields (trim, case, whitespace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3edc8e5-f85a-4676-af98-ef0411ba67ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_standardized - DataFrame with standardized text fields\n",
    "\n",
    "# Standardize text fields - remove whitespace\n",
    "df_standardized = df_customers\n",
    "\n",
    "# Trim whitespace from all string columns\n",
    "for col_name in [\"first_name\", \"last_name\", \"email\", \"phone\", \"city\", \"state\", \"country\"]:\n",
    "    if col_name in df_standardized.columns:\n",
    "        df_standardized = df_standardized.withColumn(\n",
    "            col_name,\n",
    "            F.trim(F.col(col_name))\n",
    "        )\n",
    "\n",
    "# 2. Standardize specific columns\n",
    "# first_name, last_name: Title Case\n",
    "df_standardized = df_standardized.withColumn(\n",
    "    \"first_name\",\n",
    "    F.initcap(F.col(\"first_name\"))\n",
    ").withColumn(\n",
    "    \"last_name\", \n",
    "    F.initcap(F.col(\"last_name\"))\n",
    ")\n",
    "\n",
    "# email: lowercase (standard for emails)\n",
    "df_standardized = df_standardized.withColumn(\n",
    "    \"email\",\n",
    "    F.lower(F.col(\"email\"))\n",
    ")\n",
    "\n",
    "# country: uppercase (ISO standard for country codes)\n",
    "df_standardized = df_standardized.withColumn(\n",
    "    \"country\",\n",
    "    F.upper(F.col(\"country\"))\n",
    ")\n",
    "\n",
    "# city: Title Case\n",
    "df_standardized = df_standardized.withColumn(\n",
    "    \"city\",\n",
    "    F.initcap(F.col(\"city\"))\n",
    ")\n",
    "\n",
    "# Comparison before and after\n",
    "print(\"=== Standardization Comparison ===\")\n",
    "display(df_customers.select(\"first_name\", \"last_name\", \"email\", \"city\", \"country\").limit(5))\n",
    "print(\"\\n↓↓↓ AFTER STANDARDIZATION ↓↓↓\\n\")\n",
    "display(df_standardized.select(\"first_name\", \"last_name\", \"email\", \"city\", \"country\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "913937df-ee00-4bce-8920-5c76a0503551",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We remove whitespace (spaces, tabs) from the beginning and end of strings using `trim()`. This is a basic standardization step that eliminates accidental spaces that can interfere with analysis and joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0d61108-eb27-4c63-9054-9e421beb9953",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Standardize names - Title Case\n",
    "df_standardized = df_standardized.withColumn(\n",
    "    \"first_name\",\n",
    "    F.initcap(F.col(\"first_name\"))\n",
    ").withColumn(\n",
    "    \"last_name\", \n",
    "    F.initcap(F.col(\"last_name\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c748332a-58f8-4b81-aecc-2dad1721ed4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We standardize first and last names to Title Case using `initcap()` - first letter uppercase, others lowercase. This is standard for proper name fields, ensuring uniform formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afcd7829-c779-4b64-ae60-b3a2c6d4365f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Standardize emails (lowercase) and countries (uppercase)\n",
    "df_standardized = df_standardized.withColumn(\n",
    "    \"email\",\n",
    "    F.lower(F.col(\"email\"))\n",
    ").withColumn(\n",
    "    \"country\",\n",
    "    F.upper(F.col(\"country\"))\n",
    ").withColumn(\n",
    "    \"city\",\n",
    "    F.initcap(F.col(\"city\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b626829-34db-4210-9d1d-841f51f2ebe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We apply different case conventions for different data types: email `lowercase` (internet standard), country `uppercase` (ISO codes), city `Title Case` (place names). Each type has its justified conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15fc2c7b-0540-48f9-9625-2a127f105e5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Comparison before standardization\n",
    "display(\"BEFORE Standardization:\")\n",
    "display(df_customers.select(\"first_name\", \"last_name\", \"email\", \"city\", \"country\").limit(5))\n",
    "\n",
    "display(\"AFTER Standardization:\")\n",
    "display(df_standardized.select(\"first_name\", \"last_name\", \"email\", \"city\", \"country\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63b0c0b9-d208-48f5-a115-ae2004577e88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Comparison before and after standardization shows the effect of the transformation - unified formatting, removed whitespace, and consistent case conventions. This is crucial for ensuring analytical data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68584674-f31c-4315-b93f-6027094167c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Code and Category Standardization\n",
    "\n",
    "**Goal:** Unify code formats and map category variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "168546b9-4a72-4645-a8a3-20dfda790e0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_standardized\n",
    "# VARIABLE: df_codes_standardized - DataFrame with standardized codes\n",
    "\n",
    "df_codes_standardized = df_standardized\n",
    "\n",
    "# 1. Standardize phone numbers (remove all non-digits, international format)\n",
    "if \"phone\" in df_codes_standardized.columns:\n",
    "    df_codes_standardized = df_codes_standardized.withColumn(\n",
    "        \"phone_standardized\",\n",
    "        F.when(F.col(\"phone\").isNotNull(),\n",
    "            F.concat(\n",
    "                F.when(F.col(\"phone\").startswith(\"+\"), \"\")\n",
    "                 .otherwise(\"+1-\"),  # Default prefix for USA\n",
    "                F.regexp_replace(F.col(\"phone\"), \"[^0-9]\", \"\")\n",
    "            )\n",
    "        ).otherwise(F.col(\"phone\"))\n",
    "    )\n",
    "\n",
    "# 2. Standardize customer_segment (consistent naming)\n",
    "if \"customer_segment\" in df_codes_standardized.columns:\n",
    "    df_codes_standardized = df_codes_standardized.withColumn(\n",
    "        \"customer_segment_standardized\",\n",
    "        F.when(F.upper(F.trim(F.col(\"customer_segment\"))) == \"PREMIUM\", \"Premium\")\n",
    "         .when(F.upper(F.trim(F.col(\"customer_segment\"))) == \"STANDARD\", \"Standard\")\n",
    "         .when(F.upper(F.trim(F.col(\"customer_segment\"))) == \"BASIC\", \"Basic\")\n",
    "         .otherwise(\"Unknown\")\n",
    "    )\n",
    "\n",
    "# 3. Standardize country codes (3-letter ISO codes as example)\n",
    "df_codes_standardized = df_codes_standardized.withColumn(\n",
    "    \"country_iso\",\n",
    "    F.when(F.upper(F.col(\"country\")) == \"USA\", \"USA\")\n",
    "     .when(F.upper(F.col(\"country\")) == \"POLAND\", \"POL\")\n",
    "     .when(F.upper(F.col(\"country\")) == \"GERMANY\", \"DEU\")\n",
    "     .when(F.upper(F.col(\"country\")) == \"FRANCE\", \"FRA\")\n",
    "     .otherwise(F.upper(F.col(\"country\")))\n",
    ")\n",
    "\n",
    "# Verify standardization\n",
    "print(\"=== Code and Category Standardization ===\")\n",
    "\n",
    "if \"phone\" in df_codes_standardized.columns:\n",
    "    print(\"\\n--- Phone numbers ---\")\n",
    "    display(df_codes_standardized.select(\"phone\", \"phone_standardized\").limit(5))\n",
    "\n",
    "if \"customer_segment\" in df_codes_standardized.columns:\n",
    "    print(\"\\n--- Customer segments ---\")\n",
    "    display(df_codes_standardized.groupBy(\"customer_segment\", \"customer_segment_standardized\").count().orderBy(\"customer_segment\"))\n",
    "\n",
    "print(\"\\n--- Country codes ---\")\n",
    "display(df_codes_standardized.groupBy(\"country\", \"country_iso\").count().orderBy(\"country\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "629e9ca1-ea24-493e-82ad-133cbce3fb72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "1. **Data profiling first**: Always analyze data before starting cleaning\n",
    "2. **Context matters**: Cleaning strategy depends on business context\n",
    "3. **Validation is critical**: Always validate conversion and transformation results\n",
    "4. **Document decisions**: Log statistics and decisions for auditability\n",
    "\n",
    "### Key Takeaways:\n",
    "- Loaded data from dataset/ (using DATASET_BASE_PATH)\n",
    "- Data profiling and quality issue identification\n",
    "- Null value handling (fillna, dropna, coalesce)\n",
    "- Type validation and conversion (cast, to_date, to_timestamp)\n",
    "- Record deduplication (distinct, dropDuplicates, window functions)\n",
    "- Text and code standardization (trim, case, regexp_replace)\n",
    "- PySpark vs SQL approach comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6da07bba-6574-4094-955d-5e3b2e1f2347",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Clean up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a480f43-f0b1-4b84-a13a-fab63379f2a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optional test resource cleanup\n",
    "# WARNING: Run only if you want to remove all created data\n",
    "\n",
    "# Remove temporary views\n",
    "spark.catalog.dropTempView(\"customers_raw\")\n",
    "\n",
    "# Clear cache\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "print(\"Temporary views and cache have been cleared\")\n",
    "print(\"Source data in Volume remains intact\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_data_quality",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
