{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27d8de6a-6ffc-4a65-a1b0-1890f3387beb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Quality and Cleaning\n",
    "\n",
    "**Training Objective:** Understand techniques for identifying and resolving data quality issues, understand strategies for handling null values, type validation, deduplication, and data standardization.\n",
    "\n",
    "**Topics Covered:**\n",
    "- Handling null values\n",
    "- Type validation\n",
    "- Deduplication\n",
    "- Standardization\n",
    "- Common quality issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Introduction\n",
    "\n",
    "**Section Objective:** Understand the foundations of data quality and data cleansing techniques.\n",
    "\n",
    "**Basic Concepts:**\n",
    "- **Data Quality**: A measure of data suitability for its intended purpose.\n",
    "- **Data Cleansing**: The process of identifying and correcting errors in data.\n",
    "- **Data Validation**: Verification of data compliance with business rules.\n",
    "- **Data Standardization**: Unification of data formats and representation.\n",
    "- **Data Profiling**: Analysis of structure, content, and relationships in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Configuration\n",
    "\n",
    "We import necessary libraries and set up the context (Catalog and Schema) to ensure we are working in our isolated environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Set default catalog and schema\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "# Display user context\n",
    "display(spark.createDataFrame([\n",
    "    (\"Catalog\", CATALOG),\n",
    "    (\"Bronze Schema\", BRONZE_SCHEMA),\n",
    "    (\"Silver Schema\", SILVER_SCHEMA),\n",
    "    (\"Gold Schema\", GOLD_SCHEMA),\n",
    "    (\"User\", raw_user)\n",
    "], [\"Parameter\", \"Value\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "We load customer data from a CSV file using the `DATASET_BASE_PATH` defined in `00_setup.ipynb`. We use `inferSchema` to automatically detect data types, which is useful for exploration but should be used with caution in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to file in dataset\n",
    "customers_path = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "\n",
    "# Load data with schema inference\n",
    "df_customers = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(customers_path)\n",
    "\n",
    "# Display basic info\n",
    "print(f\"Total Records: {df_customers.count()}\")\n",
    "df_customers.printSchema()\n",
    "display(df_customers.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Profiling\n",
    "\n",
    "Before cleaning, we must understand the quality of our data. We analyze completeness, uniqueness, and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Completeness - Null Value Analysis\n",
    "\n",
    "We calculate the number and percentage of null values for each column.\n",
    "*Note: We use an optimized aggregation approach to scan the data only once, rather than iterating through columns.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate null counts for all columns in one pass\n",
    "# count(when(col.isNull(), 1)) counts the null occurrences\n",
    "null_counts_row = df_customers.select([\n",
    "    F.count(F.when(F.col(c).isNull(), 1)).alias(c) \n",
    "    for c in df_customers.columns\n",
    "]).collect()[0].asDict()\n",
    "\n",
    "# Prepare data for display\n",
    "total_count = df_customers.count()\n",
    "null_stats = [(c, v, (v/total_count)*100) for c, v in null_counts_row.items()]\n",
    "\n",
    "df_nulls = spark.createDataFrame(null_stats, [\"Column\", \"Null Count\", \"Null Pct\"])\n",
    "display(df_nulls.orderBy(F.desc(\"Null Count\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Uniqueness - Duplicate Analysis\n",
    "\n",
    "We check how many unique rows exist compared to the total count to identify full-row duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate unique rows\n",
    "unique_count = df_customers.distinct().count()\n",
    "\n",
    "display(spark.createDataFrame([\n",
    "    (\"Total Rows\", total_count),\n",
    "    (\"Unique Rows\", unique_count),\n",
    "    (\"Duplicate Rows\", total_count - unique_count)\n",
    "], [\"Metric\", \"Value\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Null Values\n",
    "\n",
    "Strategies for handling missing data:\n",
    "1.  **Fill**: Replace nulls with default values.\n",
    "2.  **Drop**: Remove records with missing critical keys.\n",
    "3.  **Coalesce**: Fallback to alternative columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 1: Fill with Defaults\n",
    "\n",
    "We replace null values in non-critical columns with placeholders like \"Unknown\" or \"no phone\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_values = {\n",
    "    \"phone\": \"no phone\",\n",
    "    \"city\": \"Unknown\", \n",
    "    \"state\": \"Unknown\",\n",
    "    \"country\": \"Unknown\"\n",
    "}\n",
    "\n",
    "df_filled = df_customers.fillna(fill_values)\n",
    "\n",
    "# Verify: Check records that were originally null\n",
    "display(df_filled.filter(F.col(\"city\") == \"Unknown\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 2: Drop Records\n",
    "\n",
    "For critical columns like `customer_id`, missing values might render the record useless. In such cases, we drop the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer_id is mandatory for our business logic\n",
    "df_valid = df_customers.dropna(subset=[\"customer_id\"])\n",
    "\n",
    "display(spark.createDataFrame([\n",
    "    (\"Original Count\", df_customers.count()),\n",
    "    (\"Valid Count\", df_valid.count()),\n",
    "    (\"Dropped Count\", df_customers.count() - df_valid.count())\n",
    "], [\"Metric\", \"Value\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 3: Coalesce (Fallback)\n",
    "\n",
    "We can create a new column that takes the first non-null value from a list of columns. Here, we create a `primary_contact`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create primary_contact from email OR phone OR default\n",
    "df_with_contact = df_customers.withColumn(\n",
    "    \"primary_contact\",\n",
    "    F.coalesce(F.col(\"email\"), F.col(\"phone\"), F.lit(\"no contact\"))\n",
    ")\n",
    "\n",
    "display(df_with_contact.select(\"customer_id\", \"email\", \"phone\", \"primary_contact\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type Validation and Conversion\n",
    "\n",
    "We validate data types and formats, specifically dates and emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date and Email Validation Logic\n",
    "\n",
    "We define the logic to parse dates and validate email formats using Regular Expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We try to parse dates and flag invalid ones\n",
    "df_typed = df_customers.withColumn(\n",
    "    \"registration_date_parsed\",\n",
    "    F.to_date(F.col(\"registration_date\"), \"yyyy-MM-dd\")\n",
    ").withColumn(\n",
    "    \"registration_date_valid\",\n",
    "    (F.col(\"registration_date_parsed\").isNotNull()) & \n",
    "    (F.col(\"registration_date_parsed\") >= \"2020-01-01\") & \n",
    "    (F.col(\"registration_date_parsed\") <= \"2026-12-31\")\n",
    ").withColumn(\n",
    "    \"email_valid\",\n",
    "    F.col(\"email\").rlike(\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Statistics\n",
    "\n",
    "We calculate how many records passed our validation rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Statistics\n",
    "valid_dates = df_typed.filter(F.col(\"registration_date_valid\") == True).count()\n",
    "valid_emails = df_typed.filter(F.col(\"email_valid\") == True).count()\n",
    "\n",
    "display(spark.createDataFrame([\n",
    "    (\"Valid Dates\", valid_dates),\n",
    "    (\"Valid Emails\", valid_emails)\n",
    "], [\"Metric\", \"Count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Date Parsing\n",
    "\n",
    "Real-world data often contains mixed date formats. We can use `coalesce` to try multiple formats in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using coalesce to try multiple date formats\n",
    "df_with_dates = df_customers.withColumn(\n",
    "    \"registration_date_parsed\",\n",
    "    F.coalesce(\n",
    "        F.to_date(F.col(\"registration_date\"), \"yyyy-MM-dd\"),\n",
    "        F.to_date(F.col(\"registration_date\"), \"dd/MM/yyyy\"),\n",
    "        F.to_date(F.col(\"registration_date\"), \"MM-dd-yyyy\"),\n",
    "        F.to_date(F.col(\"registration_date\")) # Spark auto-detect fallback\n",
    "    )\n",
    ")\n",
    "\n",
    "display(df_with_dates.select(\"registration_date\", \"registration_date_parsed\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deduplication\n",
    "\n",
    "Removing duplicates is essential for data integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Exact Duplicates\n",
    "\n",
    "We remove rows where **all** columns are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distinct = df_customers.distinct()\n",
    "print(f\"Distinct count: {df_distinct.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Key-based Deduplication\n",
    "\n",
    "We remove duplicates based on a specific key (e.g., `customer_id`), keeping only the first occurrence found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeps the first occurrence found\n",
    "df_deduped = df_customers.dropDuplicates([\"customer_id\"])\n",
    "\n",
    "display(spark.createDataFrame([\n",
    "    (\"Total Records\", df_customers.count()),\n",
    "    (\"Distinct Records\", df_distinct.count()),\n",
    "    (\"Unique Customers\", df_deduped.count())\n",
    "], [\"Metric\", \"Value\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "Unifying text formats (case, whitespace) and codes (phone, country)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Standardization\n",
    "\n",
    "We trim whitespace, capitalize names, and lowercase emails to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim whitespace and apply casing rules\n",
    "df_standardized = df_customers.select(\n",
    "    *[F.trim(F.col(c)).alias(c) if c in [\"first_name\", \"last_name\", \"city\", \"email\"] else F.col(c) for c in df_customers.columns]\n",
    ").withColumn(\n",
    "    \"first_name\", F.initcap(F.col(\"first_name\"))\n",
    ").withColumn(\n",
    "    \"last_name\", F.initcap(F.col(\"last_name\"))\n",
    ").withColumn(\n",
    "    \"email\", F.lower(F.col(\"email\"))\n",
    ").withColumn(\n",
    "    \"country\", F.upper(F.col(\"country\"))\n",
    ")\n",
    "\n",
    "display(df_standardized.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Standardization\n",
    "\n",
    "We clean phone numbers (removing non-digits) and standardize country codes to ISO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean phone numbers and standardize country codes\n",
    "df_codes = df_standardized.withColumn(\n",
    "    \"phone_clean\",\n",
    "    F.regexp_replace(F.col(\"phone\"), \"[^0-9]\", \"\") # Keep digits only\n",
    ").withColumn(\n",
    "    \"country_iso\",\n",
    "    F.when(F.col(\"country\") == \"USA\", \"USA\")\n",
    "     .when(F.col(\"country\") == \"POLAND\", \"POL\")\n",
    "     .otherwise(F.col(\"country\"))\n",
    ")\n",
    "\n",
    "display(df_codes.select(\"phone\", \"phone_clean\", \"country\", \"country_iso\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Standardization (Company Names)\n",
    "\n",
    "Company names often appear in various formats (e.g., \"Dad & Sons\", \"Dad and Sons\", \"dad&sons\"). We can use regular expressions to normalize them into a standard format for better matching and aggregation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data with messy company names\n",
    "# We simulate common data entry variations\n",
    "data = [\n",
    "    (1, \"Dad & Sons\"),\n",
    "    (2, \"Dad And Sons\"),\n",
    "    (3, \"Dad Sons\"),\n",
    "    (4, \"dad&sons\"),\n",
    "    (5, \"Dad  &  Sons\"),\n",
    "    (6, \"Dad-and-Sons\"),\n",
    "    (7, \"DadnSons\") # Hard case: missing delimiters\n",
    "]\n",
    "df_companies = spark.createDataFrame(data, [\"id\", \"company_name\"])\n",
    "\n",
    "# Standardization logic\n",
    "df_normalized = df_companies.withColumn(\n",
    "    \"company_normalized\",\n",
    "    F.lower(F.col(\"company_name\"))\n",
    ").withColumn(\n",
    "    # Replace '&' or '+' with ' and '\n",
    "    \"company_normalized\",\n",
    "    F.regexp_replace(F.col(\"company_normalized\"), \"[&\\\\+]\", \" and \")\n",
    ").withColumn(\n",
    "    # Replace non-alphanumeric characters with space\n",
    "    \"company_normalized\",\n",
    "    F.regexp_replace(F.col(\"company_normalized\"), \"[^a-z0-9]\", \" \")\n",
    ").withColumn(\n",
    "    # Collapse multiple spaces\n",
    "    \"company_normalized\",\n",
    "    F.regexp_replace(F.col(\"company_normalized\"), \"\\\\s+\", \" \")\n",
    ").withColumn(\n",
    "    \"company_normalized\",\n",
    "    F.trim(F.col(\"company_normalized\"))\n",
    ")\n",
    "\n",
    "display(df_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **Profiling**: Analyzed nulls and duplicates.\n",
    "2.  **Null Handling**: Used `fillna`, `dropna`, and `coalesce`.\n",
    "3.  **Validation**: Validated dates and emails.\n",
    "4.  **Deduplication**: Removed exact and key-based duplicates.\n",
    "5.  **Standardization**: Cleaned text and codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear cache if used\n",
    "spark.catalog.clearCache()\n",
    "print(\"Cleanup completed.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_data_quality",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
