{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Delta Lake Optimization\n",
        "\n",
        "**Training Objective:** Optimize Delta Lake performance and understand Change Data Feed.\n",
        "\n",
        "**Topics Covered:**\n",
        "- Small Files Problem & OPTIMIZE\n",
        "- Partitioning & Z-ORDER\n",
        "- Liquid Clustering\n",
        "- Change Data Feed (CDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "8f0b53e1-b8ba-4236-9ead-c2a298a15d12",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "## Theoretical Introduction\n",
        "\n",
        "As data grows, query performance can degrade due to several factors:\n",
        "- **Small Files Problem**: Too many small files increase metadata overhead\n",
        "- **Data Layout**: Data not organized for common query patterns\n",
        "- **Predicate Pushdown Inefficiency**: Scanning more data than necessary\n",
        "\n",
        "Delta Lake provides several optimization techniques:\n",
        "\n",
        "| Technique | Description | When to Use |\n",
        "|-----------|-------------|-------------|\n",
        "| **OPTIMIZE** | Compacts small files into larger ones | After many small writes |\n",
        "| **Partitioning** | Physical data separation by column values | High-cardinality filter columns |\n",
        "| **Z-ORDER** | Co-locates related data for better pruning | Frequently filtered columns |\n",
        "| **Liquid Clustering** | Modern alternative to partitioning + Z-ORDER | New tables (recommended) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## User Isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b34f1ea4-5277-4844-b864-01eea5e24f5c",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "%run ../00_setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "45a8db5a-a1d1-44f3-8175-c71f3e42a257",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Display user context\n",
        "display(\n",
        "    spark.createDataFrame([\n",
        "        (CATALOG, BRONZE_SCHEMA, SILVER_SCHEMA, GOLD_SCHEMA)\n",
        "    ], ['catalog', 'bronze_schema', 'silver_schema', 'gold_schema'])\n",
        ")\n",
        "\n",
        "# Set catalog and schema as default\n",
        "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
        "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1345ad36-163c-49af-915f-f674f595d19a",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "## Example: The Small Files Problem\n",
        "\n",
        "**Objective:** Demonstrate how many small files impact performance and how OPTIMIZE solves it\n",
        "\n",
        "The \"small files problem\" occurs when:\n",
        "- Streaming jobs write many small files\n",
        "- Frequent small batch inserts\n",
        "- High-concurrency writes\n",
        "\n",
        "This leads to:\n",
        "- Increased metadata overhead\n",
        "- Slower listing of files\n",
        "- Inefficient I/O (opening/closing many files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a6256554-625c-4264-ab8a-594cb08d15d8",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Create a table with many small files (simulating streaming ingestion)\n",
        "spark.sql(f\"\"\"\n",
        "CREATE OR REPLACE TABLE {CATALOG}.{BRONZE_SCHEMA}.small_files_demo (\n",
        "    id INT,\n",
        "    data STRING,\n",
        "    created_at TIMESTAMP\n",
        ") USING DELTA\n",
        "\"\"\")\n",
        "\n",
        "# Insert data in many small batches (simulating streaming)\n",
        "from pyspark.sql.functions import lit, current_timestamp\n",
        "import random\n",
        "import string\n",
        "\n",
        "print(\"Inserting 500 small batches to simulate streaming ingestion...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c9850648-1202-4a18-aca1-1d548d6d8de9",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import lit, expr, current_timestamp\n",
        "import random\n",
        "\n",
        "# 1. Configuration\n",
        "total_files = 5000\n",
        "rows_per_file = 2  # Average 2 records per file\n",
        "total_rows = total_files * rows_per_file\n",
        "\n",
        "# 2. Generate data in memory (no Python loop!)\n",
        "df = (\n",
        "    spark.range(0, total_rows)\n",
        "    .withColumn(\"id\", lit(random.randint(1, 100)))\n",
        "    .withColumn(\"data\", expr(\"uuid()\"))\n",
        "    .withColumn(\"created_at\", current_timestamp())\n",
        ")\n",
        "\n",
        "# 3. Write with forced number of files\n",
        "df.repartition(total_files).write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"append\") \\\n",
        "    .saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.small_files_demo\")\n",
        "\n",
        "print(f\"Done! Created {total_files} small files in a single transaction.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0ad418a3-a05a-4169-937b-764adec9f270",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.small_files_demo\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "587edefa-9b2c-4f74-85ed-1ada3b3a34ad",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Check the number of files BEFORE optimization\n",
        "print(\"=== BEFORE OPTIMIZE ===\")\n",
        "before_optimize = spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.small_files_demo\")\n",
        "display(before_optimize.select(\"numFiles\", \"sizeInBytes\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2eb16366-c750-4f05-a19b-9e1e32386f80",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Run OPTIMIZE to compact small files\n",
        "optimize_result = spark.sql(f\"\"\"\n",
        "    OPTIMIZE {CATALOG}.{BRONZE_SCHEMA}.small_files_demo\n",
        "\"\"\")\n",
        "\n",
        "display(optimize_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3dc2477f-5eb4-4dd4-8e3f-ab8ebd64f147",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Check the number of files AFTER optimization\n",
        "print(\"=== AFTER OPTIMIZE ===\")\n",
        "after_optimize = spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.small_files_demo\")\n",
        "display(after_optimize.select(\"numFiles\", \"sizeInBytes\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d3c1074b-5e35-4225-9c0d-3eaf081d9ec7",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "## Example: Partitioning\n",
        "\n",
        "**Objective:** Demonstrate how partitioning improves query performance for filtered queries\n",
        "\n",
        "Partitioning physically separates data into directories based on column values. This enables:\n",
        "- **Partition Pruning**: Skip entire partitions that don't match the filter\n",
        "- **Parallel Processing**: Process partitions independently\n",
        "- **Efficient Deletes/Updates**: Only touch affected partitions\n",
        "\n",
        "**Best Practices:**\n",
        "- Use low-cardinality columns (e.g., date, country, status)\n",
        "- Avoid over-partitioning (too many small partitions)\n",
        "- Consider partition size: aim for 1GB+ per partition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "dabece7c-5bfd-4acc-ae02-dd6df5696a89",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Create a partitioned table\n",
        "spark.sql(f\"\"\"\n",
        "CREATE OR REPLACE TABLE {CATALOG}.{BRONZE_SCHEMA}.orders_partitioned (\n",
        "    order_id STRING,\n",
        "    customer_id STRING,\n",
        "    product_id STRING,\n",
        "    order_date DATE,\n",
        "    amount DOUBLE,\n",
        "    status STRING\n",
        ") \n",
        "USING DELTA\n",
        "PARTITIONED BY (order_date)\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f8cc00cb-7a27-4b51-bf87-0d61480c5923",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Insert sample data across multiple dates\n",
        "from datetime import date, timedelta\n",
        "\n",
        "orders_data = []\n",
        "base_date = date(2024, 1, 1)\n",
        "\n",
        "for day_offset in range(30):  # 30 days of data\n",
        "    order_date = base_date + timedelta(days=day_offset)\n",
        "    for i in range(100):  # 100 orders per day\n",
        "        orders_data.append((\n",
        "            f\"ORD-{day_offset:02d}-{i:04d}\",\n",
        "            f\"CUST{i % 50:04d}\",\n",
        "            f\"PROD{i % 20:03d}\",\n",
        "            order_date,\n",
        "            round(50 + (i * 2.5), 2),\n",
        "            \"completed\" if i % 3 != 0 else \"pending\"\n",
        "        ))\n",
        "\n",
        "orders_df = spark.createDataFrame(orders_data, \n",
        "    [\"order_id\", \"customer_id\", \"product_id\", \"order_date\", \"amount\", \"status\"])\n",
        "\n",
        "orders_df.write.format(\"delta\").mode(\"append\").partitionBy(\"order_date\") \\\n",
        "    .saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.orders_partitioned\")\n",
        "\n",
        "print(f\"Inserted {len(orders_data)} orders across 30 days\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3c53cfda-d7e7-4b47-9f7c-09dc33e9a373",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Check partitioning structure\n",
        "display(spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.orders_partitioned\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6a6061d1-68a1-4ae1-ba9d-0fb8ea7b2c6c",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Query with partition filter - only scans relevant partitions\n",
        "# Check the Spark UI to see partition pruning in action\n",
        "result = spark.sql(f\"\"\"\n",
        "    SELECT * FROM {CATALOG}.{BRONZE_SCHEMA}.orders_partitioned\n",
        "    WHERE order_date = '2024-01-15'\n",
        "\"\"\")\n",
        "\n",
        "print(\"Query for single date (should scan only 1 partition):\")\n",
        "display(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "05513694-7990-402a-8be1-0c1f048b9011",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "## Example: Z-ORDER (Data Skipping)\n",
        "\n",
        "**Objective:** Demonstrate how Z-ORDER improves query performance by co-locating related data\n",
        "\n",
        "Z-ORDER is a multi-dimensional clustering technique that:\n",
        "- Co-locates related data in the same files\n",
        "- Enables efficient data skipping based on file-level statistics\n",
        "- Works best with high-cardinality columns used in filters\n",
        "\n",
        "**When to use Z-ORDER:**\n",
        "- Columns frequently used in WHERE clauses\n",
        "- Columns with high cardinality\n",
        "- Can specify up to 4 columns (effectiveness decreases with more)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "bfd8d912-6954-4372-a566-afce479e0765",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{BRONZE_SCHEMA}.sales_zorder_demo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1725469c-75da-4371-9945-05a7b913e06e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Create a table for Z-ORDER demonstration with auto-optimization disabled\n",
        "spark.sql(f\"\"\"\n",
        "CREATE OR REPLACE TABLE {CATALOG}.{BRONZE_SCHEMA}.sales_zorder_demo (\n",
        "    sale_id STRING,\n",
        "    customer_id STRING,\n",
        "    product_id STRING,\n",
        "    store_id STRING,\n",
        "    sale_date DATE,\n",
        "    amount DOUBLE,\n",
        "    quantity INT\n",
        ") USING DELTA\n",
        "TBLPROPERTIES (\n",
        "    delta.autoOptimize.optimizeWrite = false,\n",
        "    delta.autoOptimize.autoCompact = false\n",
        ")\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "9f5a7ef8-4e00-4e5b-89eb-35f22e130f7f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Insert sample data\n",
        "from datetime import date\n",
        "import random\n",
        "\n",
        "sales_data = []\n",
        "for i in range(100000):  # 100K records\n",
        "    sales_data.append((\n",
        "        f\"SALE-{i:08d}\",\n",
        "        f\"CUST{random.randint(1, 1000):04d}\",\n",
        "        f\"PROD{random.randint(1, 500):03d}\",\n",
        "        f\"STORE{random.randint(1, 50):02d}\",\n",
        "        date(2024, random.randint(1, 12), random.randint(1, 28)),\n",
        "        round(random.uniform(10, 500), 2),\n",
        "        random.randint(1, 10)\n",
        "    ))\n",
        "\n",
        "sales_df = spark.createDataFrame(\n",
        "    sales_data, \n",
        "    [\"sale_id\", \"customer_id\", \"product_id\", \"store_id\", \"sale_date\", \"amount\", \"quantity\"]\n",
        ")\n",
        "\n",
        "sales_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\n",
        "    f\"{CATALOG}.{BRONZE_SCHEMA}.sales_zorder_demo\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b0c57206-2887-4e0b-bf16-6f7dd39f4e64",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "display(sales_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "34538db8-8523-4d31-8df0-05c55b550a9a",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Check file statistics BEFORE Z-ORDER\n",
        "print(\"=== BEFORE Z-ORDER ===\")\n",
        "display(spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.sales_zorder_demo\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3b08a9d5-0864-483d-b062-aaa717611bf3",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Queries filtering before Z-Order\n",
        "result = spark.sql(f\"\"\"\n",
        "    SELECT * FROM {CATALOG}.{BRONZE_SCHEMA}.sales_zorder_demo\n",
        "    WHERE customer_id = 'CUST0274' AND product_id = 'PROD367'\n",
        "\"\"\")\n",
        "\n",
        "display(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2c8bf255-a8b8-400a-8eda-3be7322576e6",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Apply Z-ORDER on frequently filtered columns\n",
        "# In this case: customer_id and product_id are common filter columns\n",
        "zorder_result = spark.sql(f\"\"\"\n",
        "    OPTIMIZE {CATALOG}.{BRONZE_SCHEMA}.sales_zorder_demo\n",
        "    ZORDER BY (customer_id, product_id)\n",
        "\"\"\")\n",
        "\n",
        "display(zorder_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e73adeb6-a92e-4144-8478-cb95b367cd70",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Example query that benefits from Z-ORDER\n",
        "result = spark.sql(f\"\"\"\n",
        "    SELECT * FROM {CATALOG}.{BRONZE_SCHEMA}.sales_zorder_demo\n",
        "    WHERE customer_id = 'CUST0274' AND product_id = 'PROD367'\n",
        "\"\"\")\n",
        "\n",
        "print(\"Query with Z-ORDER optimized columns (check Spark UI for data skipping):\")\n",
        "display(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0f071fa3-0b05-47d8-a066-9474797353a8",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "## Example: Liquid Clustering (Modern Approach)\n",
        "\n",
        "**Objective:** Introduce Liquid Clustering as a modern replacement for partitioning and Z-ORDER\n",
        "\n",
        "**Liquid Clustering** is Databricks' newest optimization technique that:\n",
        "- Automatically manages data layout\n",
        "- Adapts to changing query patterns\n",
        "- Eliminates need for manual partitioning decisions\n",
        "- Works incrementally (no need to re-cluster entire table)\n",
        "\n",
        "**Key Benefits:**\n",
        "- No upfront partitioning decisions required\n",
        "- Can change clustering columns without rewriting data\n",
        "- Better performance for evolving workloads\n",
        "- Simpler to manage than partitioning + Z-ORDER combo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d567b3a1-434c-43ec-bcf5-a3439e719281",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{BRONZE_SCHEMA}.sales_liquid_clustering\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4e47153b-1cdf-4d55-bea2-9d35c2b183c0",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Create a table with Liquid Clustering and auto-optimization disabled\n",
        "spark.sql(f\"\"\"\n",
        "CREATE OR REPLACE TABLE {CATALOG}.{BRONZE_SCHEMA}.sales_liquid_clustering (\n",
        "    sale_id STRING,\n",
        "    customer_id STRING,\n",
        "    product_id STRING,\n",
        "    region STRING,\n",
        "    sale_date DATE,\n",
        "    amount DOUBLE,\n",
        "    quantity INT\n",
        ") \n",
        "USING DELTA\n",
        "TBLPROPERTIES (\n",
        "    delta.autoOptimize.optimizeWrite = false,\n",
        "    delta.autoOptimize.autoCompact = false\n",
        ")\n",
        "\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "dae3322d-fb0b-45ba-abaf-cb54a5c24c6d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, rand, lit, concat, lpad, element_at, array, date_add, to_date, round\n",
        "\n",
        "# 1. Configuration\n",
        "target_files = 5000       # We want 5000 files\n",
        "rows_per_file = 10        # 10 records per file\n",
        "total_rows = target_files * rows_per_file # Total 50,000 records\n",
        "\n",
        "# Array of regions for random selection\n",
        "regions_list = array([lit(x) for x in ['North', 'South', 'East', 'West', 'Central']])\n",
        "\n",
        "# 2. Data generation (no for loop!)\n",
        "df = spark.range(0, total_rows).withColumnRenamed(\"id\", \"idx\") \\\n",
        "    .withColumn(\"sale_id\", concat(lit(\"SALE-\"), lpad(col(\"idx\"), 8, \"0\"))) \\\n",
        "    .withColumn(\"customer_id\", concat(lit(\"CUST\"), lpad((rand() * 500 + 1).cast(\"int\"), 4, \"0\"))) \\\n",
        "    .withColumn(\"product_id\", concat(lit(\"PROD\"), lpad((rand() * 200 + 1).cast(\"int\"), 3, \"0\"))) \\\n",
        "    .withColumn(\"region\", element_at(regions_list, (rand() * 5 + 1).cast(\"int\"))) \\\n",
        "    .withColumn(\"sale_date\", date_add(to_date(lit(\"2024-01-01\")), (rand() * 364).cast(\"int\"))) \\\n",
        "    .withColumn(\"amount\", round(rand() * 490 + 10, 2)) \\\n",
        "    .withColumn(\"quantity\", (rand() * 10 + 1).cast(\"int\")) \\\n",
        "    .drop(\"idx\") # Remove helper column\n",
        "\n",
        "# 3. Write - repartition is key\n",
        "# Create table with Liquid Clustering enabled (if not exists)\n",
        "# or append to existing one.\n",
        "df.repartition(target_files).write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"append\") \\\n",
        "    .saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.sales_liquid_clustering\")\n",
        "\n",
        "print(f\"Done! Inserted {total_rows} records in {target_files} small files.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2c64603c-281f-41bc-9be1-befe33ae125a",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "df = spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.sales_liquid_clustering\")\n",
        "display(df.select(\"numFiles\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1c5dd1b5-43c9-4890-b4e6-442916913d81",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Enable liquid clustering on an existing table by specifying clustering columns\n",
        "spark.sql(f\"\"\"\n",
        "ALTER TABLE {CATALOG}.{BRONZE_SCHEMA}.sales_liquid_clustering\n",
        "CLUSTER BY (customer_id,product_id)\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "48cb56d5-0256-40f6-a8cd-084b407066e0",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# OPTIMIZE automatically applies Liquid Clustering\n",
        "# No need to specify ZORDER - it's built into the table definition!\n",
        "optimize_result = spark.sql(f\"\"\"\n",
        "    OPTIMIZE {CATALOG}.{BRONZE_SCHEMA}.sales_liquid_clustering\n",
        "\"\"\")\n",
        "\n",
        "display(optimize_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "bbbdca62-32f8-4602-a898-a970f6b45d54",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Check clustering information\n",
        "display(spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.sales_liquid_clustering\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "40e95234-86d1-4849-a440-8b24be141012",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Queries filtering by clustering columns are automatically optimized\n",
        "result = spark.sql(f\"\"\"\n",
        "    SELECT region, COUNT(*) as sales_count, SUM(amount) as total_amount\n",
        "    FROM {CATALOG}.{BRONZE_SCHEMA}.sales_liquid_clustering\n",
        "    WHERE customer_id LIKE 'CUST00%' AND region = 'North'\n",
        "    GROUP BY region\n",
        "\"\"\")\n",
        "\n",
        "display(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "745a1e46-9041-41b1-a3a9-387d3dbdd10c",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "**Comparison: Partitioning vs Z-ORDER vs Liquid Clustering**\n",
        "\n",
        "| Feature | Partitioning | Z-ORDER | Liquid Clustering |\n",
        "|---------|-------------|---------|-------------------|\n",
        "| When to choose | Low-cardinality columns | High-cardinality filter columns | General purpose (recommended) |\n",
        "| Data layout | Directory per partition | Co-located in files | Automatic clustering |\n",
        "| Schema change | Requires rewrite | Easy to change | Easy to change |\n",
        "| Maintenance | Manual | Manual OPTIMIZE | Automatic with OPTIMIZE |\n",
        "| Best for | Date/Region filters | Multi-column filters | Evolving workloads |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5eb35fb6-d930-43e1-8c64-499144995ce0",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "# Change Data Feed vs Change Data Capture\n",
        "\n",
        "**Theoretical Introduction:**\n",
        "\n",
        "Two terms are often confused in the data engineering world: **Change Data Feed (CDF)** and **Change Data Capture (CDC)**. Understanding the difference is crucial:\n",
        "\n",
        "### Change Data Capture (CDC)\n",
        "**What it is:** A *pattern/technique* for capturing changes from source systems (databases, APIs, etc.)\n",
        "\n",
        "**Characteristics:**\n",
        "- Source-side technology\n",
        "- Captures INSERT, UPDATE, DELETE from operational databases\n",
        "- Tools: Debezium, AWS DMS, Fivetran, Qlik Replicate\n",
        "- Produces a stream of change events\n",
        "\n",
        "**Example:** Capturing changes from PostgreSQL and streaming them to Kafka\n",
        "\n",
        "### Change Data Feed (CDF)\n",
        "**What it is:** A *Delta Lake feature* that records row-level changes within Delta tables\n",
        "\n",
        "**Characteristics:**\n",
        "- Delta Lake native feature\n",
        "- Tracks changes that happen WITHIN Delta tables\n",
        "- Provides `_change_type`, `_commit_version`, `_commit_timestamp` columns\n",
        "- Enables efficient incremental processing\n",
        "\n",
        "**Example:** Reading only the rows that changed since the last pipeline run\n",
        "\n",
        "### How They Work Together\n",
        "```\n",
        "[Source DB] --CDC--> [Bronze Delta] --CDF--> [Silver Delta] --CDF--> [Gold Delta]\n",
        "     ^                    ^                      ^                       ^\n",
        "     |                    |                      |                       |\n",
        "   CDC captures      CDF tracks            CDF tracks              CDF tracks\n",
        "   source changes    Bronze changes        Silver changes          Gold changes\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "cb39be04-b261-4ce5-b788-ac3248b1156e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "## Example: Enabling Change Data Feed\n",
        "\n",
        "**Objective:** Enable CDF on a Delta table and understand what metadata is captured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e46d409c-5047-4927-833e-6b30facf28fc",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Create a table with CDF enabled from the start\n",
        "spark.sql(f\"\"\"\n",
        "CREATE OR REPLACE TABLE {CATALOG}.{BRONZE_SCHEMA}.cdf_demo (\n",
        "    user_id STRING,\n",
        "    name STRING,\n",
        "    email STRING,\n",
        "    status STRING,\n",
        "    updated_at TIMESTAMP\n",
        ") \n",
        "USING DELTA\n",
        "TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
        "\"\"\")\n",
        "\n",
        "print(\"Table created with Change Data Feed enabled\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "df6f5ad3-7521-4adc-867a-d2bffea5583a",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Verify CDF is enabled\n",
        "properties = spark.sql(f\"SHOW TBLPROPERTIES {CATALOG}.{BRONZE_SCHEMA}.cdf_demo\")\n",
        "display(properties.filter(F.col(\"key\").like(\"%change%\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5e998bfb-bd2a-433e-8cf4-4cb4074f97fd",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "## Example: Generating and Tracking Changes\n",
        "\n",
        "**Objective:** Execute various DML operations and see how CDF records them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "152fb78b-0f62-469c-8e04-c0d1d5b5dd64",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# INSERT some initial data\n",
        "spark.sql(f\"\"\"\n",
        "INSERT INTO {CATALOG}.{BRONZE_SCHEMA}.cdf_demo VALUES\n",
        "    ('U001', 'Alice', 'alice@example.com', 'active', current_timestamp()),\n",
        "    ('U002', 'Bob', 'bob@example.com', 'active', current_timestamp()),\n",
        "    ('U003', 'Charlie', 'charlie@example.com', 'active', current_timestamp())\n",
        "\"\"\")\n",
        "print(\"Version 1: Initial INSERT completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "134459ab-94c3-4cb2-af5a-c378dc2ebe62",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# UPDATE a record\n",
        "spark.sql(f\"\"\"\n",
        "UPDATE {CATALOG}.{BRONZE_SCHEMA}.cdf_demo\n",
        "SET status = 'premium', updated_at = current_timestamp()\n",
        "WHERE user_id = 'U001'\n",
        "\"\"\")\n",
        "print(\"Version 2: UPDATE completed - Alice upgraded to premium\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "dd1e7aae-1034-474e-82b4-9e5b318d1ec2",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# DELETE a record\n",
        "spark.sql(f\"\"\"\n",
        "DELETE FROM {CATALOG}.{BRONZE_SCHEMA}.cdf_demo\n",
        "WHERE user_id = 'U002'\n",
        "\"\"\")\n",
        "print(\"Version 3: DELETE completed - Bob removed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f55bf949-d3e6-47c8-b27c-23542e83bf12",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# INSERT new record\n",
        "spark.sql(f\"\"\"\n",
        "INSERT INTO {CATALOG}.{BRONZE_SCHEMA}.cdf_demo VALUES\n",
        "    ('U004', 'Diana', 'diana@example.com', 'trial', current_timestamp())\n",
        "\"\"\")\n",
        "print(\"Version 4: INSERT completed - Diana added\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ef3f3dc6-8887-48a1-8d8d-e7df25cda2f7",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "## Example: Reading Change Data Feed\n",
        "\n",
        "**Objective:** Query the Change Data Feed to see all recorded changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "75a16c02-d0cb-4639-be52-1ad7d2be0bf2",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Read all changes from the beginning\n",
        "changes = spark.read \\\n",
        "    .format(\"delta\") \\\n",
        "    .option(\"readChangeFeed\", \"true\") \\\n",
        "    .option(\"startingVersion\", 0) \\\n",
        "    .table(f\"{CATALOG}.{BRONZE_SCHEMA}.cdf_demo\")\n",
        "\n",
        "# Show changes with CDF metadata columns\n",
        "display(\n",
        "    changes.select(\n",
        "        \"user_id\", \"name\", \"status\",\n",
        "        \"_change_type\",        # insert, update_preimage, update_postimage, delete\n",
        "        \"_commit_version\",     # Delta version number\n",
        "        \"_commit_timestamp\"    # When the change occurred\n",
        "    ).orderBy(\"_commit_version\", \"user_id\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "885e9b6e-eb22-46a3-98d6-0b148b705ac8",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "**Understanding `_change_type` values:**\n",
        "\n",
        "| Change Type | Description |\n",
        "|-------------|-------------|\n",
        "| `insert` | New row inserted |\n",
        "| `update_preimage` | Row value BEFORE update |\n",
        "| `update_postimage` | Row value AFTER update |\n",
        "| `delete` | Row that was deleted |\n",
        "\n",
        "This enables powerful incremental processing patterns - you can process only what changed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b9c70462-0945-4f22-8c1e-0ee252951874",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Example: Get only new inserts since version 2\n",
        "new_inserts = spark.read \\\n",
        "    .format(\"delta\") \\\n",
        "    .option(\"readChangeFeed\", \"true\") \\\n",
        "    .option(\"startingVersion\", 2) \\\n",
        "    .table(f\"{CATALOG}.{BRONZE_SCHEMA}.cdf_demo\") \\\n",
        "    .filter(F.col(\"_change_type\") == \"insert\")\n",
        "\n",
        "print(\"New inserts since version 2:\")\n",
        "display(new_inserts.select(\"user_id\", \"name\", \"status\", \"_commit_version\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ccd47c79-7f85-4a18-a9e3-f027f6edd111",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Example: Get all deletions for audit purposes\n",
        "deletions = spark.read \\\n",
        "    .format(\"delta\") \\\n",
        "    .option(\"readChangeFeed\", \"true\") \\\n",
        "    .option(\"startingVersion\", 0) \\\n",
        "    .table(f\"{CATALOG}.{BRONZE_SCHEMA}.cdf_demo\") \\\n",
        "    .filter(F.col(\"_change_type\") == \"delete\")\n",
        "\n",
        "print(\"All deleted records (for audit):\")\n",
        "display(deletions.select(\"user_id\", \"name\", \"_commit_version\", \"_commit_timestamp\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c6ba49bf-2a86-426f-8b3a-60ff78d3a157",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "## Example: CDF for Incremental ETL\n",
        "\n",
        "**Objective:** Demonstrate how CDF enables efficient incremental processing in ETL pipelines\n",
        "\n",
        "Instead of reprocessing entire tables, use CDF to process only changed rows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "8df0cbfd-422c-4ed5-ab6d-c2a570349b74",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Simulate an incremental ETL pipeline\n",
        "# First run: Process all data (startingVersion = 0)\n",
        "# Subsequent runs: Process only changes since last processed version\n",
        "\n",
        "# Store the last processed version (in practice, save this to a checkpoint table)\n",
        "last_processed_version = 0\n",
        "\n",
        "# Read incremental changes\n",
        "incremental_changes = spark.read \\\n",
        "    .format(\"delta\") \\\n",
        "    .option(\"readChangeFeed\", \"true\") \\\n",
        "    .option(\"startingVersion\", last_processed_version) \\\n",
        "    .table(f\"{CATALOG}.{BRONZE_SCHEMA}.cdf_demo\")\n",
        "\n",
        "# Apply transformations only to changed records\n",
        "transformed = incremental_changes \\\n",
        "    .filter(F.col(\"_change_type\").isin([\"insert\", \"update_postimage\"])) \\\n",
        "    .withColumn(\"processed_at\", F.current_timestamp()) \\\n",
        "    .withColumn(\"email_domain\", F.split(F.col(\"email\"), \"@\")[1])\n",
        "\n",
        "print(\"Incremental processing - only changed records:\")\n",
        "display(transformed.select(\"user_id\", \"name\", \"email_domain\", \"status\", \"_change_type\", \"processed_at\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "cd7e7f58-66d2-49a1-bcc8-555013ec35ae",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "**Key Takeaways - CDF vs CDC:**\n",
        "\n",
        "1. **CDC** captures changes FROM source systems INTO your lakehouse\n",
        "2. **CDF** tracks changes WITHIN Delta Lake tables\n",
        "3. Use CDC tools (Debezium, DMS) to ingest data into Bronze\n",
        "4. Use CDF to build efficient incremental Silver and Gold layers\n",
        "5. CDF eliminates need for expensive full-table scans in pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "cc98f619-739b-4d55-b83c-771337ed9d7e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "---\n",
        "\n",
        "# Summary\n",
        "\n",
        "### What Has Been Achieved (Modules 4 & 5):\n",
        "\n",
        "| Module | Key Learnings |\n",
        "|---------|--------------|\n",
        "| **Module 4: Operations** | Delta table creation, Schema Enforcement, Schema Evolution, Constraints, CRUD, Time Travel |\n",
        "| **Module 5: Optimization** | Small Files Problem, Partitioning, Z-ORDER, Liquid Clustering, Change Data Feed (CDF) |\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "1. **Delta Lake = Data Lake + ACID**: Combines Data Lake flexibility with transactional reliability\n",
        "2. **Schema Evolution safely**: Additive changes are automatic, breaking changes require planning\n",
        "3. **Time Travel + Copy-on-Write**: Every version is preserved, enabling rollback and audit\n",
        "4. **VACUUM trade-off**: Storage optimization vs Time Travel capability\n",
        "5. **Optimization matters**: Choose the right technique (Partitioning, Z-ORDER, Liquid Clustering)\n",
        "6. **CDF  CDC**: CDC ingests from sources, CDF tracks Delta Lake changes\n",
        "\n",
        "### Quick Reference - Key Commands:\n",
        "\n",
        "| Operation | SQL | PySpark |\n",
        "|-----------|-----|---------|\n",
        "| Create Delta Table | `CREATE TABLE USING DELTA` | `df.write.format(\"delta\").saveAsTable()` |\n",
        "| Time Travel | `SELECT * FROM table VERSION AS OF 1` | `.option(\"versionAsOf\", 1)` |\n",
        "| Restore | `RESTORE TABLE table TO VERSION AS OF 1` | N/A |\n",
        "| MERGE | `MERGE INTO target USING source` | `DeltaTable.forName().merge()` |\n",
        "| Optimize | `OPTIMIZE table` | N/A |\n",
        "| Z-ORDER | `OPTIMIZE table ZORDER BY (col)` | N/A |\n",
        "| VACUUM | `VACUUM table RETAIN X HOURS` | N/A |\n",
        "| History | `DESCRIBE HISTORY table` | N/A |\n",
        "| Enable CDF | `ALTER TABLE SET TBLPROPERTIES (delta.enableChangeDataFeed = true)` | N/A |\n",
        "| Read CDF | N/A | `.option(\"readChangeFeed\", \"true\")` |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7345fa46-39e7-40b3-85cb-0f989d83baa2",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "---\n",
        "\n",
        "# Resource Cleanup\n",
        "\n",
        "Clean up resources created during the notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "57591f66-475d-425f-bfc8-6b6f4d9e3094",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Optional test resource cleanup\n",
        "# NOTE: Run only if you want to delete all created data\n",
        "\n",
        "# Tables to clean up:\n",
        "cleanup_tables = [\n",
        "    \"customers_delta\",\n",
        "    \"orders_modern\", \n",
        "    \"time_travel_demo\",\n",
        "    \"small_files_demo\",\n",
        "    \"orders_partitioned\",\n",
        "    \"sales_zorder_demo\",\n",
        "    \"sales_liquid_clustering\",\n",
        "    \"cdf_demo\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "0e99b47a-4e64-4887-b7ee-90841ba4465c",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Uncomment below to execute cleanup:\n",
        "# for table in cleanup_tables:\n",
        "#     spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{BRONZE_SCHEMA}.{table}\")\n",
        "#     print(f\"Dropped: {table}\")\n",
        "\n",
        "# spark.sql(\"DROP VIEW IF EXISTS customer_updates\")\n",
        "# spark.catalog.clearCache()\n",
        "\n",
        "# print(\"All resources cleaned up!\")"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": null,
      "dashboards": [],
      "environmentMetadata": null,
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "03_delta_lake_optimization",
      "widgets": {}
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
