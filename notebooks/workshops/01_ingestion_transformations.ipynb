{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efe7f794-d87e-40e2-9a5d-32271338eece",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ingestion & Transformations\n",
    "\n",
    "## Scenario\n",
    "\n",
    "You are a Data Engineer at a retail company. The marketing team has requested a clean list of customers to run a new email campaign.\n",
    "The data is currently sitting in a CSV file in the landing zone, but it's raw and needs processing.\n",
    "\n",
    "**Task:**\n",
    "1. Ingest the raw customer data from CSV.\n",
    "2. Select only the relevant columns (Name, Email, Company).\n",
    "3. Create a `FullName` column by combining First and Last names.\n",
    "4. Add an audit timestamp to track when the data was processed.\n",
    "5. Save the clean data as a Delta table for the marketing team to use.\n",
    "\n",
    "**Time:** 30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c9857b6-1390-48e3-bb41-e03db5c152a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## User Isolation\n",
    "This notebook is designed to be run in a shared environment.\n",
    "To avoid conflicts, we will use a unique `catalog` and `schema` for your user.\n",
    "The `00_setup` script will automatically configure these for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "748521eb-539b-4c7a-92c6-1dfc269c37e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc836325-15c7-44a5-907d-e8db59bbc569",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- INDEPENDENT SETUP ---\n",
    "# Ensure source data exists for this workshop\n",
    "import os\n",
    "\n",
    "# Define path\n",
    "source_dir = f\"{DATASET_BASE_PATH}/workshop/main\"\n",
    "source_file = f\"{source_dir}/Customers.csv\"\n",
    "\n",
    "# Check if source file exists\n",
    "try:\n",
    "    dbutils.fs.ls(source_file)\n",
    "    print(f\"Source file found: {source_file}\")\n",
    "except:\n",
    "    print(f\"WARNING: Source file not found at {source_file}. Please ensure datasets are uploaded to the Volume.\")\n",
    "\n",
    "print(f\"Catalog: {CATALOG}\")\n",
    "print(f\"Bronze Schema: {BRONZE_SCHEMA}\")\n",
    "print(f\"Silver Schema: {SILVER_SCHEMA}\")\n",
    "print(f\"Gold Schema:   {GOLD_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4578182c-63b4-4c18-b812-44b365786d03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Environment Configuration\n",
    "We will configure the environment variables and paths used in this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb5200d-95f5-4ac9-9e3d-96f500cde691",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if variables were loaded\n",
    "print(f\"Catalog: {CATALOG}\")\n",
    "print(f\"Volume:  {DATASET_BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99a29918-5552-4f55-a878-d6f0cc1e9421",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Source Data Exploration\n",
    "\n",
    "Before loading data, let's see what we have available in the source directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd6439e5-847b-4d5a-8c0d-ce4d4f51cd4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List files in the workshop directory\n",
    "dbutils.fs.ls(f\"{DATASET_BASE_PATH}/workshop/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85f9e131-edc8-4ead-bf00-f63694eabdc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Loading Customer Data\n",
    "\n",
    "### Load `Customers.csv` file\n",
    "\n",
    "**Requirements:**\n",
    "- Use CSV format\n",
    "- File has headers\n",
    "- Let Spark automatically detect data types (`inferSchema`)\n",
    "\n",
    "**Hint:**\n",
    "```python\n",
    "spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .load(\"path\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf02aac1-779a-4bf1-80d3-0aa73045bf67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Path to file\n",
    "customers_path = f\"{DATASET_BASE_PATH}/workshop/Customers.csv\"\n",
    "\n",
    "# TODO: Load Customers.csv file into df_customers DataFrame\n",
    "df_customers = (\n",
    "    spark.read\n",
    "    # Complete the code here\n",
    "    # .format(...)\n",
    "    # .option(...)\n",
    "    # .load(...)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7964b49d-576e-4886-bddf-3012a04ab047",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check result\n",
    "print(f\"Loaded {df_customers.count()} customers\")\n",
    "display(df_customers.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1525e7f-b617-4d84-8acb-bc2cbc66dd80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Transformations\n",
    "\n",
    "### Select required columns\n",
    "\n",
    "The marketing team needs only:\n",
    "- `CustomerID`\n",
    "- `FirstName`\n",
    "- `LastName`\n",
    "- `EmailAddress`\n",
    "- `CompanyName`\n",
    "- `Phone`\n",
    "\n",
    "**Hint:** Use `.select(\"column1\", \"column2\", ...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff0a72cb-bd2b-4c0b-a96a-0360679f26a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Select only required columns\n",
    "df_customers_clean = df_customers.select(\n",
    "    # Add columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e06d192-1603-4baa-935c-c651de2c0a5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create `FullName` column\n",
    "\n",
    "Combine `FirstName` and `LastName` into a single `FullName` column.\n",
    "\n",
    "**Hint:** Use the `concat_ws` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c87491b8-611a-42d6-9f17-45a45f4823ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws, col, upper, trim, current_timestamp\n",
    "\n",
    "# TODO: Add FullName column\n",
    "df_customers_enriched = df_customers_clean.withColumn(\n",
    "    \"FullName\",\n",
    "    # Complete the code here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51bb38e4-dec8-4717-a021-962700744f4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Filter invalid emails\n",
    "\n",
    "Filter out customers who do not have a valid email address (must contain '@')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f3e798f-0c3b-4b0e-a9ea-afe81caedd58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Filter rows where EmailAddress contains '@'\n",
    "df_customers_filtered = df_customers_enriched.filter(\n",
    "    # Complete the code here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "714997b3-849f-468c-b126-bd964efeaf9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Analyze Company Distribution\n",
    "\n",
    "Check how many customers belong to each company. Sort the result by count in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89fd6fcf-9861-4b0f-95d2-23dbd792f31d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Group by CompanyName and count\n",
    "# display(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5aeddeb-5858-4a66-b6d8-a0f5f897fc0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Adding Audit Column\n",
    "\n",
    "### Add audit column\n",
    "\n",
    "Add an `ingestion_timestamp` column with the current time - this is a good practice in ETL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e567b32-b8b9-46b6-8d40-c32c662b6e43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Add ingestion_timestamp column\n",
    "df_final = df_customers_filtered.withColumn(\n",
    "    \"ingestion_timestamp\",\n",
    "    # Complete the code here - use current_timestamp()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0a1bbf4-ff34-4b0e-baa7-9cc1e19fc1fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save to Delta Lake\n",
    "\n",
    "### Save as Delta table\n",
    "\n",
    "Save the resulting DataFrame as a managed Delta Lake table named `customers_silver`.\n",
    "\n",
    "**Hint:**\n",
    "```python\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"catalog.schema.table_name\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ad4f05d-3f5d-4f4d-b552-e011936c4a0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name = f\"{CATALOG}.{SILVER_SCHEMA}.customers_silver\"\n",
    "\n",
    "# TODO: Save df_final as Delta table\n",
    "(\n",
    "    df_final.write\n",
    "    # Complete the code here\n",
    ")\n",
    "\n",
    "print(f\"Saved table: {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e38537b-99ca-4a3e-b007-d020324f312e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Verification\n",
    "\n",
    "Let's check if the table was created correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1b8f9b2-b226-41ac-85a4-8723d674ba84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check the table\n",
    "display(spark.table(table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82cdec46-16c5-47f3-8118-2b7b0bce0787",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check Delta metadata\n",
    "display(spark.sql(f\"DESCRIBE DETAIL {table_name}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77a4a9da-a5ee-4f16-9747-78e5ced8824c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SQL Access (The Lakehouse Advantage)\n",
    "\n",
    "You just created a table using Python. Now, let's query it immediately using SQL!\n",
    "This demonstrates how Data Engineers and Data Analysts can work on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "287f5202-5cf3-4929-8b24-1a260ba92d8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "# Query the table using SQL (via Spark)\n",
    "display(spark.sql(f\"\"\"\n",
    "SELECT * \n",
    "FROM {CATALOG}.{SILVER_SCHEMA}.customers_silver \n",
    "WHERE CompanyName = 'Smith Group'\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03c5d401-38c5-4d49-bb10-7141dfeb21d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73eeab6c-c7c7-471b-9d4e-3fdf9aaa6a39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# WARNING: Uncomment only if you want to delete the table!\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5faafc88-1929-450d-b2a7-9a194978755c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Solution\n",
    "\n",
    "The complete code is below. Try to solve it yourself first!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "107a073e-d7f6-445a-892b-e06403628da0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FULL SOLUTION - Workshop 1: Ingestion & Transformations\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import concat_ws, col, current_timestamp, trim\n",
    "\n",
    "# --- Step 2: Loading data ---\n",
    "customers_path = f\"{DATASET_BASE_PATH}/workshop/Customers.csv\"\n",
    "\n",
    "df_customers = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .load(customers_path)\n",
    ")\n",
    "\n",
    "# --- Step 3: Transformations ---\n",
    "df_customers_clean = df_customers.select(\n",
    "    \"CustomerID\", \"FirstName\", \"LastName\", \n",
    "    \"EmailAddress\", \"CompanyName\", \"Phone\"\n",
    ")\n",
    "\n",
    "df_customers_enriched = df_customers_clean.withColumn(\n",
    "    \"FullName\",\n",
    "    concat_ws(\" \", col(\"FirstName\"), col(\"LastName\"))\n",
    ")\n",
    "\n",
    "# Task 3.3: Filter\n",
    "df_customers_filtered = df_customers_enriched.filter(col(\"EmailAddress\").contains(\"@\"))\n",
    "\n",
    "# Task 3.4: Analysis\n",
    "print(\"Company Distribution:\")\n",
    "# Using display() allows for built-in plotting!\n",
    "display(df_customers_filtered.groupBy(\"CompanyName\").count().orderBy(\"count\", ascending=False))\n",
    "\n",
    "# --- Step 4: Add audit column ---\n",
    "df_final = df_customers_filtered.withColumn(\n",
    "    \"ingestion_timestamp\",\n",
    "    current_timestamp()\n",
    ")\n",
    "\n",
    "# --- Step 5: Save to Delta ---\n",
    "table_name = f\"{CATALOG}.{SILVER_SCHEMA}.customers_silver\"\n",
    "\n",
    "(\n",
    "    df_final.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(table_name)\n",
    ")\n",
    "\n",
    "print(f\"Solution executed! Table: {table_name}\")\n",
    "print(f\"Row count: {spark.table(table_name).count()}\")\n",
    "display(spark.table(table_name).limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd034631-99b9-47aa-b41b-757331a784c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Clean up resources\n",
    "Stop any active streams and remove the created resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15c578cd-4604-42ae-a223-a282256c666a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "# dbutils.fs.rm(CHECKPOINT_PATH, True)\n",
    "# spark.sql(f\"DROP SCHEMA IF EXISTS {CATALOG}.{SCHEMA} CASCADE\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingestion_transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
