{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "998fd0e4-377f-4a66-a95b-1a3846db62b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Streaming & Incremental Data\n",
    "\n",
    "## Scenario\n",
    "We will simulate a real-time data ingestion pipeline using Auto Loader and Delta Lake.\n",
    "You will build a streaming pipeline that ingests JSON files, handles schema evolution, and performs real-time aggregations.\n",
    "\n",
    "**Task:**\n",
    "1. Configure Auto Loader to ingest files.\n",
    "2. Handle schema evolution with \"Rescue Mode\".\n",
    "3. Perform windowed aggregations with watermarking.\n",
    "4. Join streaming data with static tables.\n",
    "\n",
    "**Time:** 30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d46a9251-acdf-4ff1-b0f5-a1128d8a855f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## User Isolation\n",
    "This notebook is designed to be run in a shared environment.\n",
    "To avoid conflicts, we will use a unique `catalog` and `schema` for your user.\n",
    "The `00_setup` script will automatically configure these for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea8dab21-34f5-43d1-907d-16249e57d70a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87c818c8-606e-43bb-a217-81e5d63fa01b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Environment Configuration\n",
    "We will configure the environment variables and paths used in this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10013936-83ae-49f8-bafa-eb7cd30d8896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import col, current_timestamp, window, expr\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# We will use a temporary directory to simulate data arrival\n",
    "STREAM_SOURCE_PATH = f\"{DATASET_BASE_PATH}/workshop/stream/orders\"\n",
    "SIMULATION_PATH = f\"{DATASET_BASE_PATH}/workshop/simulation/incoming_orders\"\n",
    "CHECKPOINT_PATH = f\"{DATASET_BASE_PATH}/workshop/simulation/checkpoints\"\n",
    "SCHEMA_PATH = f\"{DATASET_BASE_PATH}/workshop/simulation/schemas\"\n",
    "\n",
    "# Clean up previous runs\n",
    "dbutils.fs.rm(SIMULATION_PATH, True)\n",
    "dbutils.fs.rm(CHECKPOINT_PATH, True)\n",
    "dbutils.fs.rm(SCHEMA_PATH, True)\n",
    "dbutils.fs.mkdirs(SIMULATION_PATH)\n",
    "\n",
    "print(f\"Source Data: {STREAM_SOURCE_PATH}\")\n",
    "print(f\"Simulation Path: {SIMULATION_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80acae57-93be-477f-9230-b5441faec52e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Simulation Setup\n",
    "\n",
    "In a real scenario, files would land in S3/ADLS automatically.\n",
    "For this workshop, we will manually \"drop\" files into our simulation folder to trigger processing.\n",
    "\n",
    "Let's check our source files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59bf470f-838c-4146-aff8-6a0928b3865e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "STREAM_BASIC_FILE = \"/Volumes/ecommerce_platform_trainer/default/datasets/workshop/SalesOrderDetail.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e273196-d2fe-47a6-8275-511c8ab29e40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the source CSV file as a Spark DataFrame\n",
    "df = spark.read.csv(STREAM_BASIC_FILE, header=True, inferSchema=True)\n",
    "\n",
    "# Randomly split into 20 roughly equal batches\n",
    "splits = df.randomSplit([0.05]*50, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fbfd792-f5d8-4e1e-95f3-51f341495a0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to simulate data arrival\n",
    "def arrive_batch(batch_id):\n",
    "    filename = f\"orders_stream_{batch_id:03d}.json\"\n",
    "    target = f\"{SIMULATION_PATH}/{filename}\"\n",
    "    try:\n",
    "        splits[batch_id].coalesce(1).write.mode(\"overwrite\").json(target)\n",
    "        print(f\"✅ Batch {batch_id} arrived: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error writing batch {batch_id}: {e}\")\n",
    "\n",
    "# Problem: If the source file does not exist or there are permission issues, dbutils.fs.cp will raise an exception.\n",
    "\n",
    "# Simulate arrival of the first batch\n",
    "arrive_batch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13a67293-1bd0-41af-9641-7dab81deb04b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Auto Loader (cloudFiles)\n",
    "\n",
    "**Auto Loader** is the standard way to ingest files in Databricks. It automatically detects new files and tracks state.\n",
    "\n",
    "### Configure Auto Loader\n",
    "\n",
    "Create a streaming DataFrame `df_stream` that reads from `SIMULATION_PATH`.\n",
    "\n",
    "**Requirements:**\n",
    "- Format: `cloudFiles`\n",
    "- File Format: `json`\n",
    "- Schema Location: `SCHEMA_PATH`\n",
    "- Infer Column Types: `true`\n",
    "\n",
    "**Hint:**\n",
    "```python\n",
    "spark.readStream.format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"json\") \\\n",
    "    .option(\"cloudFiles.schemaLocation\", path) \\\n",
    "    .load(path)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42643462-e20c-4bf4-9ed1-e1abf1979f11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Configure Auto Loader\n",
    "df_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"___\") # Use Auto Loader format (cloudFiles)\n",
    "    .option(\"cloudFiles.format\", \"___\") # Source file format\n",
    "    .option(\"cloudFiles.schemaLocation\", SCHEMA_PATH)\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .load(___) # Path to data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95226dfa-ea5e-4c07-a3aa-73a16a69b1f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write Stream to Delta Table\n",
    "\n",
    "Write the stream to a Delta table named `orders_bronze`.\n",
    "Use `trigger(availableNow=True)` to process the current batch and stop (good for testing).\n",
    "\n",
    "**Hint:**\n",
    "```python\n",
    "df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", path) \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .toTable(\"catalog.schema.table\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "118d5a93-f394-4aee-aa8b-53d0082020b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name = f\"{CATALOG}.{BRONZE_SCHEMA}.orders_bronze\"\n",
    "checkpoint_dir = f\"{CHECKPOINT_PATH}/orders_bronze\"\n",
    "\n",
    "# TODO: Write stream\n",
    "query = (\n",
    "    df_stream.writeStream\n",
    "    .format(\"___\") # Target format\n",
    "    .option(\"checkpointLocation\", ___)\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(table_name)\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "print(f\"Processed batch 1 into {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6028ba01-f924-4a76-8f6a-45a63910a9cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify data\n",
    "display(spark.table(table_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36d9d668-0762-454f-b255-2a33c5964b91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Schema Evolution & Rescue Data\n",
    "\n",
    "Data changes. New columns appear. Types change.\n",
    "Auto Loader handles this with **Schema Evolution** and **Rescued Data**.\n",
    "\n",
    "### Simulate \"Bad\" Data\n",
    "\n",
    "We will inject a file with:\n",
    "1.  A new column (`customer_mood`).\n",
    "2.  A malformed record (string in an integer field)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab5d8d6c-1a61-45e7-bc33-47fa254bea0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a file with unexpected data\n",
    "bad_data = [\n",
    "    {\"order_id\": 9001, \"total_amount\": 50.0, \"customer_mood\": \"happy\"}, # New column\n",
    "    {\"order_id\": 9002, \"total_amount\": \"INVALID\", \"customer_mood\": \"angry\"} # Type mismatch\n",
    "]\n",
    "spark.createDataFrame(bad_data).write.mode(\"overwrite\").json(f\"{SIMULATION_PATH}/bad_data_batch\")\n",
    "\n",
    "print(\"⚠️ Bad data batch arrived!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2319cc37-59ea-49bb-82eb-f81385618c0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configure Rescue Mode\n",
    "\n",
    "Re-configure the stream to use `cloudFiles.schemaEvolutionMode` = `rescue`.\n",
    "This will put unexpected data into a `_rescued_data` column instead of failing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33f5423f-004b-4ce6-8c12-d1385351ae31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Re-configure stream with rescue mode\n",
    "df_stream_rescue = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", SCHEMA_PATH)\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"___\") # Enable Rescue Mode\n",
    "    .load(SIMULATION_PATH)\n",
    ")\n",
    "\n",
    "# Write again\n",
    "query_rescue = (\n",
    "    df_stream_rescue.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", checkpoint_dir)\n",
    "    .option(\"mergeSchema\", \"___\") # Allow Delta table schema to evolve\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(table_name)\n",
    ")\n",
    "\n",
    "query_rescue.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "329eed7e-89e5-4ce2-a52f-56b4c34630ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check the table - look for _rescued_data column\n",
    "display(spark.table(table_name).filter(\"_rescued_data IS NOT NULL\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62de8919-b966-468f-afe2-31a91427c9e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Real-Time Aggregations (Watermarking)\n",
    "\n",
    "We want to count orders every 30 seconds.\n",
    "Since streams are infinite, we need **Watermarking** to tell Spark when to close a window and discard old state.\n",
    "\n",
    "### Define Windowed Aggregation\n",
    "\n",
    "1.  Add a timestamp column `processing_time`.\n",
    "2.  Define a watermark (e.g., \"1 minute\").\n",
    "3.  Group by `window(\"processing_time\", \"30 seconds\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e34a3ea1-7ef0-495c-a08c-558c7718c838",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare stream with timestamp\n",
    "df_with_time = df_stream_rescue.withColumn(\"processing_time\", current_timestamp())\n",
    "\n",
    "# TODO: Define aggregation\n",
    "orders_count = (\n",
    "    df_with_time\n",
    "    .withWatermark(\"___\", \"___\") # Column, threshold (e.g. \"1 minute\")\n",
    "    .groupBy(\n",
    "        window(\"___\", \"___\") # Column, window duration (e.g. \"30 seconds\")\n",
    "    )\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4701ac0e-2bc7-497a-b7dd-163ec7ae7cd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start aggregation stream (Output Mode: Update)\n",
    "# We use processingTime trigger for continuous processing\n",
    "query_agg = (\n",
    "    orders_count.writeStream\n",
    "    .format(\"memory\") # Writing to memory for easy visualization\n",
    "    .queryName(\"orders_dashboard\")\n",
    "    .outputMode(\"update\")\n",
    "    .trigger(processingTime=\"5 seconds\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "print(\"Dashboard stream started...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4604b620-54f1-4295-a1ce-e3bf315843ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simulate more data arrival to see updates\n",
    "for i in range(2, 5):\n",
    "    arrive_batch(i)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa8991f8-760f-465e-87a9-9a90c6016106",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# View the dashboard\n",
    "display(spark.sql(\"SELECT * FROM orders_dashboard ORDER BY window DESC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f17672b7-67c5-4caa-b2bf-f4cd3417c514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_agg.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae4e6d0f-fa7c-443c-8a9e-2cd437c44420",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Stream-Static Join\n",
    "\n",
    "Enrich the incoming orders with Customer data (which is a static Delta table).\n",
    "\n",
    "**Note:** Stream-Static joins are stateless on the static side (Spark reads the static table at the start of each micro-batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4034024e-b24e-4f13-ab96-b3061f406d60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load static customers table\n",
    "customers_table = f\"{CATALOG}.{SILVER_SCHEMA}.customers_silver\"\n",
    "df_customers = spark.read.table(customers_table)\n",
    "\n",
    "# TODO: Join Stream (df_stream_rescue) with Static (df_customers)\n",
    "# Join on customer_id\n",
    "df_enriched = (\n",
    "    df_stream_rescue.alias(\"o\")\n",
    "    .join(\n",
    "        df_customers.alias(\"c\"),\n",
    "        col(\"___\") == col(\"___\"), # Join condition\n",
    "        \"___\" # Join type (left)\n",
    "    )\n",
    "    .select(\"o.order_id\", \"o.total_amount\", \"c.FullName\", \"c.EmailAddress\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f6bb9f8-0acb-42d1-a05a-b71638cd1db4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write enriched stream\n",
    "query_join = (\n",
    "    df_enriched.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/enriched_orders\")\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(f\"{CATALOG}.{BRONZE_SCHEMA}.orders_enriched\")\n",
    ")\n",
    "\n",
    "query_join.awaitTermination()\n",
    "\n",
    "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.orders_enriched\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8be558b6-164b-4e67-96f1-6efcaf724f03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "Stop all streams and remove temporary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38b2198b-937c-4349-bb59-9def599489b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stop any active streams\n",
    "for q in spark.streams.active:\n",
    "    q.stop()\n",
    "\n",
    "# Optional: Remove simulation files\n",
    "# dbutils.fs.rm(SIMULATION_PATH, True)\n",
    "# dbutils.fs.rm(CHECKPOINT_PATH, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f477c494-27e2-46d3-bfae-6261b55d1c13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Solution\n",
    "\n",
    "The complete code is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c88f210f-bdec-4452-bc61-080f0284c953",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FULL SOLUTION - Workshop 4: Streaming\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "df_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", SCHEMA_PATH)\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    "    .load(SIMULATION_PATH)\n",
    ")\n",
    "\n",
    "# --- Task 2.2: Write Stream ---\n",
    "query = (\n",
    "    df_stream.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/orders_bronze\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(f\"{CATALOG}.{BRONZE_SCHEMA}.orders_bronze\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09c4af7d-644d-4c73-93eb-a86c644f96b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.orders_bronze\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0af079f-5900-4e8c-9037-cd9f902abcfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Task 4.1: Aggregation ---\n",
    "df_with_time = df_stream.withColumn(\"processing_time\", current_timestamp())\n",
    "\n",
    "orders_count = (\n",
    "    df_with_time\n",
    "    .withWatermark(\"processing_time\", \"1 minute\")\n",
    "    .groupBy(window(\"processing_time\", \"30 seconds\"))\n",
    "    .count()\n",
    ")\n",
    "\n",
    "# --- Task 5: Stream-Static Join ---\n",
    "df_customers = spark.read.table(f\"{CATALOG}.{SILVER_SCHEMA}.customers_silver\")\n",
    "\n",
    "df_enriched = (\n",
    "    df_stream.alias(\"o\")\n",
    "    .join(df_customers.alias(\"c\"), col(\"o.customer_id\") == col(\"c.CustomerID\"), \"left\")\n",
    "    .select(\"o.order_id\", \"o.total_amount\", \"c.FullName\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_streaming_incremental",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
