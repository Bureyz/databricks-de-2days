{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a8a030a",
   "metadata": {},
   "source": [
    "## User Isolation\n",
    "This notebook is designed to be run in a shared environment.\n",
    "To avoid conflicts, we will use a unique `catalog` and `schema` for your user.\n",
    "The `00_setup` script will automatically configure these for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff6022a",
   "metadata": {},
   "source": [
    "# Streaming & Incremental Data\n",
    "\n",
    "## Scenario\n",
    "We will simulate a real-time data ingestion pipeline using Auto Loader and Delta Lake.\n",
    "You will build a streaming pipeline that ingests JSON files, handles schema evolution, and performs real-time aggregations.\n",
    "\n",
    "**Task:**\n",
    "1. Configure Auto Loader to ingest files.\n",
    "2. Handle schema evolution with \"Rescue Mode\".\n",
    "3. Perform windowed aggregations with watermarking.\n",
    "4. Join streaming data with static tables.\n",
    "\n",
    "**Time:** 45 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dbe384",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d436a9",
   "metadata": {},
   "source": [
    "## Environment Configuration\n",
    "We will configure the environment variables and paths used in this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028cd94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import col, current_timestamp, window, expr\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# We will use a temporary directory to simulate data arrival\n",
    "STREAM_SOURCE_PATH = f\"{DATASET_BASE_PATH}/workshop/stream/orders\"\n",
    "SIMULATION_PATH = f\"{DATASET_BASE_PATH}/workshop/simulation/incoming_orders\"\n",
    "CHECKPOINT_PATH = f\"{DATASET_BASE_PATH}/workshop/simulation/checkpoints\"\n",
    "SCHEMA_PATH = f\"{DATASET_BASE_PATH}/workshop/simulation/schemas\"\n",
    "\n",
    "# Clean up previous runs\n",
    "dbutils.fs.rm(SIMULATION_PATH, True)\n",
    "dbutils.fs.rm(CHECKPOINT_PATH, True)\n",
    "dbutils.fs.rm(SCHEMA_PATH, True)\n",
    "dbutils.fs.mkdirs(SIMULATION_PATH)\n",
    "\n",
    "print(f\"Source Data: {STREAM_SOURCE_PATH}\")\n",
    "print(f\"Simulation Path: {SIMULATION_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acc405c",
   "metadata": {},
   "source": [
    "## Data Simulation Setup\n",
    "\n",
    "In a real scenario, files would land in S3/ADLS automatically.\n",
    "For this workshop, we will manually \"drop\" files into our simulation folder to trigger processing.\n",
    "\n",
    "Let's check our source files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734fc92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available source files\n",
    "source_files = dbutils.fs.ls(STREAM_SOURCE_PATH)\n",
    "display(source_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbe422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to simulate data arrival\n",
    "def arrive_batch(batch_id):\n",
    "    filename = f\"orders_stream_{batch_id:03d}.json\"\n",
    "    source = f\"{STREAM_SOURCE_PATH}/{filename}\"\n",
    "    target = f\"{SIMULATION_PATH}/{filename}\"\n",
    "    \n",
    "    try:\n",
    "        dbutils.fs.cp(source, target)\n",
    "        print(f\"✅ Batch {batch_id} arrived: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error moving batch {batch_id}: {e}\")\n",
    "\n",
    "# Simulate arrival of the first batch\n",
    "arrive_batch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7db62d",
   "metadata": {},
   "source": [
    "## Auto Loader (cloudFiles)\n",
    "\n",
    "**Auto Loader** is the standard way to ingest files in Databricks. It automatically detects new files and tracks state.\n",
    "\n",
    "### Configure Auto Loader\n",
    "\n",
    "Create a streaming DataFrame `df_stream` that reads from `SIMULATION_PATH`.\n",
    "\n",
    "**Requirements:**\n",
    "- Format: `cloudFiles`\n",
    "- File Format: `json`\n",
    "- Schema Location: `SCHEMA_PATH`\n",
    "- Infer Column Types: `true`\n",
    "\n",
    "**Hint:**\n",
    "```python\n",
    "spark.readStream.format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"json\") \\\n",
    "    .option(\"cloudFiles.schemaLocation\", path) \\\n",
    "    .load(path)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fd6374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Configure Auto Loader\n",
    "df_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"___\") # Use Auto Loader format (cloudFiles)\n",
    "    .option(\"cloudFiles.format\", \"___\") # Source file format\n",
    "    .option(\"cloudFiles.schemaLocation\", SCHEMA_PATH)\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .load(___) # Path to data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4191244e",
   "metadata": {},
   "source": [
    "### Write Stream to Delta Table\n",
    "\n",
    "Write the stream to a Delta table named `orders_bronze`.\n",
    "Use `trigger(availableNow=True)` to process the current batch and stop (good for testing).\n",
    "\n",
    "**Hint:**\n",
    "```python\n",
    "df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", path) \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .toTable(\"catalog.schema.table\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69f91cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = f\"{CATALOG}.{BRONZE_SCHEMA}.orders_bronze\"\n",
    "checkpoint_dir = f\"{CHECKPOINT_PATH}/orders_bronze\"\n",
    "\n",
    "# TODO: Write stream\n",
    "query = (\n",
    "    df_stream.writeStream\n",
    "    .format(\"___\") # Target format\n",
    "    .option(\"checkpointLocation\", ___)\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(table_name)\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "print(f\"Processed batch 1 into {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a443936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data\n",
    "display(spark.table(table_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf7c075",
   "metadata": {},
   "source": [
    "## Schema Evolution & Rescue Data\n",
    "\n",
    "Data changes. New columns appear. Types change.\n",
    "Auto Loader handles this with **Schema Evolution** and **Rescued Data**.\n",
    "\n",
    "### Simulate \"Bad\" Data\n",
    "\n",
    "We will inject a file with:\n",
    "1.  A new column (`customer_mood`).\n",
    "2.  A malformed record (string in an integer field)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2291e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a file with unexpected data\n",
    "bad_data = [\n",
    "    {\"order_id\": 9001, \"total_amount\": 50.0, \"customer_mood\": \"happy\"}, # New column\n",
    "    {\"order_id\": 9002, \"total_amount\": \"INVALID\", \"customer_mood\": \"angry\"} # Type mismatch\n",
    "]\n",
    "spark.createDataFrame(bad_data).write.mode(\"overwrite\").json(f\"{SIMULATION_PATH}/bad_data_batch\")\n",
    "\n",
    "print(\"⚠️ Bad data batch arrived!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2e83a0",
   "metadata": {},
   "source": [
    "### Configure Rescue Mode\n",
    "\n",
    "Re-configure the stream to use `cloudFiles.schemaEvolutionMode` = `rescue`.\n",
    "This will put unexpected data into a `_rescued_data` column instead of failing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad47ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Re-configure stream with rescue mode\n",
    "df_stream_rescue = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", SCHEMA_PATH)\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"___\") # Enable Rescue Mode\n",
    "    .load(SIMULATION_PATH)\n",
    ")\n",
    "\n",
    "# Write again\n",
    "query_rescue = (\n",
    "    df_stream_rescue.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", checkpoint_dir)\n",
    "    .option(\"mergeSchema\", \"___\") # Allow Delta table schema to evolve\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(table_name)\n",
    ")\n",
    "\n",
    "query_rescue.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d83dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the table - look for _rescued_data column\n",
    "display(spark.table(table_name).filter(\"_rescued_data IS NOT NULL\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3237187",
   "metadata": {},
   "source": [
    "## Real-Time Aggregations (Watermarking)\n",
    "\n",
    "We want to count orders every 30 seconds.\n",
    "Since streams are infinite, we need **Watermarking** to tell Spark when to close a window and discard old state.\n",
    "\n",
    "### Define Windowed Aggregation\n",
    "\n",
    "1.  Add a timestamp column `processing_time`.\n",
    "2.  Define a watermark (e.g., \"1 minute\").\n",
    "3.  Group by `window(\"processing_time\", \"30 seconds\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6557ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare stream with timestamp\n",
    "df_with_time = df_stream_rescue.withColumn(\"processing_time\", current_timestamp())\n",
    "\n",
    "# TODO: Define aggregation\n",
    "orders_count = (\n",
    "    df_with_time\n",
    "    .withWatermark(\"___\", \"___\") # Column, threshold (e.g. \"1 minute\")\n",
    "    .groupBy(\n",
    "        window(\"___\", \"___\") # Column, window duration (e.g. \"30 seconds\")\n",
    "    )\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ac27bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start aggregation stream (Output Mode: Update)\n",
    "# We use processingTime trigger for continuous processing\n",
    "query_agg = (\n",
    "    orders_count.writeStream\n",
    "    .format(\"memory\") # Writing to memory for easy visualization\n",
    "    .queryName(\"orders_dashboard\")\n",
    "    .outputMode(\"update\")\n",
    "    .trigger(processingTime=\"5 seconds\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "print(\"Dashboard stream started...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2da828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate more data arrival to see updates\n",
    "for i in range(2, 5):\n",
    "    arrive_batch(i)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2748d2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the dashboard\n",
    "display(spark.sql(\"SELECT * FROM orders_dashboard ORDER BY window DESC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948f5a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_agg.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0edaf7",
   "metadata": {},
   "source": [
    "## Stream-Static Join\n",
    "\n",
    "Enrich the incoming orders with Customer data (which is a static Delta table).\n",
    "\n",
    "**Note:** Stream-Static joins are stateless on the static side (Spark reads the static table at the start of each micro-batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d181600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load static customers table\n",
    "customers_table = f\"{CATALOG}.{SILVER_SCHEMA}.customers_silver\"\n",
    "df_customers = spark.read.table(customers_table)\n",
    "\n",
    "# TODO: Join Stream (df_stream_rescue) with Static (df_customers)\n",
    "# Join on customer_id\n",
    "df_enriched = (\n",
    "    df_stream_rescue.alias(\"o\")\n",
    "    .join(\n",
    "        df_customers.alias(\"c\"),\n",
    "        col(\"___\") == col(\"___\"), # Join condition\n",
    "        \"___\" # Join type (left)\n",
    "    )\n",
    "    .select(\"o.order_id\", \"o.total_amount\", \"c.FullName\", \"c.EmailAddress\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1cbe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write enriched stream\n",
    "query_join = (\n",
    "    df_enriched.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/enriched_orders\")\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(f\"{CATALOG}.{BRONZE_SCHEMA}.orders_enriched\")\n",
    ")\n",
    "\n",
    "query_join.awaitTermination()\n",
    "\n",
    "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.orders_enriched\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fda829",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Stop all streams and remove temporary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c16cf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop any active streams\n",
    "for q in spark.streams.active:\n",
    "    q.stop()\n",
    "\n",
    "# Optional: Remove simulation files\n",
    "# dbutils.fs.rm(SIMULATION_PATH, True)\n",
    "# dbutils.fs.rm(CHECKPOINT_PATH, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc23273",
   "metadata": {},
   "source": [
    "# Solution\n",
    "\n",
    "The complete code is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10c79a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FULL SOLUTION - Workshop 4: Streaming\n",
    "# ============================================================\n",
    "\n",
    "# --- Task 2.1: Auto Loader ---\n",
    "df_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", SCHEMA_PATH)\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    "    .load(SIMULATION_PATH)\n",
    ")\n",
    "\n",
    "# --- Task 2.2: Write Stream ---\n",
    "query = (\n",
    "    df_stream.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/orders_bronze\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(f\"{CATALOG}.{BRONZE_SCHEMA}.orders_bronze\")\n",
    ")\n",
    "\n",
    "# --- Task 4.1: Aggregation ---\n",
    "df_with_time = df_stream.withColumn(\"processing_time\", current_timestamp())\n",
    "\n",
    "orders_count = (\n",
    "    df_with_time\n",
    "    .withWatermark(\"processing_time\", \"1 minute\")\n",
    "    .groupBy(window(\"processing_time\", \"30 seconds\"))\n",
    "    .count()\n",
    ")\n",
    "\n",
    "# --- Task 5: Stream-Static Join ---\n",
    "df_customers = spark.read.table(f\"{CATALOG}.{SILVER_SCHEMA}.customers_silver\")\n",
    "\n",
    "df_enriched = (\n",
    "    df_stream.alias(\"o\")\n",
    "    .join(df_customers.alias(\"c\"), col(\"o.customer_id\") == col(\"c.CustomerID\"), \"left\")\n",
    "    .select(\"o.order_id\", \"o.total_amount\", \"c.FullName\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
