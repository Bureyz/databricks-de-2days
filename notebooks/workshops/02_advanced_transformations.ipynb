{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82829b2a-f055-4c7f-9cc7-35cfd8db248b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Advanced Transformations (PySpark & SQL)\n",
    "\n",
    "**Workshop Objective:**\n",
    "- Practical application of Window Functions (lag, lead, rank, rolling aggregations)\n",
    "- Processing complex structures (JSON, arrays, structs)\n",
    "- Advanced date and time operations\n",
    "- Transformation optimization for performance\n",
    "\n",
    "**Note:** You can choose to solve the tasks using **PySpark** or **SQL**. Both approaches are provided.\n",
    "\n",
    "**Time:** 30 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa229e54-de72-4937-844e-262de00bce11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## User Isolation\n",
    "This notebook is designed to be run in a shared environment.\n",
    "To avoid conflicts, we will use a unique `catalog` and `schema` for your user.\n",
    "The `00_setup` script will automatically configure these for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "def2badb-7ba4-483f-9970-e34aa9547e66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Environment Configuration\n",
    "We will configure the environment variables and paths used in this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1188c3a5-7ecf-423f-9ed8-3ba871a7ada9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61f21d3b-f241-4b44-abbc-ee8c12810752",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b073135-79fe-4a82-aa24-26bd295d7c7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Display user context\n",
    "print(\"=== User Context ===\")\n",
    "print(f\"Catalog: {CATALOG}\")\n",
    "print(f\"Schema: {BRONZE_SCHEMA}\")\n",
    "print(f\"User: {raw_user}\")\n",
    "\n",
    "# Set catalog and schema as default\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f728938-9c02-4e38-a859-7a1cf420d2a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Preparation from Databricks Volume\n",
    "\n",
    "Load data from Databricks Volume for the workshop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a54387b-d01b-4082-b53a-cc1300523fa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load customer data\n",
    "customers_df = spark.read.csv(f\"{volume_path}/customers/customers.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0164c86c-5c1d-4e67-abc8-a27e2635b1bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate sample orders data for the workshop\n",
    "data = [\n",
    "    (1, \"2024-01-01\", 100), (1, \"2024-01-02\", 150), (1, \"2024-01-02\", 150), (1, \"2024-01-05\", 200),\n",
    "    (2, \"2024-01-10\", 50), (2, \"2024-01-12\", 80),\n",
    "    (3, \"2024-02-01\", 300), (3, \"2024-02-01\", 300), (3, \"2024-02-05\", 350)\n",
    "]\n",
    "columns = [\"customer_id\", \"order_date\", \"total_amount\"]\n",
    "test_orders = spark.createDataFrame(data, columns)\n",
    "test_orders = test_orders.withColumn(\"order_date\", F.to_date(\"order_date\"))\n",
    "\n",
    "# Register as temp view for SQL exercises\n",
    "test_orders.createOrReplaceTempView(\"orders\")\n",
    "print(\"Created 'orders' temporary view for SQL exercises.\")\n",
    "display(test_orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58fd828e-b196-4212-8453-2b07672b3f53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Part A: PySpark Implementation\n",
    "\n",
    "In this section, you will implement the transformations using the PySpark DataFrame API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a91d5ff6-e890-42e3-880d-1cc57f0501dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load JSON data from Volume (orders may contain nested structures)\n",
    "# Volume already contains parsed JSON, but we can create an example with nested structure\n",
    "\n",
    "\n",
    "# For practice, create test data with nested JSON string\n",
    "json_data = spark.createDataFrame([\n",
    " (1, '{\"items\": [{\"product\": \"laptop\", \"price\": 1200}, {\"product\": \"mouse\", \"price\": 25}], \"total\": 1225}'),\n",
    " (2, '{\"items\": [{\"product\": \"keyboard\", \"price\": 80}], \"total\": 80}'),\n",
    " (3, '{\"items\": [{\"product\": \"monitor\", \"price\": 350}, {\"product\": \"cable\", \"price\": 15}], \"total\": 365}')\n",
    "], [\"order_id\", \"order_json\"])\n",
    "\n",
    "# Register for SQL\n",
    "json_data.createOrReplaceTempView(\"json_orders_raw\")\n",
    "\n",
    "display(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20b7d289-e961-44ae-9dfb-310f43d8b9b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Window Functions\n",
    "\n",
    "### Ranking - ROW_NUMBER, RANK, DENSE_RANK\n",
    "\n",
    "**Instructions:**\n",
    "1. For each customer, rank orders by date (newest first)\n",
    "2. Add columns:\n",
    " - `row_num`: using `row_number()`\n",
    " - `rank`: using `rank()`\n",
    " - `dense_rank`: using `dense_rank()`\n",
    "3. Window spec: `partitionBy(\"customer_id\").orderBy(F.desc(\"order_date\"))`\n",
    "\n",
    "**Expected Result:**\n",
    "- Each customer has orders numbered starting from 1 (newest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6c5c409-6155-4c1c-a9ac-f7417cff8f3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Task 1.1 - Ranking functions\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define window spec\n",
    "window_spec = Window.____(\"____\").orderBy(F.____(\"____\")) # partitionBy customer_id, orderBy desc order_date\n",
    "\n",
    "# Add ranking columns\n",
    "orders_ranked = (\n",
    " test_orders\n",
    " .withColumn(\"row_num\", F.____().____(window_spec)) # row_number, over\n",
    " .withColumn(\"rank\", F.____().over(____)) # rank, window_spec\n",
    " .withColumn(\"dense_rank\", F.____().over(window_spec)) # dense_rank\n",
    ")\n",
    "\n",
    "display(orders_ranked.orderBy(\"customer_id\", \"order_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9acb6b0d-dfe5-472d-94c5-59e2b48f0826",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Explanation of differences:**\n",
    "\n",
    "- **ROW_NUMBER**: Unique sequential numbers (1, 2, 3...)\n",
    "- **RANK**: Gaps in numbering for ties (1, 2, 2, 4...)\n",
    "- **DENSE_RANK**: No gaps for ties (1, 2, 2, 3...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d326f152-14cb-4032-b565-72fcd8728769",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### LAG and LEAD - Compare with previous/next values\n",
    "\n",
    "**Instructions:**\n",
    "1. For each customer, calculate:\n",
    " - `previous_order_amount`: value of the previous order (using `lag`)\n",
    " - `next_order_amount`: value of the next order (using `lead`)\n",
    " - `amount_diff_vs_previous`: difference between current and previous\n",
    "2. Window spec: `partitionBy(\"customer_id\").orderBy(\"order_date\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a7c1fe1-e9a5-4318-9cf2-3a4cc3ee6f29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Task 1.2 - LAG and LEAD\n",
    "\n",
    "# Window spec - chronological order\n",
    "window_chrono = Window.partitionBy(\"____\").orderBy(\"____\") # customer_id, order_date\n",
    "\n",
    "# Use LAG and LEAD\n",
    "orders_lag_lead = (\n",
    " test_orders\n",
    " .withColumn(\"previous_order_amount\", F.____(____, ____).over(____)) # lag, total_amount, 1, window_chrono\n",
    " .withColumn(\"next_order_amount\", F.____(____, 1).over(window_chrono)) # lead, total_amount\n",
    " .withColumn(\n",
    " \"amount_diff_vs_previous\",\n",
    " F.col(\"____\") - F.col(\"____\") # total_amount, previous_order_amount\n",
    " )\n",
    ")\n",
    "\n",
    "display(orders_lag_lead.select(\n",
    " \"customer_id\", \"order_date\", \"total_amount\", \n",
    " \"previous_order_amount\", \"next_order_amount\", \"amount_diff_vs_previous\"\n",
    ").orderBy(\"customer_id\", \"order_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85fe3457-f149-4827-bb80-2ce51b418732",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Rolling Aggregations - Moving Averages\n",
    "\n",
    "**Instructions:**\n",
    "1. Calculate rolling average for order amount:\n",
    " - Window: 3 last orders (current + 2 previous)\n",
    "2. Use `.rowsBetween(-2, 0)` for window spec\n",
    "3. Add column `rolling_avg_3_orders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "498928d7-bb6d-4e91-81b8-e80d022fb241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Task 1.3 - Rolling aggregations\n",
    "\n",
    "# Window spec with rowsBetween\n",
    "window_rolling = (\n",
    " Window\n",
    " .partitionBy(\"customer_id\")\n",
    " .orderBy(\"order_date\")\n",
    " .____(____, ____) # rowsBetween, -2, 0 (3 last records)\n",
    ")\n",
    "\n",
    "# Rolling average\n",
    "orders_rolling = (\n",
    " test_orders\n",
    " .withColumn(\n",
    " \"rolling_avg_3_orders\",\n",
    " F.____(\"____\").over(____) # avg, total_amount, window_rolling\n",
    " )\n",
    " .withColumn(\n",
    " \"rolling_sum_3_orders\",\n",
    " F.sum(\"total_amount\").over(window_rolling)\n",
    " )\n",
    ")\n",
    "\n",
    "display(orders_rolling.select(\n",
    " \"customer_id\", \"order_date\", \"total_amount\", \n",
    " \"rolling_avg_3_orders\", \"rolling_sum_3_orders\"\n",
    ").orderBy(\"customer_id\", \"order_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b0deed8-d02d-4c1b-94a1-2be56d0fa51e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cumulative Sum\n",
    "\n",
    "**Instructions:**\n",
    "1. Calculate cumulative sum of order amounts per customer\n",
    "2. Use `.rowsBetween(Window.unboundedPreceding, Window.currentRow)`\n",
    "3. Add column `cumulative_amount`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b50160e3-feb9-413f-9735-3d1dbd7f10c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Task 1.4 - Cumulative sum\n",
    "\n",
    "# Window spec for cumulative\n",
    "window_cumulative = (\n",
    " Window\n",
    " .partitionBy(\"____\")\n",
    " .orderBy(\"____\")\n",
    " .rowsBetween(Window.____, Window.____) # unboundedPreceding, currentRow\n",
    ")\n",
    "\n",
    "# Cumulative sum\n",
    "orders_cumulative = (\n",
    " test_orders\n",
    " .withColumn(\n",
    " \"cumulative_amount\",\n",
    " F.____(____(\"____\")).over(window_cumulative) # round, sum total_amount\n",
    " )\n",
    ")\n",
    "\n",
    "display(orders_cumulative.select(\n",
    " \"customer_id\", \"order_date\", \"total_amount\", \"cumulative_amount\"\n",
    ").orderBy(\"customer_id\", \"order_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9e14ed7-0f57-4b60-bdfe-b3bdff8fce46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Processing Complex Structures\n",
    "\n",
    "### JSON Processing - from_json() and explode()\n",
    "\n",
    "**Instructions:**\n",
    "1. Load JSON data from Volume (orders)\n",
    "2. Use `from_json()` to parse JSON if needed\n",
    "3. Use `explode()` to \"unpack\" array\n",
    "4. Extract fields from nested struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "949b4b56-3c77-4244-bbc3-44ded9b13d3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Task 2.1 - JSON processing\n",
    "\n",
    "# Define JSON schema\n",
    "json_schema = StructType([\n",
    " StructField(\"items\", ArrayType(StructType([\n",
    " StructField(\"product\", StringType()),\n",
    " StructField(\"price\", IntegerType())\n",
    " ]))),\n",
    " StructField(\"total\", IntegerType())\n",
    "])\n",
    "\n",
    "# Parse JSON\n",
    "orders_parsed = (\n",
    " json_data\n",
    " .withColumn(\"parsed\", F.____(____(\"____\"), ____)) # from_json, order_json, json_schema\n",
    ")\n",
    "\n",
    "display(orders_parsed.select(\"order_id\", \"parsed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93ee9278-f763-4445-8b39-333f2b3008b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Explode array and extract fields\n",
    "\n",
    "orders_exploded = (\n",
    " orders_parsed\n",
    " .withColumn(\"item\", F.____(\"____\")) # explode, parsed.items\n",
    " .select(\n",
    " \"order_id\",\n",
    " F.col(\"____\").alias(\"product_name\"), # item.product\n",
    " F.col(\"____\").alias(\"product_price\"), # item.price\n",
    " F.col(\"____\").alias(\"order_total\") # parsed.total\n",
    " )\n",
    ")\n",
    "\n",
    "display(orders_exploded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97b1150a-7ddd-4828-b59f-9170215d9282",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Array Functions - collect_list, array_contains\n",
    "\n",
    "**Instructions:**\n",
    "1. Group orders per customer\n",
    "2. Use `collect_list()` to gather all order amounts into an array\n",
    "3. Use `array_contains()` to check if customer has an order > 500\n",
    "4. Use `size()` to count number of orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c82c044-13ed-4c68-a687-2c9073a0cd52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Task 2.2 - Array functions\n",
    "\n",
    "customer_arrays = (\n",
    " test_orders\n",
    " .groupBy(\"____\") # customer_id\n",
    " .agg(\n",
    " F.____(____(\"____\")).alias(\"order_amounts\"), # collect_list, total_amount\n",
    " F.collect_list(\"order_date\").alias(\"order_dates\"),\n",
    " F.count(\"*\").alias(\"total_orders\")\n",
    " )\n",
    " .withColumn(\n",
    " \"num_orders\",\n",
    " F.____(\"____\") # size, order_amounts\n",
    " )\n",
    ")\n",
    "\n",
    "display(customer_arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c47b2194-b1ed-41e6-85dc-4db44789cfac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Struct - Combining columns into structures\n",
    "\n",
    "**Instructions:**\n",
    "1. Create struct `customer_info` containing: customer_id, total_orders\n",
    "2. Create struct `order_summary` containing: min/max/avg amount\n",
    "3. Extract fields from struct using `.` notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ead1b07-9de4-4cc4-a8e8-67f6f41d60c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Task 2.3 - Struct operations\n",
    "\n",
    "customer_structs = (\n",
    " test_orders\n",
    " .groupBy(\"customer_id\")\n",
    " .agg(\n",
    " F.count(\"*\").alias(\"total_orders\"),\n",
    " F.min(\"total_amount\").alias(\"min_amount\"),\n",
    " F.max(\"total_amount\").alias(\"max_amount\"),\n",
    " F.avg(\"total_amount\").alias(\"avg_amount\")\n",
    " )\n",
    " .withColumn(\n",
    " \"customer_info\",\n",
    " F.____(\"____\", \"____\") # struct, customer_id, total_orders\n",
    " )\n",
    " .withColumn(\n",
    " \"order_summary\",\n",
    " F.struct(\"min_amount\", \"____\", \"____\") # max_amount, avg_amount\n",
    " )\n",
    ")\n",
    "\n",
    "display(customer_structs.select(\"customer_info\", \"order_summary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15798915-fb8a-4651-9b29-c4035fb13705",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract fields from struct\n",
    "customer_flat = (\n",
    " customer_structs\n",
    " .select(\n",
    " F.col(\"customer_info.____\").alias(\"customer_id\"), # customer_id\n",
    " F.col(\"order_summary.____\").alias(\"avg_order_value\") # avg_amount\n",
    " )\n",
    ")\n",
    "\n",
    "display(customer_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2110332b-b95c-44aa-b770-45ccd1135776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Part 3: Advanced Date Operations\n",
    "\n",
    "### Task 3.1: Date truncation and extraction\n",
    "\n",
    "**Instructions:**\n",
    "1. Use `date_trunc()` to round dates to: month, quarter, year\n",
    "2. Use `year()`, `month()`, `dayofweek()` to extract date parts\n",
    "3. Calculate `days_since_order` (difference between today and order date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e623e42e-103c-4ade-a9e7-38e968e08784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Task 3.1 - Date functions\n",
    "\n",
    "orders_dates = (\n",
    " test_orders\n",
    " .withColumn(\"order_month\", F.____(____(\"____\"), \"____\")) # date_trunc, order_date, month\n",
    " .withColumn(\"order_quarter\", F.date_trunc(\"____\", \"order_date\")) # quarter\n",
    " .withColumn(\"order_year_num\", F.____(\"____\")) # year, order_date\n",
    " .withColumn(\"order_month_num\", F.____(____(\"____\"))) # month, order_date\n",
    " .withColumn(\"day_of_week\", F.____(____(\"order_date\"))) # dayofweek\n",
    " .withColumn(\n",
    " \"days_since_order\",\n",
    " F.datediff(F.____, \"____\") # current_date, order_date\n",
    " )\n",
    ")\n",
    "\n",
    "display(orders_dates.select(\n",
    " \"order_id\", \"order_date\", \"order_month\", \"order_quarter\",\n",
    " \"order_year_num\", \"order_month_num\", \"day_of_week\", \"days_since_order\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f0ada69-1f80-4f2a-a932-0632e68cdaa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 3.2: Date arithmetic - adding/subtracting periods\n",
    "\n",
    "**Instructions:**\n",
    "1. Use `date_add()` to add 30 days to order date\n",
    "2. Use `add_months()` to add 3 months\n",
    "3. Use `last_day()` to get the last day of the month\n",
    "4. Use `next_day()` to get the next Monday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd685d7c-d9a9-496e-ae6c-f012965ebdc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Task 3.2 - Date arithmetic\n",
    "\n",
    "orders_date_math = (\n",
    " test_orders\n",
    " .withColumn(\"delivery_date_estimate\", F.____(____(\"____\"), ____)) # date_add, order_date, 30\n",
    " .withColumn(\"renewal_date\", F.____(____(\"order_date\"), ____)) # add_months, 3\n",
    " .withColumn(\"month_end\", F.____(____(\"____\"))) # last_day, order_date\n",
    " .withColumn(\"next_monday\", F.next_day(\"____\", \"____\")) # order_date, Monday\n",
    ")\n",
    "\n",
    "display(orders_date_math.select(\n",
    " \"order_date\", \"delivery_date_estimate\", \"renewal_date\", \n",
    " \"month_end\", \"next_monday\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19f1caff-d17e-4746-ac4a-18ba3b085f02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 3.3: Generating date sequences\n",
    "\n",
    "**Instructions:**\n",
    "1. Use `sequence()` to generate an array of dates between two dates\n",
    "2. Use `explode()` to create one row per date\n",
    "3. Create a calendar table with all days between min and max order_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01bf5abc-b87e-4255-bdca-03684729cfed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Task 3.3 - Date sequences\n",
    "\n",
    "# Find min and max dates\n",
    "date_range = test_orders.select(\n",
    " F.min(\"order_date\").alias(\"min_date\"),\n",
    " F.max(\"order_date\").alias(\"max_date\")\n",
    ").first()\n",
    "\n",
    "# Generate date sequence\n",
    "calendar = (\n",
    " spark.range(1)\n",
    " .select(\n",
    " F.____( # explode\n",
    " F.____(\n",
    " F.lit(date_range[\"____\"]), # min_date\n",
    " F.lit(date_range[\"max_date\"]),\n",
    " F.expr(\"____\") # interval 1 day\n",
    " )\n",
    " ).alias(\"date\")\n",
    " )\n",
    " .withColumn(\"year\", F.year(\"date\"))\n",
    " .withColumn(\"month\", F.____(____(\"____\"))) # month, date\n",
    " .withColumn(\"day_of_week\", F.dayofweek(\"date\"))\n",
    ")\n",
    "\n",
    "print(f\"Calendar table: {calendar.count()} days\")\n",
    "display(calendar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02b04380-eb57-4384-b6b5-dffeeaa08a05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Part B: SQL Implementation\n",
    "\n",
    "In this section, you will implement the same transformations using **Spark SQL**.\n",
    "We have already registered the `orders` and `json_orders_raw` temporary views for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "133d891b-8d1a-4653-8833-906da0c6534e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ranking - ROW_NUMBER, RANK, DENSE_RANK\n",
    "\n",
    "**Instructions:**\n",
    "1. For each customer, rank orders by date (newest first)\n",
    "2. Add columns:\n",
    " - `row_num`: using `row_number()`\n",
    " - `rank`: using `rank()`\n",
    " - `dense_rank`: using `dense_rank()`\n",
    "3. Window spec: `partitionBy(\"customer_id\").orderBy(F.desc(\"order_date\"))`\n",
    "\n",
    "**Expected Result:**\n",
    "- Each customer has orders numbered starting from 1 (newest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b70d6c08-ca3e-4263-a9fe-ca87c8ae1614",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Task 1.1 - Ranking functions (SQL)\n",
    "SELECT \n",
    "    *,\n",
    "    ___ OVER (PARTITION BY ___ ORDER BY ___ DESC) as row_num,\n",
    "    ___ OVER (PARTITION BY ___ ORDER BY ___ DESC) as rank,\n",
    "    ___ OVER (PARTITION BY ___ ORDER BY ___ DESC) as dense_rank\n",
    "FROM orders\n",
    "ORDER BY customer_id, order_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22783681-0e78-4977-ae69-146ab3202ea4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Explanation of differences:**\n",
    "\n",
    "- **ROW_NUMBER**: Unique sequential numbers (1, 2, 3...)\n",
    "- **RANK**: Gaps in numbering for ties (1, 2, 2, 4...)\n",
    "- **DENSE_RANK**: No gaps for ties (1, 2, 2, 3...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbc00409-cd84-4292-b6a0-3cd1d3b00ee4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### LAG and LEAD - Compare with previous/next values\n",
    "\n",
    "**Instructions:**\n",
    "1. For each customer, calculate:\n",
    " - `previous_order_amount`: value of the previous order (using `lag`)\n",
    " - `next_order_amount`: value of the next order (using `lead`)\n",
    " - `amount_diff_vs_previous`: difference between current and previous\n",
    "2. Window spec: `partitionBy(\"customer_id\").orderBy(\"order_date\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1f65bfd-6f54-4fd4-8b01-004c06b323e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Task 1.2 - LAG and LEAD (SQL)\n",
    "SELECT \n",
    "    customer_id, order_date, total_amount,\n",
    "    ___(total_amount) OVER (PARTITION BY customer_id ORDER BY order_date) as previous_order_amount,\n",
    "    ___(total_amount) OVER (PARTITION BY customer_id ORDER BY order_date) as next_order_amount,\n",
    "    total_amount - ___(total_amount) OVER (PARTITION BY customer_id ORDER BY order_date) as amount_diff_vs_previous\n",
    "FROM orders\n",
    "ORDER BY customer_id, order_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7212a3d2-cb24-40b1-8096-5bc4b6541576",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Rolling Aggregations - Moving Averages\n",
    "\n",
    "**Instructions:**\n",
    "1. Calculate rolling average for order amount:\n",
    " - Window: 3 last orders (current + 2 previous)\n",
    "2. Use `.rowsBetween(-2, 0)` for window spec\n",
    "3. Add column `rolling_avg_3_orders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbf8665c-9b84-48f9-b24f-bfba8c97f0de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Task 1.3 - Rolling aggregations (SQL)\n",
    "SELECT \n",
    "    customer_id, order_date, total_amount,\n",
    "    AVG(total_amount) OVER (PARTITION BY customer_id ORDER BY order_date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) as rolling_avg_3_orders,\n",
    "    SUM(total_amount) OVER (PARTITION BY customer_id ORDER BY order_date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) as rolling_sum_3_orders\n",
    "FROM orders\n",
    "ORDER BY customer_id, order_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5235712-540f-48f3-a4be-9cb156381125",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cumulative Sum\n",
    "\n",
    "**Instructions:**\n",
    "1. Calculate cumulative sum of order amounts per customer\n",
    "2. Use `.rowsBetween(Window.unboundedPreceding, Window.currentRow)`\n",
    "3. Add column `cumulative_amount`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "affceb10-7b6a-4c1a-9f44-4f4eb7ca13c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Task 1.4 - Cumulative sum (SQL)\n",
    "SELECT \n",
    "    customer_id, order_date, total_amount,\n",
    "    SUM(total_amount) OVER (PARTITION BY customer_id ORDER BY order_date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as cumulative_amount\n",
    "FROM orders\n",
    "ORDER BY customer_id, order_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5605923e-cf53-4895-8c2e-2f53241e9ce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### JSON Processing - from_json() and explode()\n",
    "\n",
    "**Instructions:**\n",
    "1. Load JSON data from Volume (orders)\n",
    "2. Use `from_json()` to parse JSON if needed\n",
    "3. Use `explode()` to \"unpack\" array\n",
    "4. Extract fields from nested struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e26905bd-1eb1-488f-bc48-cd226fafef99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Task 2.1 - JSON processing (SQL)\n",
    "SELECT \n",
    "    order_id,\n",
    "    from_json(order_json, 'items ARRAY<STRUCT<product: STRING, price: INT>>, total INT') as parsed\n",
    "FROM json_orders_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85d50f1f-9703-451e-b00d-863c29bf3879",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Explode array and extract fields (SQL)\n",
    "-- Note: We need to parse first, then explode. In SQL we can do it in one query or use CTE.\n",
    "WITH parsed_data AS (\n",
    "  SELECT order_id, from_json(order_json, 'items ARRAY<STRUCT<product: STRING, price: INT>>, total INT') as parsed\n",
    "  FROM json_orders_raw\n",
    ")\n",
    "SELECT \n",
    "    order_id,\n",
    "    item.product as product_name,\n",
    "    item.price as product_price,\n",
    "    parsed.total as order_total\n",
    "FROM parsed_data\n",
    "LATERAL VIEW explode(parsed.items) AS item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "227832f8-3b98-49cf-9d79-318fe9f958e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Array Functions - collect_list, array_contains\n",
    "\n",
    "**Instructions:**\n",
    "1. Group orders per customer\n",
    "2. Use `collect_list()` to gather all order amounts into an array\n",
    "3. Use `array_contains()` to check if customer has an order > 500\n",
    "4. Use `size()` to count number of orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcdafd39-8743-44f9-8b81-fd0cc9db1203",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Task 2.2 - Array functions (SQL)\n",
    "SELECT \n",
    "    customer_id,\n",
    "    collect_list(total_amount) as order_amounts,\n",
    "    collect_list(order_date) as order_dates,\n",
    "    count(*) as total_orders,\n",
    "    size(collect_list(total_amount)) as num_orders\n",
    "FROM orders\n",
    "GROUP BY customer_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ca04fad-8d02-4d0f-86b3-325208dbb7fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Struct - Combining columns into structures\n",
    "\n",
    "**Instructions:**\n",
    "1. Create struct `customer_info` containing: customer_id, total_orders\n",
    "2. Create struct `order_summary` containing: min/max/avg amount\n",
    "3. Extract fields from struct using `.` notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d617e508-bfcd-42e9-9a02-15137ebe8c4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Task 2.3 - Struct operations (SQL)\n",
    "SELECT \n",
    "    struct(customer_id, count(*) as total_orders) as customer_info,\n",
    "    struct(min(total_amount) as min_amount, max(total_amount) as max_amount, avg(total_amount) as avg_amount) as order_summary\n",
    "FROM orders\n",
    "GROUP BY customer_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85687e8f-3384-4f23-8e97-b4de2c452835",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Extract fields from struct (SQL)\n",
    "-- Assuming we have the structs from previous query (using CTE for demo)\n",
    "WITH struct_data AS (\n",
    "    SELECT \n",
    "        struct(customer_id, count(*) as total_orders) as customer_info,\n",
    "        struct(min(total_amount) as min_amount, max(total_amount) as max_amount, avg(total_amount) as avg_amount) as order_summary\n",
    "    FROM orders\n",
    "    GROUP BY customer_id\n",
    ")\n",
    "SELECT \n",
    "    customer_info.customer_id,\n",
    "    order_summary.avg_amount as avg_order_value\n",
    "FROM struct_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "145b2e64-34d7-42d6-b406-4209eb160fbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 3.1: Date truncation and extraction\n",
    "\n",
    "**Instructions:**\n",
    "1. Use `date_trunc()` to round dates to: month, quarter, year\n",
    "2. Use `year()`, `month()`, `dayofweek()` to extract date parts\n",
    "3. Calculate `days_since_order` (difference between today and order date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb446737-5f30-4329-aea1-49649da6cd73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Task 3.1 - Date functions (SQL)\n",
    "SELECT \n",
    "    customer_id, order_date,\n",
    "    date_trunc('month', order_date) as order_month,\n",
    "    date_trunc('quarter', order_date) as order_quarter,\n",
    "    year(order_date) as order_year_num,\n",
    "    month(order_date) as order_month_num,\n",
    "    dayofweek(order_date) as day_of_week,\n",
    "    datediff(current_date(), order_date) as days_since_order\n",
    "FROM orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7663384-c8c5-411c-a3b5-672bd5996a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 3.2: Date arithmetic - adding/subtracting periods\n",
    "\n",
    "**Instructions:**\n",
    "1. Use `date_add()` to add 30 days to order date\n",
    "2. Use `add_months()` to add 3 months\n",
    "3. Use `last_day()` to get the last day of the month\n",
    "4. Use `next_day()` to get the next Monday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b4329bb-9639-46d9-b985-e8a83d16074d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: Task 3.2 - Date arithmetic (SQL)\n",
    "SELECT \n",
    "    order_date,\n",
    "    date_add(order_date, 30) as delivery_date_estimate,\n",
    "    add_months(order_date, 3) as renewal_date,\n",
    "    last_day(order_date) as month_end,\n",
    "    next_day(order_date, 'Monday') as next_monday\n",
    "FROM orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9525c53-1868-49fd-982d-2f26022045af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 3.3: Generating date sequences\n",
    "\n",
    "**Instructions:**\n",
    "1. Use `sequence()` to generate an array of dates between two dates\n",
    "2. Use `explode()` to create one row per date\n",
    "3. Create a calendar table with all days between min and max order_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c7a682d-1286-4a20-8558-ba21f2f85017",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- TODO: Task 3.3 - Date sequences (SQL)\n",
    "WITH range AS (\n",
    "  SELECT min(order_date) as min_date, max(order_date) as max_date FROM orders\n",
    ")\n",
    "SELECT \n",
    "    explode(sequence(min_date, max_date, interval 1 day)) as date\n",
    "FROM range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e96c629-965b-4bd5-b275-523ec9caf610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Workshop Summary\n",
    "\n",
    "**Objectives Achieved:**\n",
    "- Window Functions (ranking, lag/lead, rolling aggregations, cumulative sum)\n",
    "- JSON Processing (from_json, explode, struct)\n",
    "- Array operations (collect_list, array_contains, size)\n",
    "- Advanced date operations (truncation, arithmetic, sequences)\n",
    "\n",
    "**Key Takeaways:**\n",
    "1. Window Functions allow per-group analysis without GROUP BY\n",
    "2. JSON and complex structures are native in Spark\n",
    "3. Date functions enable advanced temporal analysis\n",
    "4. Optimization: use broadcast for small tables in JOIN\n",
    "\n",
    "**Best Practices:**\n",
    "- Window Functions: always define explicit window spec\n",
    "- JSON: use schema inference only for exploration\n",
    "- Dates: use native date types (not string)\n",
    "- Performance: cache() for frequently used DataFrames\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0e1eee3-8cfe-458e-9e28-5a56357162ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Solution\n",
    "\n",
    "The complete code is below. Try to solve it yourself first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6232db73-6d85-4a4b-97e6-82f4f2bb1d12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- ============================================================\n",
    "-- FULL SOLUTION - Workshop 2: SQL\n",
    "-- ============================================================\n",
    "\n",
    "-- Task 1.1: Ranking\n",
    "SELECT *, \n",
    "  row_number() OVER (PARTITION BY customer_id ORDER BY order_date DESC) as row_num,\n",
    "  rank() OVER (PARTITION BY customer_id ORDER BY order_date DESC) as rank,\n",
    "  dense_rank() OVER (PARTITION BY customer_id ORDER BY order_date DESC) as dense_rank\n",
    "FROM orders;\n",
    "\n",
    "-- Task 1.2: LAG/LEAD\n",
    "SELECT *, \n",
    "  lag(total_amount) OVER (PARTITION BY customer_id ORDER BY order_date) as prev_amt,\n",
    "  lead(total_amount) OVER (PARTITION BY customer_id ORDER BY order_date) as next_amt\n",
    "FROM orders;\n",
    "\n",
    "-- Task 1.3: Rolling Aggregations\n",
    "SELECT *, \n",
    "  avg(total_amount) OVER (PARTITION BY customer_id ORDER BY order_date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) as rolling_avg \n",
    "FROM orders;\n",
    "\n",
    "-- Task 2.1: JSON\n",
    "SELECT from_json(order_json, 'items ARRAY<STRUCT<product: STRING, price: INT>>, total INT') as parsed FROM json_orders_raw;\n",
    "\n",
    "-- Task 2.2: Arrays\n",
    "SELECT customer_id, collect_list(total_amount) as amounts, size(collect_list(total_amount)) as count FROM orders GROUP BY customer_id;\n",
    "\n",
    "-- Task 3.1: Dates\n",
    "SELECT order_date, date_trunc('month', order_date) as mth, datediff(current_date(), order_date) as diff FROM orders;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dc5a70a-ba87-4af1-b6e2-3f62a213a5c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FULL SOLUTION - Workshop 2: Advanced Transformations (PySpark)\n",
    "# ============================================================\n",
    "\n",
    "# --- Task 1.1: Ranking ---\n",
    "window_spec = Window.partitionBy(\"customer_id\").orderBy(F.desc(\"order_date\"))\n",
    "orders_ranked = (\n",
    "    test_orders\n",
    "    .withColumn(\"row_num\", F.row_number().over(window_spec))\n",
    "    .withColumn(\"rank\", F.rank().over(window_spec))\n",
    "    .withColumn(\"dense_rank\", F.dense_rank().over(window_spec))\n",
    ")\n",
    "\n",
    "# --- Task 1.2: LAG/LEAD ---\n",
    "window_chrono = Window.partitionBy(\"customer_id\").orderBy(\"order_date\")\n",
    "orders_lag_lead = (\n",
    "    test_orders\n",
    "    .withColumn(\"previous_order_amount\", F.lag(\"total_amount\", 1).over(window_chrono))\n",
    "    .withColumn(\"next_order_amount\", F.lead(\"total_amount\", 1).over(window_chrono))\n",
    "    .withColumn(\"amount_diff_vs_previous\", F.col(\"total_amount\") - F.col(\"previous_order_amount\"))\n",
    ")\n",
    "\n",
    "# --- Task 1.3: Rolling Aggregations ---\n",
    "window_rolling = Window.partitionBy(\"customer_id\").orderBy(\"order_date\").rowsBetween(-2, 0)\n",
    "orders_rolling = (\n",
    "    test_orders\n",
    "    .withColumn(\"rolling_avg_3_orders\", F.avg(\"total_amount\").over(window_rolling))\n",
    "    .withColumn(\"rolling_sum_3_orders\", F.sum(\"total_amount\").over(window_rolling))\n",
    ")\n",
    "\n",
    "# --- Task 1.4: Cumulative Sum ---\n",
    "window_cumulative = Window.partitionBy(\"customer_id\").orderBy(\"order_date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "orders_cumulative = (\n",
    "    test_orders\n",
    "    .withColumn(\"cumulative_amount\", F.sum(\"total_amount\").over(window_cumulative))\n",
    ")\n",
    "\n",
    "# --- Task 2.1: JSON Processing ---\n",
    "json_schema = \"items ARRAY<STRUCT<product: STRING, price: INT>>, total INT\"\n",
    "orders_parsed = json_data.withColumn(\"parsed\", F.from_json(\"order_json\", json_schema))\n",
    "\n",
    "# --- Task 2.2: Array Functions ---\n",
    "customer_arrays = (\n",
    "    test_orders\n",
    "    .groupBy(\"customer_id\")\n",
    "    .agg(\n",
    "        F.collect_list(\"total_amount\").alias(\"order_amounts\"),\n",
    "        F.collect_list(\"order_date\").alias(\"order_dates\"),\n",
    "        F.count(\"*\").alias(\"total_orders\")\n",
    "    )\n",
    "    .withColumn(\"num_orders\", F.size(\"order_amounts\"))\n",
    ")\n",
    "\n",
    "# --- Task 2.3: Struct Operations ---\n",
    "customer_structs = (\n",
    "    test_orders\n",
    "    .groupBy(\"customer_id\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"total_orders\"),\n",
    "        F.min(\"total_amount\").alias(\"min_amount\"),\n",
    "        F.max(\"total_amount\").alias(\"max_amount\"),\n",
    "        F.avg(\"total_amount\").alias(\"avg_amount\")\n",
    "    )\n",
    "    .withColumn(\"customer_info\", F.struct(\"customer_id\", \"total_orders\"))\n",
    "    .withColumn(\"order_summary\", F.struct(\"min_amount\", \"max_amount\", \"avg_amount\"))\n",
    ")\n",
    "\n",
    "# --- Task 3.1: Date Functions ---\n",
    "orders_dates = (\n",
    "    test_orders\n",
    "    .withColumn(\"order_month\", F.date_trunc(\"month\", \"order_date\"))\n",
    "    .withColumn(\"order_quarter\", F.date_trunc(\"quarter\", \"order_date\"))\n",
    "    .withColumn(\"order_year_num\", F.year(\"order_date\"))\n",
    "    .withColumn(\"order_month_num\", F.month(\"order_date\"))\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(\"order_date\"))\n",
    "    .withColumn(\"days_since_order\", F.datediff(F.current_date(), \"order_date\"))\n",
    ")\n",
    "\n",
    "# --- Task 3.2: Date Arithmetic ---\n",
    "orders_date_math = (\n",
    "    test_orders\n",
    "    .withColumn(\"delivery_date_estimate\", F.date_add(\"order_date\", 30))\n",
    "    .withColumn(\"renewal_date\", F.add_months(\"order_date\", 3))\n",
    "    .withColumn(\"month_end\", F.last_day(\"order_date\"))\n",
    "    .withColumn(\"next_monday\", F.next_day(\"order_date\", \"Monday\"))\n",
    ")\n",
    "\n",
    "# --- Task 3.3: Date Sequences ---\n",
    "date_range = test_orders.select(F.min(\"order_date\").alias(\"min_date\"), F.max(\"order_date\").alias(\"max_date\")).first()\n",
    "calendar = (\n",
    "    spark.range(1)\n",
    "    .select(F.explode(F.sequence(F.lit(date_range[\"min_date\"]), F.lit(date_range[\"max_date\"]), F.expr(\"interval 1 day\"))).alias(\"date\"))\n",
    ")\n",
    "\n",
    "print(\"PySpark Solutions executed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7bc6f7c-8cff-4ddb-b4e8-10ff60942ece",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean up temporary views\n",
    "# spark.catalog.dropTempView(\"orders\")\n",
    "# spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f08d0dd4-5f42-4a2d-818c-f1ab07d4df0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Clean up resources\n",
    "Stop any active streams and remove the created resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "228fbe33-82c6-4a53-8fad-019f8f1f6bbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "# dbutils.fs.rm(CHECKPOINT_PATH, True)\n",
    "# spark.sql(f\"DROP SCHEMA IF EXISTS {CATALOG}.{SCHEMA} CASCADE\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4941141843295194,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_advanced_transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
