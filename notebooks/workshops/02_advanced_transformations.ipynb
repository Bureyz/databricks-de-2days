{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b79fe2f8",
   "metadata": {},
   "source": [
    "# Advanced Transformations (PySpark & SQL)\n",
    "\n",
    "**Workshop Objective:**\n",
    "- Practical application of Window Functions (lag, lead, rank, rolling aggregations)\n",
    "- Processing complex structures (JSON, arrays, structs)\n",
    "- Advanced date and time operations\n",
    "- Transformation optimization for performance\n",
    "\n",
    "**Note:** You can choose to solve the tasks using **PySpark** or **SQL**. Both approaches are provided.\n",
    "\n",
    "**Time:** 30 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf463019",
   "metadata": {},
   "source": [
    "## User Isolation\n",
    "This notebook is designed to be run in a shared environment.\n",
    "To avoid conflicts, we will use a unique `catalog` and `schema` for your user.\n",
    "The `00_setup` script will automatically configure these for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b049d992",
   "metadata": {},
   "source": [
    "## Environment Configuration\n",
    "We will configure the environment variables and paths used in this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06065d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7079deec",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7cc526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Display user context\n",
    "print(\"=== User Context ===\")\n",
    "print(f\"Catalog: {CATALOG}\")\n",
    "print(f\"Schema: {BRONZE_SCHEMA}\")\n",
    "print(f\"User: {raw_user}\")\n",
    "\n",
    "# Set catalog and schema as default\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba81015",
   "metadata": {},
   "source": [
    "## Data Preparation from Databricks Volume\n",
    "\n",
    "Load data from Databricks Volume for the workshop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28939c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to Volume\n",
    "volume_path = \"/Volumes/main/default/\"\n",
    "\n",
    "# Load customer data\n",
    "customers_df = spark.read.csv(f\"{volume_path}/customers/customers.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e40d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample orders data for the workshop\n",
    "data = [\n",
    "    (1, \"2024-01-01\", 100), (1, \"2024-01-02\", 150), (1, \"2024-01-02\", 150), (1, \"2024-01-05\", 200),\n",
    "    (2, \"2024-01-10\", 50), (2, \"2024-01-12\", 80),\n",
    "    (3, \"2024-02-01\", 300), (3, \"2024-02-01\", 300), (3, \"2024-02-05\", 350)\n",
    "]\n",
    "columns = [\"customer_id\", \"order_date\", \"total_amount\"]\n",
    "test_orders = spark.createDataFrame(data, columns)\n",
    "test_orders = test_orders.withColumn(\"order_date\", F.to_date(\"order_date\"))\n",
    "\n",
    "# Register as temp view for SQL exercises\n",
    "test_orders.createOrReplaceTempView(\"orders\")\n",
    "print(\"Created 'orders' temporary view for SQL exercises.\")\n",
    "display(test_orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9939c285",
   "metadata": {},
   "source": [
    "# Part A: PySpark Implementation\n",
    "\n",
    "In this section, you will implement the transformations using the PySpark DataFrame API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb547007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON data from Volume (orders may contain nested structures)\n",
    "# Volume already contains parsed JSON, but we can create an example with nested structure\n",
    "\n",
    "# Option 1: Use data from Volume and create nested JSON\n",
    "json_orders = spark.read.json(f\"{volume_path}/orders/orders_batch.json\")\n",
    "\n",
    "# Option 2: For practice, create test data with nested JSON string\n",
    "json_data = spark.createDataFrame([\n",
    " (1, '{\"items\": [{\"product\": \"laptop\", \"price\": 1200}, {\"product\": \"mouse\", \"price\": 25}], \"total\": 1225}'),\n",
    " (2, '{\"items\": [{\"product\": \"keyboard\", \"price\": 80}], \"total\": 80}'),\n",
    " (3, '{\"items\": [{\"product\": \"monitor\", \"price\": 350}, {\"product\": \"cable\", \"price\": 15}], \"total\": 365}')\n",
    "], [\"order_id\", \"order_json\"])\n",
    "\n",
    "# Register for SQL\n",
    "json_data.createOrReplaceTempView(\"json_orders_raw\")\n",
    "\n",
    "display(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb614d00",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Window Functions\n",
    "\n",
    "### Ranking - ROW_NUMBER, RANK, DENSE_RANK\n",
    "\n",
    "**Instructions:**\n",
    "1. For each customer, rank orders by date (newest first)\n",
    "2. Add columns:\n",
    " - `row_num`: using `row_number()`\n",
    " - `rank`: using `rank()`\n",
    " - `dense_rank`: using `dense_rank()`\n",
    "3. Window spec: `partitionBy(\"customer_id\").orderBy(F.desc(\"order_date\"))`\n",
    "\n",
    "**Expected Result:**\n",
    "- Each customer has orders numbered starting from 1 (newest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04376587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Task 1.1 - Ranking functions\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define window spec\n",
    "window_spec = Window.____(\"____\").orderBy(F.____(\"____\")) # partitionBy customer_id, orderBy desc order_date\n",
    "\n",
    "# Add ranking columns\n",
    "orders_ranked = (\n",
    " test_orders\n",
    " .withColumn(\"row_num\", F.____().____(window_spec)) # row_number, over\n",
    " .withColumn(\"rank\", F.____().over(____)) # rank, window_spec\n",
    " .withColumn(\"dense_rank\", F.____().over(window_spec)) # dense_rank\n",
    ")\n",
    "\n",
    "display(orders_ranked.orderBy(\"customer_id\", \"order_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8efa955",
   "metadata": {},
   "source": [
    "**Explanation of differences:**\n",
    "\n",
    "- **ROW_NUMBER**: Unique sequential numbers (1, 2, 3...)\n",
    "- **RANK**: Gaps in numbering for ties (1, 2, 2, 4...)\n",
    "- **DENSE_RANK**: No gaps for ties (1, 2, 2, 3...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69daf8af",
   "metadata": {},
   "source": [
    "### LAG and LEAD - Compare with previous/next values\n",
    "\n",
    "**Instructions:**\n",
    "1. For each customer, calculate:\n",
    " - `previous_order_amount`: value of the previous order (using `lag`)\n",
    " - `next_order_amount`: value of the next order (using `lead`)\n",
    " - `amount_diff_vs_previous`: difference between current and previous\n",
    "2. Window spec: `partitionBy(\"customer_id\").orderBy(\"order_date\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5cbec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Task 1.2 - LAG and LEAD\n",
    "\n",
    "# Window spec - chronological order\n",
    "window_chrono = Window.partitionBy(\"____\").orderBy(\"____\") # customer_id, order_date\n",
    "\n",
    "# Use LAG and LEAD\n",
    "orders_lag_lead = (\n",
    " test_orders\n",
    " .withColumn(\"previous_order_amount\", F.____(____, ____).over(____)) # lag, total_amount, 1, window_chrono\n",
    " .withColumn(\"next_order_amount\", F.____(____, 1).over(window_chrono)) # lead, total_amount\n",
    " .withColumn(\n",
    " \"amount_diff_vs_previous\",\n",
    " F.col(\"____\") - F.col(\"____\") # total_amount, previous_order_amount\n",
    " )\n",
    ")\n",
    "\n",
    "display(orders_lag_lead.select(\n",
    " \"customer_id\", \"order_date\", \"total_amount\", \n",
    " \"previous_order_amount\", \"next_order_amount\", \"amount_diff_vs_previous\"\n",
    ").orderBy(\"customer_id\", \"order_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8ac7c9",
   "metadata": {},
   "source": [
    "### Rolling Aggregations - Moving Averages\n",
    "\n",
    "**Instructions:**\n",
    "1. Calculate rolling average for order amount:\n",
    " - Window: 3 last orders (current + 2 previous)\n",
    "2. Use `.rowsBetween(-2, 0)` for window spec\n",
    "3. Add column `rolling_avg_3_orders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db295511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Task 1.3 - Rolling aggregations\n",
    "\n",
    "# Window spec with rowsBetween\n",
    "window_rolling = (\n",
    " Window\n",
    " .partitionBy(\"customer_id\")\n",
    " .orderBy(\"order_date\")\n",
    " .____(____, ____) # rowsBetween, -2, 0 (3 last records)\n",
    ")\n",
    "\n",
    "# Rolling average\n",
    "orders_rolling = (\n",
    " test_orders\n",
    " .withColumn(\n",
    " \"rolling_avg_3_orders\",\n",
    " F.____(\"____\").over(____) # avg, total_amount, window_rolling\n",
    " )\n",
    " .withColumn(\n",
    " \"rolling_sum_3_orders\",\n",
    " F.sum(\"total_amount\").over(window_rolling)\n",
    " )\n",
    ")\n",
    "\n",
    "display(orders_rolling.select(\n",
    " \"customer_id\", \"order_date\", \"total_amount\", \n",
    " \"rolling_avg_3_orders\", \"rolling_sum_3_orders\"\n",
    ").orderBy(\"customer_id\", \"order_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da347e5",
   "metadata": {},
   "source": [
    "### Cumulative Sum\n",
    "\n",
    "**Instructions:**\n",
    "1. Calculate cumulative sum of order amounts per customer\n",
    "2. Use `.rowsBetween(Window.unboundedPreceding, Window.currentRow)`\n",
    "3. Add column `cumulative_amount`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc2f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Task 1.4 - Cumulative sum\n",
    "\n",
    "# Window spec for cumulative\n",
    "window_cumulative = (\n",
    " Window\n",
    " .partitionBy(\"____\")\n",
    " .orderBy(\"____\")\n",
    " .rowsBetween(Window.____, Window.____) # unboundedPreceding, currentRow\n",
    ")\n",
    "\n",
    "# Cumulative sum\n",
    "orders_cumulative = (\n",
    " test_orders\n",
    " .withColumn(\n",
    " \"cumulative_amount\",\n",
    " F.____(____(\"____\")).over(window_cumulative) # round, sum total_amount\n",
    " )\n",
    ")\n",
    "\n",
    "display(orders_cumulative.select(\n",
    " \"customer_id\", \"order_date\", \"total_amount\", \"cumulative_amount\"\n",
    ").orderBy(\"customer_id\", \"order_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b5e47e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Processing Complex Structures\n",
    "\n",
    "### JSON Processing - from_json() and explode()\n",
    "\n",
    "**Instructions:**\n",
    "1. Load JSON data from Volume (orders)\n",
    "2. Use `from_json()` to parse JSON if needed\n",
    "3. Use `explode()` to \"unpack\" array\n",
    "4. Extract fields from nested struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcd357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Task 2.1 - JSON processing\n",
    "\n",
    "# Define JSON schema\n",
    "json_schema = StructType([\n",
    " StructField(\"items\", ArrayType(StructType([\n",
    " StructField(\"product\", StringType()),\n",
    " StructField(\"price\", IntegerType())\n",
    " ]))),\n",
    " StructField(\"total\", IntegerType())\n",
    "])\n",
    "\n",
    "# Parse JSON\n",
    "orders_parsed = (\n",
    " json_data\n",
    " .withColumn(\"parsed\", F.____(____(\"____\"), ____)) # from_json, order_json, json_schema\n",
    ")\n",
    "\n",
    "display(orders_parsed.select(\"order_id\", \"parsed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa66a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Explode array and extract fields\n",
    "\n",
    "orders_exploded = (\n",
    " orders_parsed\n",
    " .withColumn(\"item\", F.____(\"____\")) # explode, parsed.items\n",
    " .select(\n",
    " \"order_id\",\n",
    " F.col(\"____\").alias(\"product_name\"), # item.product\n",
    " F.col(\"____\").alias(\"product_price\"), # item.price\n",
    " F.col(\"____\").alias(\"order_total\") # parsed.total\n",
    " )\n",
    ")\n",
    "\n",
    "display(orders_exploded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8f6ca2",
   "metadata": {},
   "source": [
    "### Array Functions - collect_list, array_contains\n",
    "\n",
    "**Instructions:**\n",
    "1. Group orders per customer\n",
    "2. Use `collect_list()` to gather all order amounts into an array\n",
    "3. Use `array_contains()` to check if customer has an order > 500\n",
    "4. Use `size()` to count number of orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec89493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Task 2.2 - Array functions\n",
    "\n",
    "customer_arrays = (\n",
    " test_orders\n",
    " .groupBy(\"____\") # customer_id\n",
    " .agg(\n",
    " F.____(____(\"____\")).alias(\"order_amounts\"), # collect_list, total_amount\n",
    " F.collect_list(\"order_date\").alias(\"order_dates\"),\n",
    " F.count(\"*\").alias(\"total_orders\")\n",
    " )\n",
    " .withColumn(\n",
    " \"num_orders\",\n",
    " F.____(\"____\") # size, order_amounts\n",
    " )\n",
    ")\n",
    "\n",
    "display(customer_arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cba817",
   "metadata": {},
   "source": [
    "### Struct - Combining columns into structures\n",
    "\n",
    "**Instructions:**\n",
    "1. Create struct `customer_info` containing: customer_id, total_orders\n",
    "2. Create struct `order_summary` containing: min/max/avg amount\n",
    "3. Extract fields from struct using `.` notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b221a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Task 2.3 - Struct operations\n",
    "\n",
    "customer_structs = (\n",
    " test_orders\n",
    " .groupBy(\"customer_id\")\n",
    " .agg(\n",
    " F.count(\"*\").alias(\"total_orders\"),\n",
    " F.min(\"total_amount\").alias(\"min_amount\"),\n",
    " F.max(\"total_amount\").alias(\"max_amount\"),\n",
    " F.avg(\"total_amount\").alias(\"avg_amount\")\n",
    " )\n",
    " .withColumn(\n",
    " \"customer_info\",\n",
    " F.____(\"____\", \"____\") # struct, customer_id, total_orders\n",
    " )\n",
    " .withColumn(\n",
    " \"order_summary\",\n",
    " F.struct(\"min_amount\", \"____\", \"____\") # max_amount, avg_amount\n",
    " )\n",
    ")\n",
    "\n",
    "display(customer_structs.select(\"customer_info\", \"order_summary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5d4e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract fields from struct\n",
    "customer_flat = (\n",
    " customer_structs\n",
    " .select(\n",
    " F.col(\"customer_info.____\").alias(\"customer_id\"), # customer_id\n",
    " F.col(\"order_summary.____\").alias(\"avg_order_value\") # avg_amount\n",
    " )\n",
    ")\n",
    "\n",
    "display(customer_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdf507d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Advanced Date Operations\n",
    "\n",
    "### Task 3.1: Date truncation and extraction\n",
    "\n",
    "**Instructions:**\n",
    "1. Use `date_trunc()` to round dates to: month, quarter, year\n",
    "2. Use `year()`, `month()`, `dayofweek()` to extract date parts\n",
    "3. Calculate `days_since_order` (difference between today and order date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ae6bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Task 3.1 - Date functions\n",
    "\n",
    "orders_dates = (\n",
    " test_orders\n",
    " .withColumn(\"order_month\", F.____(____(\"____\"), \"____\")) # date_trunc, order_date, month\n",
    " .withColumn(\"order_quarter\", F.date_trunc(\"____\", \"order_date\")) # quarter\n",
    " .withColumn(\"order_year_num\", F.____(\"____\")) # year, order_date\n",
    " .withColumn(\"order_month_num\", F.____(____(\"____\"))) # month, order_date\n",
    " .withColumn(\"day_of_week\", F.____(____(\"order_date\"))) # dayofweek\n",
    " .withColumn(\n",
    " \"days_since_order\",\n",
    " F.datediff(F.____, \"____\") # current_date, order_date\n",
    " )\n",
    ")\n",
    "\n",
    "display(orders_dates.select(\n",
    " \"order_id\", \"order_date\", \"order_month\", \"order_quarter\",\n",
    " \"order_year_num\", \"order_month_num\", \"day_of_week\", \"days_since_order\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faf8565",
   "metadata": {},
   "source": [
    "### Task 3.2: Date arithmetic - adding/subtracting periods\n",
    "\n",
    "**Instructions:**\n",
    "1. Use `date_add()` to add 30 days to order date\n",
    "2. Use `add_months()` to add 3 months\n",
    "3. Use `last_day()` to get the last day of the month\n",
    "4. Use `next_day()` to get the next Monday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c0b1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Task 3.2 - Date arithmetic\n",
    "\n",
    "orders_date_math = (\n",
    " test_orders\n",
    " .withColumn(\"delivery_date_estimate\", F.____(____(\"____\"), ____)) # date_add, order_date, 30\n",
    " .withColumn(\"renewal_date\", F.____(____(\"order_date\"), ____)) # add_months, 3\n",
    " .withColumn(\"month_end\", F.____(____(\"____\"))) # last_day, order_date\n",
    " .withColumn(\"next_monday\", F.next_day(\"____\", \"____\")) # order_date, Monday\n",
    ")\n",
    "\n",
    "display(orders_date_math.select(\n",
    " \"order_date\", \"delivery_date_estimate\", \"renewal_date\", \n",
    " \"month_end\", \"next_monday\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91643db",
   "metadata": {},
   "source": [
    "### Task 3.3: Generating date sequences\n",
    "\n",
    "**Instructions:**\n",
    "1. Use `sequence()` to generate an array of dates between two dates\n",
    "2. Use `explode()` to create one row per date\n",
    "3. Create a calendar table with all days between min and max order_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10cdbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Task 3.3 - Date sequences\n",
    "\n",
    "# Find min and max dates\n",
    "date_range = test_orders.select(\n",
    " F.min(\"order_date\").alias(\"min_date\"),\n",
    " F.max(\"order_date\").alias(\"max_date\")\n",
    ").first()\n",
    "\n",
    "# Generate date sequence\n",
    "calendar = (\n",
    " spark.range(1)\n",
    " .select(\n",
    " F.____( # explode\n",
    " F.____(\n",
    " F.lit(date_range[\"____\"]), # min_date\n",
    " F.lit(date_range[\"max_date\"]),\n",
    " F.expr(\"____\") # interval 1 day\n",
    " )\n",
    " ).alias(\"date\")\n",
    " )\n",
    " .withColumn(\"year\", F.year(\"date\"))\n",
    " .withColumn(\"month\", F.____(____(\"____\"))) # month, date\n",
    " .withColumn(\"day_of_week\", F.dayofweek(\"date\"))\n",
    ")\n",
    "\n",
    "print(f\"Calendar table: {calendar.count()} days\")\n",
    "display(calendar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2da46ce",
   "metadata": {},
   "source": [
    "# Part B: SQL Implementation\n",
    "\n",
    "In this section, you will implement the same transformations using **Spark SQL**.\n",
    "We have already registered the `orders` and `json_orders_raw` temporary views for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6842a50c",
   "metadata": {},
   "source": [
    "### Ranking - ROW_NUMBER, RANK, DENSE_RANK\n",
    "\n",
    "**Instructions:**\n",
    "1. For each customer, rank orders by date (newest first)\n",
    "2. Add columns:\n",
    " - `row_num`: using `row_number()`\n",
    " - `rank`: using `rank()`\n",
    " - `dense_rank`: using `dense_rank()`\n",
    "3. Window spec: `partitionBy(\"customer_id\").orderBy(F.desc(\"order_date\"))`\n",
    "\n",
    "**Expected Result:**\n",
    "- Each customer has orders numbered starting from 1 (newest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302e3629",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- TODO: Task 1.1 - Ranking functions (SQL)\n",
    "SELECT \n",
    "    *,\n",
    "    ___ OVER (PARTITION BY ___ ORDER BY ___ DESC) as row_num,\n",
    "    ___ OVER (PARTITION BY ___ ORDER BY ___ DESC) as rank,\n",
    "    ___ OVER (PARTITION BY ___ ORDER BY ___ DESC) as dense_rank\n",
    "FROM orders\n",
    "ORDER BY customer_id, order_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f7ad8c",
   "metadata": {},
   "source": [
    "**Explanation of differences:**\n",
    "\n",
    "- **ROW_NUMBER**: Unique sequential numbers (1, 2, 3...)\n",
    "- **RANK**: Gaps in numbering for ties (1, 2, 2, 4...)\n",
    "- **DENSE_RANK**: No gaps for ties (1, 2, 2, 3...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6100586",
   "metadata": {},
   "source": [
    "### LAG and LEAD - Compare with previous/next values\n",
    "\n",
    "**Instructions:**\n",
    "1. For each customer, calculate:\n",
    " - `previous_order_amount`: value of the previous order (using `lag`)\n",
    " - `next_order_amount`: value of the next order (using `lead`)\n",
    " - `amount_diff_vs_previous`: difference between current and previous\n",
    "2. Window spec: `partitionBy(\"customer_id\").orderBy(\"order_date\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753524cb",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- TODO: Task 1.2 - LAG and LEAD (SQL)\n",
    "SELECT \n",
    "    customer_id, order_date, total_amount,\n",
    "    ___(total_amount) OVER (PARTITION BY customer_id ORDER BY order_date) as previous_order_amount,\n",
    "    ___(total_amount) OVER (PARTITION BY customer_id ORDER BY order_date) as next_order_amount,\n",
    "    total_amount - ___(total_amount) OVER (PARTITION BY customer_id ORDER BY order_date) as amount_diff_vs_previous\n",
    "FROM orders\n",
    "ORDER BY customer_id, order_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ba4fb7",
   "metadata": {},
   "source": [
    "### Rolling Aggregations - Moving Averages\n",
    "\n",
    "**Instructions:**\n",
    "1. Calculate rolling average for order amount:\n",
    " - Window: 3 last orders (current + 2 previous)\n",
    "2. Use `.rowsBetween(-2, 0)` for window spec\n",
    "3. Add column `rolling_avg_3_orders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82608e69",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- TODO: Task 1.3 - Rolling aggregations (SQL)\n",
    "SELECT \n",
    "    customer_id, order_date, total_amount,\n",
    "    AVG(total_amount) OVER (PARTITION BY customer_id ORDER BY order_date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) as rolling_avg_3_orders,\n",
    "    SUM(total_amount) OVER (PARTITION BY customer_id ORDER BY order_date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) as rolling_sum_3_orders\n",
    "FROM orders\n",
    "ORDER BY customer_id, order_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caf9197",
   "metadata": {},
   "source": [
    "### Cumulative Sum\n",
    "\n",
    "**Instructions:**\n",
    "1. Calculate cumulative sum of order amounts per customer\n",
    "2. Use `.rowsBetween(Window.unboundedPreceding, Window.currentRow)`\n",
    "3. Add column `cumulative_amount`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace19305",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- TODO: Task 1.4 - Cumulative sum (SQL)\n",
    "SELECT \n",
    "    customer_id, order_date, total_amount,\n",
    "    SUM(total_amount) OVER (PARTITION BY customer_id ORDER BY order_date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as cumulative_amount\n",
    "FROM orders\n",
    "ORDER BY customer_id, order_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b138990",
   "metadata": {},
   "source": [
    "### JSON Processing - from_json() and explode()\n",
    "\n",
    "**Instructions:**\n",
    "1. Load JSON data from Volume (orders)\n",
    "2. Use `from_json()` to parse JSON if needed\n",
    "3. Use `explode()` to \"unpack\" array\n",
    "4. Extract fields from nested struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2205c357",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- TODO: Task 2.1 - JSON processing (SQL)\n",
    "SELECT \n",
    "    order_id,\n",
    "    from_json(order_json, 'items ARRAY<STRUCT<product: STRING, price: INT>>, total INT') as parsed\n",
    "FROM json_orders_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9e2587",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- TODO: Explode array and extract fields (SQL)\n",
    "-- Note: We need to parse first, then explode. In SQL we can do it in one query or use CTE.\n",
    "WITH parsed_data AS (\n",
    "  SELECT order_id, from_json(order_json, 'items ARRAY<STRUCT<product: STRING, price: INT>>, total INT') as parsed\n",
    "  FROM json_orders_raw\n",
    ")\n",
    "SELECT \n",
    "    order_id,\n",
    "    item.product as product_name,\n",
    "    item.price as product_price,\n",
    "    parsed.total as order_total\n",
    "FROM parsed_data\n",
    "LATERAL VIEW explode(parsed.items) AS item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db3ec34",
   "metadata": {},
   "source": [
    "### Array Functions - collect_list, array_contains\n",
    "\n",
    "**Instructions:**\n",
    "1. Group orders per customer\n",
    "2. Use `collect_list()` to gather all order amounts into an array\n",
    "3. Use `array_contains()` to check if customer has an order > 500\n",
    "4. Use `size()` to count number of orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e19ba1",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- TODO: Task 2.2 - Array functions (SQL)\n",
    "SELECT \n",
    "    customer_id,\n",
    "    collect_list(total_amount) as order_amounts,\n",
    "    collect_list(order_date) as order_dates,\n",
    "    count(*) as total_orders,\n",
    "    size(collect_list(total_amount)) as num_orders\n",
    "FROM orders\n",
    "GROUP BY customer_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a7f7ef",
   "metadata": {},
   "source": [
    "### Struct - Combining columns into structures\n",
    "\n",
    "**Instructions:**\n",
    "1. Create struct `customer_info` containing: customer_id, total_orders\n",
    "2. Create struct `order_summary` containing: min/max/avg amount\n",
    "3. Extract fields from struct using `.` notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7d87e3",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- TODO: Task 2.3 - Struct operations (SQL)\n",
    "SELECT \n",
    "    struct(customer_id, count(*) as total_orders) as customer_info,\n",
    "    struct(min(total_amount) as min_amount, max(total_amount) as max_amount, avg(total_amount) as avg_amount) as order_summary\n",
    "FROM orders\n",
    "GROUP BY customer_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1848c318",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Extract fields from struct (SQL)\n",
    "-- Assuming we have the structs from previous query (using CTE for demo)\n",
    "WITH struct_data AS (\n",
    "    SELECT \n",
    "        struct(customer_id, count(*) as total_orders) as customer_info,\n",
    "        struct(min(total_amount) as min_amount, max(total_amount) as max_amount, avg(total_amount) as avg_amount) as order_summary\n",
    "    FROM orders\n",
    "    GROUP BY customer_id\n",
    ")\n",
    "SELECT \n",
    "    customer_info.customer_id,\n",
    "    order_summary.avg_amount as avg_order_value\n",
    "FROM struct_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14af8f63",
   "metadata": {},
   "source": [
    "### Task 3.1: Date truncation and extraction\n",
    "\n",
    "**Instructions:**\n",
    "1. Use `date_trunc()` to round dates to: month, quarter, year\n",
    "2. Use `year()`, `month()`, `dayofweek()` to extract date parts\n",
    "3. Calculate `days_since_order` (difference between today and order date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061b4bf5",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- TODO: Task 3.1 - Date functions (SQL)\n",
    "SELECT \n",
    "    customer_id, order_date,\n",
    "    date_trunc('month', order_date) as order_month,\n",
    "    date_trunc('quarter', order_date) as order_quarter,\n",
    "    year(order_date) as order_year_num,\n",
    "    month(order_date) as order_month_num,\n",
    "    dayofweek(order_date) as day_of_week,\n",
    "    datediff(current_date(), order_date) as days_since_order\n",
    "FROM orders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101d633e",
   "metadata": {},
   "source": [
    "### Task 3.2: Date arithmetic - adding/subtracting periods\n",
    "\n",
    "**Instructions:**\n",
    "1. Use `date_add()` to add 30 days to order date\n",
    "2. Use `add_months()` to add 3 months\n",
    "3. Use `last_day()` to get the last day of the month\n",
    "4. Use `next_day()` to get the next Monday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c35bd5",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- TODO: Task 3.2 - Date arithmetic (SQL)\n",
    "SELECT \n",
    "    order_date,\n",
    "    date_add(order_date, 30) as delivery_date_estimate,\n",
    "    add_months(order_date, 3) as renewal_date,\n",
    "    last_day(order_date) as month_end,\n",
    "    next_day(order_date, 'Monday') as next_monday\n",
    "FROM orders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53842f32",
   "metadata": {},
   "source": [
    "### Task 3.3: Generating date sequences\n",
    "\n",
    "**Instructions:**\n",
    "1. Use `sequence()` to generate an array of dates between two dates\n",
    "2. Use `explode()` to create one row per date\n",
    "3. Create a calendar table with all days between min and max order_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5585df6",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- TODO: Task 3.3 - Date sequences (SQL)\n",
    "WITH range AS (\n",
    "  SELECT min(order_date) as min_date, max(order_date) as max_date FROM orders\n",
    ")\n",
    "SELECT \n",
    "    explode(sequence(min_date, max_date, interval 1 day)) as date\n",
    "FROM range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68a6a2b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Workshop Summary\n",
    "\n",
    "**Objectives Achieved:**\n",
    "- Window Functions (ranking, lag/lead, rolling aggregations, cumulative sum)\n",
    "- JSON Processing (from_json, explode, struct)\n",
    "- Array operations (collect_list, array_contains, size)\n",
    "- Advanced date operations (truncation, arithmetic, sequences)\n",
    "\n",
    "**Key Takeaways:**\n",
    "1. Window Functions allow per-group analysis without GROUP BY\n",
    "2. JSON and complex structures are native in Spark\n",
    "3. Date functions enable advanced temporal analysis\n",
    "4. Optimization: use broadcast for small tables in JOIN\n",
    "\n",
    "**Best Practices:**\n",
    "- Window Functions: always define explicit window spec\n",
    "- JSON: use schema inference only for exploration\n",
    "- Dates: use native date types (not string)\n",
    "- Performance: cache() for frequently used DataFrames\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd9d79a",
   "metadata": {},
   "source": [
    "# Solution\n",
    "\n",
    "The complete code is below. Try to solve it yourself first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2684bb37",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- ============================================================\n",
    "-- FULL SOLUTION - Workshop 2: SQL\n",
    "-- ============================================================\n",
    "\n",
    "-- Task 1.1: Ranking\n",
    "SELECT *, \n",
    "  row_number() OVER (PARTITION BY customer_id ORDER BY order_date DESC) as row_num,\n",
    "  rank() OVER (PARTITION BY customer_id ORDER BY order_date DESC) as rank,\n",
    "  dense_rank() OVER (PARTITION BY customer_id ORDER BY order_date DESC) as dense_rank\n",
    "FROM orders;\n",
    "\n",
    "-- Task 1.2: LAG/LEAD\n",
    "SELECT *, \n",
    "  lag(total_amount) OVER (PARTITION BY customer_id ORDER BY order_date) as prev_amt,\n",
    "  lead(total_amount) OVER (PARTITION BY customer_id ORDER BY order_date) as next_amt\n",
    "FROM orders;\n",
    "\n",
    "-- Task 1.3: Rolling Aggregations\n",
    "SELECT *, \n",
    "  avg(total_amount) OVER (PARTITION BY customer_id ORDER BY order_date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) as rolling_avg \n",
    "FROM orders;\n",
    "\n",
    "-- Task 2.1: JSON\n",
    "SELECT from_json(order_json, 'items ARRAY<STRUCT<product: STRING, price: INT>>, total INT') as parsed FROM json_orders_raw;\n",
    "\n",
    "-- Task 2.2: Arrays\n",
    "SELECT customer_id, collect_list(total_amount) as amounts, size(collect_list(total_amount)) as count FROM orders GROUP BY customer_id;\n",
    "\n",
    "-- Task 3.1: Dates\n",
    "SELECT order_date, date_trunc('month', order_date) as mth, datediff(current_date(), order_date) as diff FROM orders;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52850fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FULL SOLUTION - Workshop 2: Advanced Transformations (PySpark)\n",
    "# ============================================================\n",
    "\n",
    "# --- Task 1.1: Ranking ---\n",
    "window_spec = Window.partitionBy(\"customer_id\").orderBy(F.desc(\"order_date\"))\n",
    "orders_ranked = (\n",
    "    test_orders\n",
    "    .withColumn(\"row_num\", F.row_number().over(window_spec))\n",
    "    .withColumn(\"rank\", F.rank().over(window_spec))\n",
    "    .withColumn(\"dense_rank\", F.dense_rank().over(window_spec))\n",
    ")\n",
    "\n",
    "# --- Task 1.2: LAG/LEAD ---\n",
    "window_chrono = Window.partitionBy(\"customer_id\").orderBy(\"order_date\")\n",
    "orders_lag_lead = (\n",
    "    test_orders\n",
    "    .withColumn(\"previous_order_amount\", F.lag(\"total_amount\", 1).over(window_chrono))\n",
    "    .withColumn(\"next_order_amount\", F.lead(\"total_amount\", 1).over(window_chrono))\n",
    "    .withColumn(\"amount_diff_vs_previous\", F.col(\"total_amount\") - F.col(\"previous_order_amount\"))\n",
    ")\n",
    "\n",
    "# --- Task 1.3: Rolling Aggregations ---\n",
    "window_rolling = Window.partitionBy(\"customer_id\").orderBy(\"order_date\").rowsBetween(-2, 0)\n",
    "orders_rolling = (\n",
    "    test_orders\n",
    "    .withColumn(\"rolling_avg_3_orders\", F.avg(\"total_amount\").over(window_rolling))\n",
    "    .withColumn(\"rolling_sum_3_orders\", F.sum(\"total_amount\").over(window_rolling))\n",
    ")\n",
    "\n",
    "# --- Task 1.4: Cumulative Sum ---\n",
    "window_cumulative = Window.partitionBy(\"customer_id\").orderBy(\"order_date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "orders_cumulative = (\n",
    "    test_orders\n",
    "    .withColumn(\"cumulative_amount\", F.sum(\"total_amount\").over(window_cumulative))\n",
    ")\n",
    "\n",
    "# --- Task 2.1: JSON Processing ---\n",
    "json_schema = \"items ARRAY<STRUCT<product: STRING, price: INT>>, total INT\"\n",
    "orders_parsed = json_data.withColumn(\"parsed\", F.from_json(\"order_json\", json_schema))\n",
    "\n",
    "# --- Task 2.2: Array Functions ---\n",
    "customer_arrays = (\n",
    "    test_orders\n",
    "    .groupBy(\"customer_id\")\n",
    "    .agg(\n",
    "        F.collect_list(\"total_amount\").alias(\"order_amounts\"),\n",
    "        F.collect_list(\"order_date\").alias(\"order_dates\"),\n",
    "        F.count(\"*\").alias(\"total_orders\")\n",
    "    )\n",
    "    .withColumn(\"num_orders\", F.size(\"order_amounts\"))\n",
    ")\n",
    "\n",
    "# --- Task 2.3: Struct Operations ---\n",
    "customer_structs = (\n",
    "    test_orders\n",
    "    .groupBy(\"customer_id\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"total_orders\"),\n",
    "        F.min(\"total_amount\").alias(\"min_amount\"),\n",
    "        F.max(\"total_amount\").alias(\"max_amount\"),\n",
    "        F.avg(\"total_amount\").alias(\"avg_amount\")\n",
    "    )\n",
    "    .withColumn(\"customer_info\", F.struct(\"customer_id\", \"total_orders\"))\n",
    "    .withColumn(\"order_summary\", F.struct(\"min_amount\", \"max_amount\", \"avg_amount\"))\n",
    ")\n",
    "\n",
    "# --- Task 3.1: Date Functions ---\n",
    "orders_dates = (\n",
    "    test_orders\n",
    "    .withColumn(\"order_month\", F.date_trunc(\"month\", \"order_date\"))\n",
    "    .withColumn(\"order_quarter\", F.date_trunc(\"quarter\", \"order_date\"))\n",
    "    .withColumn(\"order_year_num\", F.year(\"order_date\"))\n",
    "    .withColumn(\"order_month_num\", F.month(\"order_date\"))\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(\"order_date\"))\n",
    "    .withColumn(\"days_since_order\", F.datediff(F.current_date(), \"order_date\"))\n",
    ")\n",
    "\n",
    "# --- Task 3.2: Date Arithmetic ---\n",
    "orders_date_math = (\n",
    "    test_orders\n",
    "    .withColumn(\"delivery_date_estimate\", F.date_add(\"order_date\", 30))\n",
    "    .withColumn(\"renewal_date\", F.add_months(\"order_date\", 3))\n",
    "    .withColumn(\"month_end\", F.last_day(\"order_date\"))\n",
    "    .withColumn(\"next_monday\", F.next_day(\"order_date\", \"Monday\"))\n",
    ")\n",
    "\n",
    "# --- Task 3.3: Date Sequences ---\n",
    "date_range = test_orders.select(F.min(\"order_date\").alias(\"min_date\"), F.max(\"order_date\").alias(\"max_date\")).first()\n",
    "calendar = (\n",
    "    spark.range(1)\n",
    "    .select(F.explode(F.sequence(F.lit(date_range[\"min_date\"]), F.lit(date_range[\"max_date\"]), F.expr(\"interval 1 day\"))).alias(\"date\"))\n",
    ")\n",
    "\n",
    "print(\"PySpark Solutions executed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d24a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary views\n",
    "# spark.catalog.dropTempView(\"orders\")\n",
    "# spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e939bbe",
   "metadata": {},
   "source": [
    "## Clean up resources\n",
    "Stop any active streams and remove the created resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ebfee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "# dbutils.fs.rm(CHECKPOINT_PATH, True)\n",
    "# spark.sql(f\"DROP SCHEMA IF EXISTS {CATALOG}.{SCHEMA} CASCADE\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
