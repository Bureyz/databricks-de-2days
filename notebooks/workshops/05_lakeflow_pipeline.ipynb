{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2674372-bd05-4b58-8e25-4e2abf5278f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lakeflow Pipelines\n",
    "\n",
    "## Scenario\n",
    "\n",
    "Our retail company, \"BikeSuperstore\", is modernizing its data platform. We have raw CSV files landing in our data lake (Orders, Customers, Products), and the business intelligence team needs a reliable, up-to-date **Star Schema** to report on sales performance.\n",
    "\n",
    "**Challenge:**\n",
    "Traditional ETL pipelines are brittle and hard to maintain. We need to use **Databricks Lakeflow (Spark Declarative Pipelines)** to build a robust system that handles:\n",
    "1.  **Ingestion**: Automatically loading new files.\n",
    "2.  **History**: Tracking changes in customer data (SCD Type 2).\n",
    "3.  **Quality**: Enforcing data quality rules.\n",
    "4.  **Modeling**: Creating a Gold layer with a Star Schema.\n",
    "\n",
    "**Task:**\n",
    "You will implement the pipeline definition. You can choose to work in **SQL** or **Python** (or both!). The code provided has some \"blanks\" that you need to fill in to make the pipeline work.\n",
    "\n",
    "**Time:** 45 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c8d2d43-380e-4f99-9094-bcc670e52d4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## User Isolation\n",
    "This notebook is designed to be run in a shared environment.\n",
    "To avoid conflicts, we will use a unique `catalog` and `schema` for your user.\n",
    "The `00_setup` script will automatically configure these for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0a9d341-b667-45c1-8c6e-9269a94c6e79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44cd6b57-203f-4344-9cd6-cbce5b6f5e81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Environment Configuration\n",
    "We will configure the environment variables and paths used in this workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2aa2d6c7-51c7-4f03-987c-ea613aa12c51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Business Logic & Architecture\n",
    "\n",
    "We are following the **Medallion Architecture**:\n",
    "\n",
    "1.  **Bronze Layer (Raw)**:\n",
    "    *   Ingest CSV files from `Customers`, `Product`, `ProductCategory`, `SalesOrderHeader`, `SalesOrderDetail`.\n",
    "    *   Use **Auto Loader** (`cloud_files`) for efficient incremental ingestion.\n",
    "\n",
    "2.  **Silver Layer (Cleaned & Enriched)**:\n",
    "    *   **Customers**: Apply **SCD Type 2** (Slowly Changing Dimensions) to track history based on `ModifiedDate`.\n",
    "    *   **Products & Categories**: Apply **SCD Type 1** (Overwrite) to keep the latest info.\n",
    "    *   **Orders**: Clean data and apply **Data Quality Expectations** (e.g., `TotalDue > 0`).\n",
    "\n",
    "3.  **Gold Layer (Star Schema)**:\n",
    "    *   **Fact Table**: `fact_sales` (Transactions).\n",
    "    *   **Dimensions**:\n",
    "        *   `dim_customer`: Current customer details.\n",
    "        *   `dim_product`: Product details enriched with Category names.\n",
    "        *   `dim_date`: A generated calendar dimension for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a438c1e-a05c-480a-871c-640d1c856e3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![assets/images/ibDi2xO_FDNG-IhAmH5CO.png](../../assets/images/ibDi2xO_FDNG-IhAmH5CO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5301dbd4-5ab7-489a-a49f-27e7c63168c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Implementation Choice: SQL or Python\n",
    "\n",
    "Lakeflow SDP supports both languages.\n",
    "*   **Section A**: SQL Implementation (Standard for analysts/engineers).\n",
    "*   **Section B**: Python Implementation (Great for complex logic and metaprogramming)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b791ee43-ea99-4c93-8206-3c14db22304d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SQL Implementation\n",
    "\n",
    "In this section, you will complete the SQL DDL statements to define the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d995ab5-59c1-4f08-9ba3-a0dba6312cf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- ============================================================\n",
    "-- 1. BRONZE LAYER: Ingestion\n",
    "-- ============================================================\n",
    "-- TODO: Complete the Auto Loader syntax to read CSV files.\n",
    "-- Hint: Use cloud_files function. We need to infer schema and read headers.\n",
    "\n",
    "-- Bronze Customers\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_customers\n",
    "COMMENT 'Raw customers data from CSV'\n",
    "AS SELECT * FROM ___('${source_path}/Customers', 'csv', map(\"header\", \"true\", \"inferSchema\", \"___\"));\n",
    "\n",
    "-- Bronze Products\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_products\n",
    "COMMENT 'Raw products data from CSV'\n",
    "AS SELECT * FROM cloud_files('${source_path}/___', 'csv', map(\"header\", \"true\", \"inferSchema\", \"___\"));\n",
    "\n",
    "-- [TASK] Complete the code for Product Categories\n",
    "-- Bronze Product Categories\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_product_categories\n",
    "COMMENT 'Raw product categories data from CSV'\n",
    "AS SELECT * FROM cloud_files('${source_path}/ProductCategory', '___', map(___));\n",
    "\n",
    "-- [TASK] Complete the code for Orders Header\n",
    "-- Bronze Orders Header\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_orders_header\n",
    "COMMENT 'Raw orders data from CSV'\n",
    "AS SELECT * FROM cloud_files('${source_path}/SalesOrderHeader', '___', map(___));\n",
    "\n",
    "-- [TASK] Complete the code for Orders Detail\n",
    "-- Bronze Orders Detail\n",
    "CREATE OR REFRESH STREAMING TABLE bronze_orders_detail\n",
    "COMMENT 'Raw orders data from CSV'\n",
    "AS SELECT * FROM ___('${source_path}/___', 'csv', map(\"header\", \"true\", \"inferSchema\", \"true\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dee74b5-7cd8-4ed3-93c8-ab98d7080b56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- ============================================================\n",
    "-- 2. SILVER LAYER: SCD & Quality\n",
    "-- ============================================================\n",
    "\n",
    "-- [TASK] Implement SCD Type 2 for Customers\n",
    "-- We need to track history using 'ModifiedDate'\n",
    "CREATE OR REFRESH STREAMING TABLE silver_customers;\n",
    "\n",
    "AUTO CDC INTO silver_customers\n",
    "FROM bronze_customers\n",
    "KEYS (CustomerID)\n",
    "SEQUENCE BY ___\n",
    "STORED AS SCD TYPE ___;\n",
    "\n",
    "-- [TASK] Implement SCD Type 1 for Products\n",
    "CREATE OR REFRESH STREAMING TABLE silver_products;\n",
    "\n",
    "AUTO CDC INTO silver_products\n",
    "FROM bronze_products\n",
    "KEYS (___)\n",
    "SEQUENCE BY ___\n",
    "STORED AS SCD TYPE 1;\n",
    "\n",
    "-- [TASK] Implement SCD Type 1 for Product Categories\n",
    "CREATE OR REFRESH STREAMING TABLE silver_product_categories;\n",
    "\n",
    "AUTO CDC INTO silver_product_categories\n",
    "FROM bronze_product_categories\n",
    "KEYS (ProductCategoryID)\n",
    "SEQUENCE BY ___\n",
    "STORED AS SCD TYPE ___;\n",
    "\n",
    "-- [TASK] Add Data Quality Expectations for Orders\n",
    "-- Ensure TotalDue is greater than 0 (DROP ROW on violation)\n",
    "-- Ensure CustomerID is not null (FAIL UPDATE on violation)\n",
    "CREATE OR REFRESH STREAMING TABLE silver_orders\n",
    "(\n",
    "  CONSTRAINT valid_amount EXPECT (___) ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT valid_customer EXPECT (CustomerID IS NOT NULL) ON VIOLATION ___\n",
    ")\n",
    "AS SELECT \n",
    "  SalesOrderID, CustomerID, TotalDue, OrderDate, Status, current_timestamp() as processed_at\n",
    "FROM STREAM(bronze_orders_header);\n",
    "\n",
    "-- [TASK] Create Silver Order Details\n",
    "-- Just a simple pass-through with some cleaning if needed\n",
    "CREATE OR REFRESH STREAMING TABLE silver_order_details\n",
    "AS SELECT \n",
    "  SalesOrderID, ___, OrderQty, ProductID, UnitPrice, LineTotal, ModifiedDate\n",
    "FROM STREAM(___);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "579d552c-1141-4622-b14c-d8fe516c4265",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- ============================================================\n",
    "-- 3. GOLD LAYER: Star Schema\n",
    "-- ============================================================\n",
    "\n",
    "-- [TASK] Create the Customer Dimension\n",
    "-- Filter out old records (SCD Type 2) using __END_AT\n",
    "CREATE OR REFRESH MATERIALIZED VIEW dim_customer\n",
    "AS SELECT \n",
    "  CustomerID, FirstName, LastName, EmailAddress, Phone\n",
    "FROM ___\n",
    "WHERE ___ IS NULL;\n",
    "\n",
    "-- [TASK] Create the Product Dimension\n",
    "-- Join silver_products with silver_product_categories to get CategoryName\n",
    "CREATE OR REFRESH MATERIALIZED VIEW dim_product\n",
    "AS SELECT \n",
    "  p.ProductID,\n",
    "  p.Name as ProductName,\n",
    "  pc.Name as CategoryName\n",
    "FROM silver_products p\n",
    "LEFT JOIN ___ pc ON p.ProductCategoryID = ___;\n",
    "\n",
    "-- [TASK] Create the Date Dimension\n",
    "-- Extract Year, Month, Day from OrderDate in silver_orders\n",
    "CREATE OR REFRESH MATERIALIZED VIEW dim_date\n",
    "AS SELECT ___\n",
    "  cast(OrderDate as date) as DateKey,\n",
    "  year(OrderDate) as Year,\n",
    "  month(OrderDate) as Month,\n",
    "  ___(OrderDate) as Day\n",
    "FROM silver_orders;\n",
    "\n",
    "-- [TASK] Create the Fact Table\n",
    "-- Join Order Headers and Details\n",
    "CREATE OR REFRESH MATERIALIZED VIEW fact_sales\n",
    "AS SELECT \n",
    "  od.SalesOrderID,\n",
    "  oh.OrderDate,\n",
    "  oh.CustomerID,\n",
    "  od.ProductID,\n",
    "  od.LineTotal\n",
    "FROM silver_order_details od\n",
    "JOIN silver_orders oh ON ___ = ___;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f461c85-77e9-4f7f-8f74-dd0f628dad17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Python Implementation\n",
    "\n",
    "In this section, you will use the `pyspark.pipelines` (dp) API to define the same logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63dbbdf1-dd97-4529-8296-0376e2b6c859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.pipelines as dp\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "source_path = spark.conf.get(\"source_path\")\n",
    "\n",
    "# ============================================================\n",
    "# 1. BRONZE LAYER\n",
    "# ============================================================\n",
    "\n",
    "# Bronze Customers\n",
    "dp.create_streaming_table(name=\"bronze_customers\")\n",
    "\n",
    "@dp.append_flow(target=\"bronze_customers\")\n",
    "def bronze_customers_flow():\n",
    "    return (\n",
    "        spark.readStream.format(\"___\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(f\"{source_path}/___\")\n",
    "    )\n",
    "\n",
    "# [TASK] Bronze Products\n",
    "dp.create_streaming_table(name=\"bronze_products\")\n",
    "\n",
    "@dp.append_flow(target=\"bronze_products\")\n",
    "def bronze_products_flow():\n",
    "    return (\n",
    "        spark.readStream.format(\"___\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(f\"{source_path}/___\")\n",
    "    )\n",
    "\n",
    "# [TASK] Bronze Product Categories\n",
    "dp.create_streaming_table(name=\"bronze_product_categories\")\n",
    "\n",
    "@dp.append_flow(target=\"bronze_product_categories\")\n",
    "def bronze_product_categories_flow():\n",
    "    return (\n",
    "        spark.readStream.format(\"___\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(f\"{source_path}/___\")\n",
    "    )\n",
    "\n",
    "# [TASK] Complete the Bronze Orders Header flow\n",
    "dp.create_streaming_table(name=\"bronze_orders_header\")\n",
    "\n",
    "@dp.append_flow(target=\"bronze_orders_header\")\n",
    "def bronze_orders_header_flow():\n",
    "    return (\n",
    "        spark.readStream.format(\"___\") # Hint: Auto Loader format\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(f\"{source_path}/___\")\n",
    "    )\n",
    "\n",
    "# [TASK] Bronze Orders Detail\n",
    "dp.create_streaming_table(name=\"bronze_orders_detail\")\n",
    "\n",
    "@dp.append_flow(target=\"bronze_orders_detail\")\n",
    "def bronze_orders_detail_flow():\n",
    "    return (\n",
    "        spark.readStream.format(\"___\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(f\"{source_path}/___\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2d84b21-92d5-48e8-9f11-d6d615c34dff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. SILVER LAYER\n",
    "# ============================================================\n",
    "\n",
    "# [TASK] Define SCD Type 2 for Customers\n",
    "dp.create_streaming_table(name=\"silver_customers\")\n",
    "\n",
    "dp.create_auto_cdc_flow(\n",
    "    target=\"silver_customers\",\n",
    "    source=\"bronze_customers\",\n",
    "    keys=[\"___\"],\n",
    "    sequence_by=col(\"___\"), # What column tracks the change time?\n",
    "    stored_as_scd_type=\"___\" # Type 1 or 2?\n",
    ")\n",
    "\n",
    "# [TASK] Define SCD Type 1 for Products\n",
    "dp.create_streaming_table(name=\"silver_products\")\n",
    "\n",
    "dp.create_auto_cdc_flow(\n",
    "    target=\"silver_products\",\n",
    "    source=\"bronze_products\",\n",
    "    keys=[\"___\"],\n",
    "    sequence_by=col(\"___\"),\n",
    "    stored_as_scd_type=\"1\"\n",
    ")\n",
    "\n",
    "# [TASK] Define SCD Type 1 for Product Categories\n",
    "dp.create_streaming_table(name=\"silver_product_categories\")\n",
    "\n",
    "dp.create_auto_cdc_flow(\n",
    "    target=\"silver_product_categories\",\n",
    "    source=\"bronze_product_categories\",\n",
    "    keys=[\"___\"],\n",
    "    sequence_by=col(\"___\"),\n",
    "    stored_as_scd_type=\"1\"\n",
    ")\n",
    "\n",
    "# [TASK] Define Data Quality for Orders\n",
    "@dp.table(\n",
    "    name=\"silver_orders\",\n",
    "    expect_all_or_drop={\"valid_amount\": \"___ > 0\"},\n",
    "    expect_all_or_fail={\"valid_customer\": \"___\"} # Check for not null\n",
    ")\n",
    "def silver_orders():\n",
    "    return (\n",
    "        spark.readStream.table(\"bronze_orders_header\")\n",
    "        .select(\"SalesOrderID\", \"CustomerID\", \"TotalDue\", \"OrderDate\", \"Status\")\n",
    "    )\n",
    "\n",
    "# [TASK] Silver Order Details\n",
    "@dp.table(name=\"silver_order_details\")\n",
    "def silver_order_details():\n",
    "    return (\n",
    "        spark.readStream.table(\"___\")\n",
    "        .select(\"SalesOrderID\", \"___\", \"OrderQty\", \"ProductID\", \"UnitPrice\", \"LineTotal\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74240531-876e-47be-8a55-f1897e9229b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3. GOLD LAYER\n",
    "# ============================================================\n",
    "\n",
    "# [TASK] Create the Customer Dimension\n",
    "@dp.materialized_view(name=\"dim_customer\")\n",
    "def dim_customer():\n",
    "    return (\n",
    "        spark.read.table(\"___\")\n",
    "        .filter(col(\"___\").isNull()) # Filter for current records\n",
    "        .select(\"CustomerID\", \"FirstName\", \"LastName\", \"EmailAddress\")\n",
    "    )\n",
    "\n",
    "# [TASK] Create the Product Dimension\n",
    "@dp.materialized_view(name=\"dim_product\")\n",
    "def dim_product():\n",
    "    p = spark.read.table(\"___\")\n",
    "    pc = spark.read.table(\"silver_product_categories\")\n",
    "    \n",
    "    return (\n",
    "        p.join(pc, \"___\", \"left\")\n",
    "        .select(\n",
    "            p[\"ProductID\"],\n",
    "            p[\"Name\"].alias(\"ProductName\"),\n",
    "            pc[\"Name\"].alias(\"CategoryName\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "# [TASK] Create the Date Dimension\n",
    "@dp.materialized_view(name=\"dim_date\")\n",
    "def dim_date():\n",
    "    return (\n",
    "        spark.read.table(\"___\")\n",
    "        .select(col(\"OrderDate\").cast(\"date\").alias(\"DateKey\"))\n",
    "        .___()\n",
    "        .select(\n",
    "            col(\"DateKey\"),\n",
    "            year(\"DateKey\").alias(\"Year\"),\n",
    "            month(\"DateKey\").alias(\"Month\"),\n",
    "            dayofmonth(\"DateKey\").alias(\"Day\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "# [TASK] Create the Fact Sales Materialized View\n",
    "@dp.materialized_view(name=\"fact_sales\")\n",
    "def fact_sales():\n",
    "    od = spark.read.table(\"___\")\n",
    "    oh = spark.read.table(\"silver_orders\")\n",
    "    \n",
    "    # Perform the join\n",
    "    return (\n",
    "        od.join(oh, \"___\", \"inner\") # Join key?\n",
    "        .select(\n",
    "            od[\"SalesOrderID\"],\n",
    "            oh[\"OrderDate\"],\n",
    "            oh[\"CustomerID\"],\n",
    "            od[\"LineTotal\"]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a08dc29b-7d82-4e07-b38d-ca81df10fe3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Orchestration & Deployment\n",
    "\n",
    "In a real-world scenario, you wouldn't run these cells interactively to process data. Instead:\n",
    "\n",
    "1.  You commit these files (`pipeline.py` or `.sql` files) to a Git repository.\n",
    "2.  You create a **Delta Live Tables (DLT)** pipeline in Databricks Workflows.\n",
    "3.  You point the DLT pipeline to your source code.\n",
    "4.  Databricks handles the orchestration, retries, and scaling.\n",
    "\n",
    "## Stuck?\n",
    "\n",
    "If you are stuck or want to see the full, working solution, check the files in the `lakeflow/lakeflow_workshop` folder:\n",
    "*   `lakeflow/lakeflow_workshop/sql/` for the SQL solution.\n",
    "*   `lakeflow/lakeflow_workshop/python/pipeline.py` for the Python solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a6573a2-f7b8-41c5-b6bb-bcc4e6aa2281",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Clean up resources\n",
    "Stop any active streams and remove the created resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d3be0e5-fd27-4ba4-a49f-b2741e8553be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "# dbutils.fs.rm(CHECKPOINT_PATH, True)\n",
    "# spark.sql(f\"DROP SCHEMA IF EXISTS {CATALOG}.{SCHEMA} CASCADE\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_lakeflow_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
